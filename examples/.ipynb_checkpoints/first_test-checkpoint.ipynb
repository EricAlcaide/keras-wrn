{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" First tests with WRN - 16-4. \"\"\"\n",
    "import keras\n",
    "import numpy as np\n",
    "import keras_wrn as wrn\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers, optimizers\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=0,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"reflect\")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 60:\n",
    "        return 0.1\n",
    "    elif x < 120:\n",
    "        return 0.1*0.2\n",
    "    elif x < 160:\n",
    "        return 0.1*0.2*0.2\n",
    "    else:\n",
    "        return 0.1*0.2*0.2*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(n,k,act,dropout):\n",
    "    init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "    # For WRN-16-8 put N = 16, k = 8\n",
    "    # For WRN-28-10 put N = 28, k = 10\n",
    "    # For WRN-40-4 put N = 40, k = 4\n",
    "    model = wrn.build_model(init_shape, num_classes, n, k, act=act dropout=dropout)\n",
    "\n",
    "    print(\"Model Created\")\n",
    "    batch_size  = 128\n",
    "    epochs = 200\n",
    "\n",
    "    opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\n",
    "    lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    print(\"Finished compiling\")\n",
    "\n",
    "    ####################\n",
    "    # Network training #\n",
    "    ####################\n",
    "\n",
    "    print(\"Gonna fit the model\")\n",
    "    his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "    print(his.history)\n",
    "    return his.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 72s 184ms/step - loss: 1.3583 - acc: 0.5045 - val_loss: 2.1123 - val_acc: 0.4237\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 70s 181ms/step - loss: 0.8754 - acc: 0.6894 - val_loss: 1.6450 - val_acc: 0.5388\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.6964 - acc: 0.7569 - val_loss: 1.1462 - val_acc: 0.6670\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.5959 - acc: 0.7931 - val_loss: 0.9043 - val_acc: 0.6999\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.5264 - acc: 0.8167 - val_loss: 0.9649 - val_acc: 0.7283\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.4744 - acc: 0.8359 - val_loss: 0.9474 - val_acc: 0.6938\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.4318 - acc: 0.8495 - val_loss: 0.6826 - val_acc: 0.7864\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.3983 - acc: 0.8617 - val_loss: 0.7455 - val_acc: 0.7788\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.3693 - acc: 0.8727 - val_loss: 0.7462 - val_acc: 0.7776\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.3454 - acc: 0.8798 - val_loss: 0.5729 - val_acc: 0.8252\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.3231 - acc: 0.8883 - val_loss: 0.6397 - val_acc: 0.8078\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.3022 - acc: 0.8947 - val_loss: 0.5732 - val_acc: 0.8227\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2845 - acc: 0.9010 - val_loss: 0.7019 - val_acc: 0.8088\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2705 - acc: 0.9050 - val_loss: 0.4758 - val_acc: 0.8546\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2537 - acc: 0.9111 - val_loss: 0.4472 - val_acc: 0.8675\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2433 - acc: 0.9156 - val_loss: 0.4924 - val_acc: 0.8533\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2281 - acc: 0.9203 - val_loss: 0.8640 - val_acc: 0.7942\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2157 - acc: 0.9238 - val_loss: 0.6509 - val_acc: 0.8234\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.2096 - acc: 0.9257 - val_loss: 0.4152 - val_acc: 0.8749\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1935 - acc: 0.9326 - val_loss: 0.6366 - val_acc: 0.8390\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1894 - acc: 0.9334 - val_loss: 0.5072 - val_acc: 0.8605\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1745 - acc: 0.9378 - val_loss: 0.5217 - val_acc: 0.8621\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1686 - acc: 0.9408 - val_loss: 0.5780 - val_acc: 0.8498\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1680 - acc: 0.9408 - val_loss: 0.7001 - val_acc: 0.8278\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1594 - acc: 0.9433 - val_loss: 0.4915 - val_acc: 0.8690\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1523 - acc: 0.9457 - val_loss: 0.6204 - val_acc: 0.8480\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1404 - acc: 0.9498 - val_loss: 0.5205 - val_acc: 0.8677\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1387 - acc: 0.9513 - val_loss: 0.5631 - val_acc: 0.8599\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1292 - acc: 0.9547 - val_loss: 0.4762 - val_acc: 0.8759\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1286 - acc: 0.9537 - val_loss: 0.5445 - val_acc: 0.8653\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1229 - acc: 0.9570 - val_loss: 0.5531 - val_acc: 0.8627\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1153 - acc: 0.9589 - val_loss: 0.7604 - val_acc: 0.8288\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1140 - acc: 0.9585 - val_loss: 0.4384 - val_acc: 0.8904\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.1056 - acc: 0.9625 - val_loss: 0.4676 - val_acc: 0.8865\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.1060 - acc: 0.9622 - val_loss: 0.5274 - val_acc: 0.8767\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0995 - acc: 0.9640 - val_loss: 0.5685 - val_acc: 0.8704\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0976 - acc: 0.9651 - val_loss: 0.5527 - val_acc: 0.8696\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0972 - acc: 0.9656 - val_loss: 0.3931 - val_acc: 0.8977\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0874 - acc: 0.9690 - val_loss: 0.5197 - val_acc: 0.8838\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0877 - acc: 0.9690 - val_loss: 0.4737 - val_acc: 0.8851\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0835 - acc: 0.9701 - val_loss: 0.4903 - val_acc: 0.8895\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0814 - acc: 0.9713 - val_loss: 0.4588 - val_acc: 0.8900\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0786 - acc: 0.9716 - val_loss: 0.4888 - val_acc: 0.8875\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0745 - acc: 0.9733 - val_loss: 0.5827 - val_acc: 0.8783\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0720 - acc: 0.9747 - val_loss: 0.5451 - val_acc: 0.8767\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0700 - acc: 0.9757 - val_loss: 0.4607 - val_acc: 0.8949\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0701 - acc: 0.9754 - val_loss: 0.5313 - val_acc: 0.8853\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0650 - acc: 0.9769 - val_loss: 0.4852 - val_acc: 0.8910\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0665 - acc: 0.9769 - val_loss: 0.5197 - val_acc: 0.8817\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0631 - acc: 0.9779 - val_loss: 0.4413 - val_acc: 0.8984\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0591 - acc: 0.9787 - val_loss: 0.4342 - val_acc: 0.8946\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0580 - acc: 0.9804 - val_loss: 0.4765 - val_acc: 0.8937\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0566 - acc: 0.9801 - val_loss: 0.4874 - val_acc: 0.8911\n",
      "Epoch 54/200\n",
      "103/390 [======>.......................] - ETA: 49s - loss: 0.0511 - acc: 0.9827"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2356be3485ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# WRN-16-4 - no dropout w/ custom Activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me_swish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Replace e_swish by \"relu\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-d6138600822f>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(n, k, act, dropout)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gonna fit the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mhis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating custom activation: E-swish. For ReLU just type \"relu\"\n",
    "def e_swish(x):\n",
    "    return 1.375*x*K.sigmoid(x)\n",
    "\n",
    "# WRN-16-4 - no dropout w/ custom Activation\n",
    "run(n=16, k=4, act=e_swish, dropout=0) # Replace e_swish by \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 85s 217ms/step - loss: 1.4276 - acc: 0.4777 - val_loss: 1.5332 - val_acc: 0.5302\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.9269 - acc: 0.6727 - val_loss: 1.1898 - val_acc: 0.6396- loss\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.7178 - acc: 0.7477 - val_loss: 0.8958 - val_acc: 0.6924\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.5994 - acc: 0.7912 - val_loss: 0.7637 - val_acc: 0.7499\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.5127 - acc: 0.8231 - val_loss: 0.7218 - val_acc: 0.7627\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.4560 - acc: 0.8420 - val_loss: 0.6512 - val_acc: 0.7885\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.4093 - acc: 0.8577 - val_loss: 0.6236 - val_acc: 0.7958\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3731 - acc: 0.8713 - val_loss: 0.4569 - val_acc: 0.8454\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3359 - acc: 0.8828 - val_loss: 0.4761 - val_acc: 0.8428\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3100 - acc: 0.8929 - val_loss: 0.5182 - val_acc: 0.8345\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2890 - acc: 0.9001 - val_loss: 0.5099 - val_acc: 0.8323\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2649 - acc: 0.9077 - val_loss: 0.4896 - val_acc: 0.8464\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2475 - acc: 0.9136 - val_loss: 0.5628 - val_acc: 0.8220\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2313 - acc: 0.9204 - val_loss: 0.5033 - val_acc: 0.8371\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.2111 - acc: 0.9261 - val_loss: 0.4215 - val_acc: 0.8595s: 0.2106 - a\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1974 - acc: 0.9309 - val_loss: 0.4741 - val_acc: 0.8556\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1860 - acc: 0.9342 - val_loss: 0.6331 - val_acc: 0.8299 ETA: 0s - loss: 0.1861 - acc: 0.93\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1730 - acc: 0.9388 - val_loss: 0.4998 - val_acc: 0.8554\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1645 - acc: 0.9424 - val_loss: 0.4306 - val_acc: 0.8714\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1518 - acc: 0.9463 - val_loss: 0.5075 - val_acc: 0.8639\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1384 - acc: 0.9512 - val_loss: 0.5315 - val_acc: 0.8571\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1341 - acc: 0.9529 - val_loss: 0.4750 - val_acc: 0.8623\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1261 - acc: 0.9554 - val_loss: 0.4448 - val_acc: 0.8807\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1194 - acc: 0.9585 - val_loss: 0.4461 - val_acc: 0.8714\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1113 - acc: 0.9605 - val_loss: 0.4268 - val_acc: 0.8854\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.1042 - acc: 0.9629 - val_loss: 0.4795 - val_acc: 0.8687\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0987 - acc: 0.9658 - val_loss: 0.4389 - val_acc: 0.8817\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0943 - acc: 0.9671 - val_loss: 0.4108 - val_acc: 0.8868\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0927 - acc: 0.9671 - val_loss: 0.5345 - val_acc: 0.8690\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0804 - acc: 0.9717 - val_loss: 0.3651 - val_acc: 0.8993 - ETA: 5s - l\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0816 - acc: 0.9713 - val_loss: 0.3948 - val_acc: 0.8994\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0792 - acc: 0.9720 - val_loss: 0.5091 - val_acc: 0.8813\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0723 - acc: 0.9753 - val_loss: 0.4455 - val_acc: 0.8846\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0651 - acc: 0.9772 - val_loss: 0.5430 - val_acc: 0.8739\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0644 - acc: 0.9771 - val_loss: 0.3908 - val_acc: 0.9048\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0574 - acc: 0.9798 - val_loss: 0.3982 - val_acc: 0.9026\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0588 - acc: 0.9794 - val_loss: 0.5335 - val_acc: 0.8817\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0580 - acc: 0.9793 - val_loss: 0.4102 - val_acc: 0.9007\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0588 - acc: 0.9792 - val_loss: 0.4370 - val_acc: 0.8982\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0513 - acc: 0.9825 - val_loss: 0.4138 - val_acc: 0.9015\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0498 - acc: 0.9826 - val_loss: 0.4726 - val_acc: 0.8980\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0529 - acc: 0.9813 - val_loss: 0.3882 - val_acc: 0.9052\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0457 - acc: 0.9842 - val_loss: 0.3952 - val_acc: 0.9087\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0455 - acc: 0.9837 - val_loss: 0.4690 - val_acc: 0.8897\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0423 - acc: 0.9853 - val_loss: 0.4509 - val_acc: 0.90022s - loss: 0.0426\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0370 - acc: 0.9874 - val_loss: 0.4604 - val_acc: 0.9037\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0371 - acc: 0.9869 - val_loss: 0.4311 - val_acc: 0.9040\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0361 - acc: 0.9875 - val_loss: 0.4106 - val_acc: 0.9054\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0386 - acc: 0.9868 - val_loss: 0.4058 - val_acc: 0.9075 - acc:\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0337 - acc: 0.9884 - val_loss: 0.4467 - val_acc: 0.9062\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0328 - acc: 0.9890 - val_loss: 0.3917 - val_acc: 0.91050328 - acc: 0.989\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0339 - acc: 0.9882 - val_loss: 0.5694 - val_acc: 0.8897\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0327 - acc: 0.9884 - val_loss: 0.5454 - val_acc: 0.8922\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.5186 - val_acc: 0.8965\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.4447 - val_acc: 0.9091\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0265 - acc: 0.9910 - val_loss: 0.4078 - val_acc: 0.9092\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0317 - acc: 0.9894 - val_loss: 0.4675 - val_acc: 0.9020\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0293 - acc: 0.9897 - val_loss: 0.4408 - val_acc: 0.9063\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0255 - acc: 0.9912 - val_loss: 0.4962 - val_acc: 0.8996\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0206 - acc: 0.9931 - val_loss: 0.4811 - val_acc: 0.9026\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.3316 - val_acc: 0.9266\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0081 - acc: 0.9978 - val_loss: 0.3362 - val_acc: 0.9273\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.3329 - val_acc: 0.9292\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.3365 - val_acc: 0.9285 - ETA: 2s - loss: 0.0063 \n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.3345 - val_acc: 0.9311\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.3363 - val_acc: 0.9289\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.3397 - val_acc: 0.9293\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.3414 - val_acc: 0.9291\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.3444 - val_acc: 0.9290\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.3405 - val_acc: 0.9299\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0033 - acc: 0.9993 - val_loss: 0.3461 - val_acc: 0.9303\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.3457 - val_acc: 0.9298\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.3487 - val_acc: 0.9290\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.3485 - val_acc: 0.9293\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.3486 - val_acc: 0.9292 0\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.3472 - val_acc: 0.9304\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.3454 - val_acc: 0.9310\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.3450 - val_acc: 0.9311 - acc: 0.99\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.3447 - val_acc: 0.9307\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.3505 - val_acc: 0.9314\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.3518 - val_acc: 0.9302\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.3500 - val_acc: 0.9310 - loss: 0.0024 - acc: \n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.3500 - val_acc: 0.9310\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.3509 - val_acc: 0.9310\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.3511 - val_acc: 0.9313 - loss: 0.0021 \n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.3510 - val_acc: 0.9310\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.3524 - val_acc: 0.9314\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.3571 - val_acc: 0.9299\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.3579 - val_acc: 0.9300\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.3598 - val_acc: 0.9303\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.3638 - val_acc: 0.9303\n",
      "Epoch 92/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.3615 - val_acc: 0.9309\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.3608 - val_acc: 0.9314\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.3642 - val_acc: 0.9306\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.3609 - val_acc: 0.9317\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.3647 - val_acc: 0.9302 - loss: 0.0 - ETA: 9s - loss: 0.0017 -  -\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.3619 - val_acc: 0.9303\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.3574 - val_acc: 0.9315\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.3627 - val_acc: 0.9311\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3610 - val_acc: 0.9324loss: 0.0014 - \n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.3615 - val_acc: 0.9304- loss: 0.0012 - a\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3646 - val_acc: 0.9309\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.3636 - val_acc: 0.9310\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.3683 - val_acc: 0.9311\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.3657 - val_acc: 0.9317loss: 0 - ETA: 0s - loss: 0.0013 - acc: 0.999\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.3666 - val_acc: 0.9310\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.3708 - val_acc: 0.9312\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.3685 - val_acc: 0.9313\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3728 - val_acc: 0.9307\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.3730 - val_acc: 0.9308\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.3704 - val_acc: 0.9319\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3742 - val_acc: 0.9318\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.3701 - val_acc: 0.9308\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.3784 - val_acc: 0.9307\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.3796 - val_acc: 0.9304\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.3780 - val_acc: 0.9326\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3819 - val_acc: 0.9313\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.3781 - val_acc: 0.9314\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3785 - val_acc: 0.9306\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.4182e-04 - acc: 0.9999 - val_loss: 0.3761 - val_acc: 0.9304\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.3773 - val_acc: 0.9308\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3777 - val_acc: 0.9303\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3782 - val_acc: 0.9304\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.5589e-04 - acc: 0.9999 - val_loss: 0.3787 - val_acc: 0.9297\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.3788 - val_acc: 0.9301\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3789 - val_acc: 0.9306\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.3790 - val_acc: 0.9307\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3789 - val_acc: 0.9307\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.6576e-04 - acc: 0.9998 - val_loss: 0.3796 - val_acc: 0.9301\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.1980e-04 - acc: 0.9999 - val_loss: 0.3799 - val_acc: 0.9303\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.3792 - val_acc: 0.9305\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.3785 - val_acc: 0.9304\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3795 - val_acc: 0.9308\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.6498e-04 - acc: 0.9999 - val_loss: 0.3789 - val_acc: 0.9304\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.3796 - val_acc: 0.9303\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3805 - val_acc: 0.9306\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.4976e-04 - acc: 0.9998 - val_loss: 0.3802 - val_acc: 0.9307\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3797 - val_acc: 0.9306\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.3793 - val_acc: 0.9306\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.3802 - val_acc: 0.9313\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.8298e-04 - acc: 0.9999 - val_loss: 0.3789 - val_acc: 0.9312\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.8851e-04 - acc: 0.9998 - val_loss: 0.3784 - val_acc: 0.9319\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.0720e-04 - acc: 0.9999 - val_loss: 0.3792 - val_acc: 0.9310\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.3792 - val_acc: 0.9314\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.4206e-04 - acc: 0.9999 - val_loss: 0.3795 - val_acc: 0.9311 9.4788e-04 - acc: 0.\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3800 - val_acc: 0.9308\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.3788 - val_acc: 0.9311\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.6510e-04 - acc: 0.9999 - val_loss: 0.3796 - val_acc: 0.9308s:\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.9076e-04 - acc: 0.9999 - val_loss: 0.3792 - val_acc: 0.9310\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.6031e-04 - acc: 0.9998 - val_loss: 0.3790 - val_acc: 0.9307\n",
      "Epoch 151/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.3791 - val_acc: 0.9315\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.5410e-04 - acc: 0.9998 - val_loss: 0.3800 - val_acc: 0.9316\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.3807 - val_acc: 0.9310\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.8711e-04 - acc: 0.9998 - val_loss: 0.3804 - val_acc: 0.9310\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.0025e-04 - acc: 0.9999 - val_loss: 0.3789 - val_acc: 0.9313\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.9502e-04 - acc: 0.9999 - val_loss: 0.3801 - val_acc: 0.9314s - loss: 8.1 - ETA: 4s - loss: \n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.3802 - val_acc: 0.9308\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.3458e-04 - acc: 0.9999 - val_loss: 0.3799 - val_acc: 0.9311\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.5875e-04 - acc: 0.9998 - val_loss: 0.3794 - val_acc: 0.9310\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.3813 - val_acc: 0.9312\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.1188e-04 - acc: 0.9998 - val_loss: 0.3803 - val_acc: 0.9308\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.8421e-04 - acc: 0.9998 - val_loss: 0.3799 - val_acc: 0.9304: 6s\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.1928e-04 - acc: 0.9998 - val_loss: 0.3808 - val_acc: 0.9311s: 9.2051e-04\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.5813e-04 - acc: 0.9999 - val_loss: 0.3809 - val_acc: 0.9311\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.4276e-04 - acc: 0.9999 - val_loss: 0.3794 - val_acc: 0.9311\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.7191e-04 - acc: 0.9998 - val_loss: 0.3809 - val_acc: 0.9304\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.2776e-04 - acc: 0.9998 - val_loss: 0.3802 - val_acc: 0.9310\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.6868e-04 - acc: 0.9999 - val_loss: 0.3799 - val_acc: 0.9310\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.5157e-04 - acc: 0.9999 - val_loss: 0.3798 - val_acc: 0.9311\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.8560e-04 - acc: 0.9999 - val_loss: 0.3809 - val_acc: 0.9307\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.2088e-04 - acc: 0.9999 - val_loss: 0.3807 - val_acc: 0.9309\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.4976e-04 - acc: 0.9998 - val_loss: 0.3803 - val_acc: 0.9308\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.2926e-04 - acc: 0.9999 - val_loss: 0.3806 - val_acc: 0.9307\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 83s 213ms/step - loss: 7.2280e-04 - acc: 1.0000 - val_loss: 0.3799 - val_acc: 0.9310\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.8641e-04 - acc: 0.9998 - val_loss: 0.3809 - val_acc: 0.9308\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.5854e-04 - acc: 0.9999 - val_loss: 0.3800 - val_acc: 0.9313\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.5894e-04 - acc: 0.9999 - val_loss: 0.3812 - val_acc: 0.9308\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 6.9410e-04 - acc: 0.9999 - val_loss: 0.3808 - val_acc: 0.9311\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.7856e-04 - acc: 0.9999 - val_loss: 0.3809 - val_acc: 0.9308\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.3807 - val_acc: 0.9312\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.7851e-04 - acc: 0.9999 - val_loss: 0.3803 - val_acc: 0.9308\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.2230e-04 - acc: 0.9998 - val_loss: 0.3814 - val_acc: 0.9308\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.1863e-04 - acc: 0.9998 - val_loss: 0.3808 - val_acc: 0.9306\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.0177e-04 - acc: 0.9999 - val_loss: 0.3807 - val_acc: 0.9309\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.4509e-04 - acc: 0.9998 - val_loss: 0.3806 - val_acc: 0.9320\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.4911e-04 - acc: 0.9998 - val_loss: 0.3815 - val_acc: 0.9307\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.7173e-04 - acc: 0.9999 - val_loss: 0.3805 - val_acc: 0.9314\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.6929e-04 - acc: 0.9999 - val_loss: 0.3808 - val_acc: 0.9305\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.9845e-04 - acc: 0.9999 - val_loss: 0.3807 - val_acc: 0.9310\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.5282e-04 - acc: 0.9999 - val_loss: 0.3804 - val_acc: 0.9312\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.7162e-04 - acc: 0.9999 - val_loss: 0.3813 - val_acc: 0.9311\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 9.3609e-04 - acc: 0.9999 - val_loss: 0.3807 - val_acc: 0.9310\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.3169e-04 - acc: 0.9999 - val_loss: 0.3802 - val_acc: 0.9320- acc: - ETA: 0s - loss: 7.3469e-04 - acc: 0.9\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.2959e-04 - acc: 0.9999 - val_loss: 0.3807 - val_acc: 0.9314oss: 8.3\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.3054e-04 - acc: 0.9999 - val_loss: 0.3806 - val_acc: 0.9309\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.7947e-04 - acc: 0.9999 - val_loss: 0.3811 - val_acc: 0.9313\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.5708e-04 - acc: 0.9999 - val_loss: 0.3797 - val_acc: 0.9315\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.4902e-04 - acc: 0.9998 - val_loss: 0.3808 - val_acc: 0.9315\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 8.4994e-04 - acc: 0.9998 - val_loss: 0.3804 - val_acc: 0.9311\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 7.1145e-04 - acc: 0.9999 - val_loss: 0.3816 - val_acc: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.47752245745293254,\n",
       "  0.67280237411588362,\n",
       "  0.7477542508440137,\n",
       "  0.79134584538325459,\n",
       "  0.82314725693961843,\n",
       "  0.84201555987141785,\n",
       "  0.85767564962489273,\n",
       "  0.87133060637138426,\n",
       "  0.88282001926840059,\n",
       "  0.8929659929036865,\n",
       "  0.90004411294822095,\n",
       "  0.90776387555957949,\n",
       "  0.91357876162977225,\n",
       "  0.92041626567828339,\n",
       "  0.92611084372178076,\n",
       "  0.93096326599910473,\n",
       "  0.93419153033673552,\n",
       "  0.93876323387872951,\n",
       "  0.94241257619505936,\n",
       "  0.94634263715110678,\n",
       "  0.95121511070247178,\n",
       "  0.95283926856567513,\n",
       "  0.95540583892858366,\n",
       "  0.95843359000949779,\n",
       "  0.96047882577491028,\n",
       "  0.96290503685620488,\n",
       "  0.96585258259236295,\n",
       "  0.96717597048444015,\n",
       "  0.96709576513968409,\n",
       "  0.97168752007045389,\n",
       "  0.97128649346140816,\n",
       "  0.97196823872929394,\n",
       "  0.97531681107449764,\n",
       "  0.97722168753917382,\n",
       "  0.97706127688790656,\n",
       "  0.97976762820512819,\n",
       "  0.97938744384046539,\n",
       "  0.97930702598652553,\n",
       "  0.97920676932948347,\n",
       "  0.98253529036240128,\n",
       "  0.98261549568803486,\n",
       "  0.98129210779595766,\n",
       "  0.98423965353211573,\n",
       "  0.98363811357074105,\n",
       "  0.98530237409676125,\n",
       "  0.9873877125823578,\n",
       "  0.98694658327225049,\n",
       "  0.98754812321450258,\n",
       "  0.98680622393326922,\n",
       "  0.98839027913365562,\n",
       "  0.98899181905678535,\n",
       "  0.98822986842502103,\n",
       "  0.98835017645171641,\n",
       "  0.98972355769230769,\n",
       "  0.98978403987758956,\n",
       "  0.99100560897435896,\n",
       "  0.98934248554913296,\n",
       "  0.98965351301238524,\n",
       "  0.99119746551170995,\n",
       "  0.99316249602797857,\n",
       "  0.99566891241578437,\n",
       "  0.99775425088225855,\n",
       "  0.9981953801732435,\n",
       "  0.99823717948717949,\n",
       "  0.99855491329479773,\n",
       "  0.99889717677253764,\n",
       "  0.99891722810394612,\n",
       "  0.99897738211729381,\n",
       "  0.99897738209817133,\n",
       "  0.9989974334295797,\n",
       "  0.99925881410256412,\n",
       "  0.99917710340398203,\n",
       "  0.99915784408084696,\n",
       "  0.99947866538338148,\n",
       "  0.9993984600577478,\n",
       "  0.99933830606352259,\n",
       "  0.99933830606352259,\n",
       "  0.99935835739493106,\n",
       "  0.99935835739493106,\n",
       "  0.999458614051973,\n",
       "  0.9993984600577478,\n",
       "  0.99949871671478985,\n",
       "  0.9993984600577478,\n",
       "  0.99961902470324027,\n",
       "  0.99949871671478985,\n",
       "  0.99951876804619821,\n",
       "  0.99965912736605711,\n",
       "  0.99957932692307694,\n",
       "  0.99969894026974948,\n",
       "  0.99931825473211422,\n",
       "  0.99943856272056464,\n",
       "  0.9995989733718319,\n",
       "  0.99967948717948718,\n",
       "  0.99955844572896591,\n",
       "  0.99971928136028232,\n",
       "  0.99969923002887395,\n",
       "  0.99957892204042353,\n",
       "  0.99961902470324027,\n",
       "  0.99977943535450753,\n",
       "  0.99965912736605711,\n",
       "  0.99983958934873274,\n",
       "  0.99971928136028232,\n",
       "  0.9995789220595459,\n",
       "  0.99975938402309916,\n",
       "  0.99983974358974359,\n",
       "  0.99979929351316632,\n",
       "  0.99961939102564101,\n",
       "  0.99973908156711622,\n",
       "  0.99965912736605711,\n",
       "  0.99979967948717952,\n",
       "  0.99981936416184969,\n",
       "  0.99973933269169069,\n",
       "  0.99977943535450753,\n",
       "  0.99975938402309916,\n",
       "  0.99965912736605711,\n",
       "  0.99971928136028232,\n",
       "  0.99969923002887395,\n",
       "  0.99971928136028232,\n",
       "  0.99967917869746548,\n",
       "  0.99989983974358976,\n",
       "  0.99961865767501601,\n",
       "  0.99981953801732437,\n",
       "  0.99983974358974359,\n",
       "  0.99987969201154958,\n",
       "  0.99973933269169069,\n",
       "  0.99973908158625713,\n",
       "  0.99989974334295795,\n",
       "  0.99979948668591589,\n",
       "  0.99977943535450753,\n",
       "  0.99985964068014122,\n",
       "  0.99975961538461533,\n",
       "  0.99975938402309916,\n",
       "  0.99983943481053306,\n",
       "  0.99987969201154958,\n",
       "  0.99973958333333335,\n",
       "  0.99977943535450753,\n",
       "  0.99983943481053306,\n",
       "  0.99979948668591589,\n",
       "  0.99979967948717952,\n",
       "  0.99981953801732437,\n",
       "  0.99985950545921642,\n",
       "  0.99983958934873274,\n",
       "  0.99989974334295795,\n",
       "  0.99973958333333335,\n",
       "  0.99985950545921642,\n",
       "  0.99969923002887395,\n",
       "  0.99973958333333335,\n",
       "  0.99985950545921642,\n",
       "  0.99985964068014122,\n",
       "  0.99975938402309916,\n",
       "  0.99983958934873274,\n",
       "  0.99983974358974359,\n",
       "  0.99973933269169069,\n",
       "  0.99981936416184969,\n",
       "  0.99991987179487174,\n",
       "  0.99987969201154958,\n",
       "  0.99977943535450753,\n",
       "  0.99987969201154958,\n",
       "  0.99979929351316632,\n",
       "  0.99973933269169069,\n",
       "  0.99979967948717952,\n",
       "  0.99985964069926359,\n",
       "  0.99981953801732437,\n",
       "  0.99985964068014122,\n",
       "  0.99989964675658316,\n",
       "  0.99981953801732437,\n",
       "  0.99983974358974359,\n",
       "  0.99985964068014122,\n",
       "  0.99987969201154958,\n",
       "  0.99985964068014122,\n",
       "  0.99987957610789979,\n",
       "  0.99977943535450753,\n",
       "  0.99991979467436642,\n",
       "  0.99995989733718316,\n",
       "  0.99977964743589742,\n",
       "  0.99987969201154958,\n",
       "  0.99987957610789979,\n",
       "  0.99993990384615383,\n",
       "  0.99985950545921642,\n",
       "  0.99975938402309916,\n",
       "  0.99991979467436642,\n",
       "  0.99981953801732437,\n",
       "  0.99981953801732437,\n",
       "  0.99987980769230766,\n",
       "  0.99979929351316632,\n",
       "  0.99983974358974359,\n",
       "  0.99989964675658316,\n",
       "  0.99985964068014122,\n",
       "  0.99991979467436642,\n",
       "  0.99985977564102568,\n",
       "  0.99987957610789979,\n",
       "  0.99985964068014122,\n",
       "  0.99991979467436642,\n",
       "  0.99985977564102568,\n",
       "  0.99987957610789979,\n",
       "  0.99989974334295795,\n",
       "  0.99991979467436642,\n",
       "  0.9998197115384615,\n",
       "  0.99979948668591589,\n",
       "  0.99991979467436642],\n",
       " 'loss': [1.4279177980924747,\n",
       "  0.92662271280720099,\n",
       "  0.71782371630299957,\n",
       "  0.59905284899413913,\n",
       "  0.51266654625052166,\n",
       "  0.45604352933648384,\n",
       "  0.40938446112156374,\n",
       "  0.3730532797743657,\n",
       "  0.33585592246721063,\n",
       "  0.30991501409654859,\n",
       "  0.28914599959613957,\n",
       "  0.26474920016548209,\n",
       "  0.24757829991658989,\n",
       "  0.23134713278850913,\n",
       "  0.21104752612985936,\n",
       "  0.19734963247857876,\n",
       "  0.18597749038636779,\n",
       "  0.17309186674527799,\n",
       "  0.16455307833307783,\n",
       "  0.15181721269338605,\n",
       "  0.13834458845794526,\n",
       "  0.13416691867824085,\n",
       "  0.12611092257507345,\n",
       "  0.11943132102365175,\n",
       "  0.11124125831032319,\n",
       "  0.10418585753370103,\n",
       "  0.098676853940813519,\n",
       "  0.094286698014987833,\n",
       "  0.092639226283558987,\n",
       "  0.080413919015117366,\n",
       "  0.081664956571880276,\n",
       "  0.079182690676967413,\n",
       "  0.072274046811703352,\n",
       "  0.065081068535416328,\n",
       "  0.064434013818562466,\n",
       "  0.05737466222057358,\n",
       "  0.058814934313694445,\n",
       "  0.058069451718944023,\n",
       "  0.058670770504802017,\n",
       "  0.05129468193015381,\n",
       "  0.049828719494121557,\n",
       "  0.052972428045896795,\n",
       "  0.045647890344955311,\n",
       "  0.04551278942085045,\n",
       "  0.042312283458515051,\n",
       "  0.036942591327775887,\n",
       "  0.037098249259775631,\n",
       "  0.036121502744537688,\n",
       "  0.038623550531424806,\n",
       "  0.033676616834368428,\n",
       "  0.032826810562490157,\n",
       "  0.033857307821108261,\n",
       "  0.032726043870838113,\n",
       "  0.029942640708759426,\n",
       "  0.029848178007622597,\n",
       "  0.026463953809490286,\n",
       "  0.03173737489847453,\n",
       "  0.029242198616098929,\n",
       "  0.025471317547471777,\n",
       "  0.020552079356133575,\n",
       "  0.013224520668851132,\n",
       "  0.0081443648260531368,\n",
       "  0.0066151451624219988,\n",
       "  0.0062734036823176445,\n",
       "  0.0050779281191703704,\n",
       "  0.0046411565348701981,\n",
       "  0.004475889705512332,\n",
       "  0.004436968726970681,\n",
       "  0.0041135685323947608,\n",
       "  0.0037591276178465137,\n",
       "  0.0032821462349369168,\n",
       "  0.0033833090478809181,\n",
       "  0.003344072934658705,\n",
       "  0.0029069968297710672,\n",
       "  0.0027971564710153971,\n",
       "  0.00266224009449554,\n",
       "  0.002971807730241948,\n",
       "  0.0027446210542203722,\n",
       "  0.0028621300101148149,\n",
       "  0.0024764868233406383,\n",
       "  0.0025568274748950831,\n",
       "  0.0024107646519472997,\n",
       "  0.002404580130776642,\n",
       "  0.0020206856468944824,\n",
       "  0.002083068358884299,\n",
       "  0.002049948777811483,\n",
       "  0.001915410128058307,\n",
       "  0.0019856467487326694,\n",
       "  0.0017013104981144648,\n",
       "  0.0022571021857912561,\n",
       "  0.002036594993127543,\n",
       "  0.0017932052215189878,\n",
       "  0.0016666595418708032,\n",
       "  0.001960368790759782,\n",
       "  0.0016241209622913835,\n",
       "  0.0016233770761534005,\n",
       "  0.0018272377950753951,\n",
       "  0.0015979882315211153,\n",
       "  0.0013716101430334168,\n",
       "  0.0014243242590794836,\n",
       "  0.0012324809407381198,\n",
       "  0.0014452137538020667,\n",
       "  0.0015811285049939737,\n",
       "  0.0014655739560398074,\n",
       "  0.0013030472151000048,\n",
       "  0.001433652547456823,\n",
       "  0.0015391320486882111,\n",
       "  0.0013503743540873591,\n",
       "  0.0014443645208641824,\n",
       "  0.0013024183013700308,\n",
       "  0.0011095935216741176,\n",
       "  0.0014137395635077817,\n",
       "  0.001271978703594967,\n",
       "  0.0012018380463558059,\n",
       "  0.0013333944572045198,\n",
       "  0.0012520176428964954,\n",
       "  0.0013747848765772938,\n",
       "  0.0014076091268900297,\n",
       "  0.0011623362214370986,\n",
       "  0.00084182003824352224,\n",
       "  0.0012879369846274047,\n",
       "  0.0010446084142381499,\n",
       "  0.0010024775514373173,\n",
       "  0.00095650037974468773,\n",
       "  0.0010277839900387675,\n",
       "  0.0011580661265816921,\n",
       "  0.0010018521493429199,\n",
       "  0.0010426708176871522,\n",
       "  0.00096576819022859567,\n",
       "  0.00092042525328809612,\n",
       "  0.001081336243908318,\n",
       "  0.0012133955903973757,\n",
       "  0.0010376667432526563,\n",
       "  0.00096503898636616484,\n",
       "  0.0010267827556442802,\n",
       "  0.0010146020640595793,\n",
       "  0.00094745828547364243,\n",
       "  0.0010069521955531678,\n",
       "  0.0011585141659359065,\n",
       "  0.0010719016544822033,\n",
       "  0.00098442286863511023,\n",
       "  0.00098894794180355529,\n",
       "  0.00090804130365109358,\n",
       "  0.0010436325328415617,\n",
       "  0.00094317005711971609,\n",
       "  0.0012501444279750763,\n",
       "  0.0010838749985571036,\n",
       "  0.00086605286289566645,\n",
       "  0.00079096918578721947,\n",
       "  0.00096098725185627233,\n",
       "  0.001079292260826151,\n",
       "  0.00095409941524662715,\n",
       "  0.0010636498263757257,\n",
       "  0.00088850114485968391,\n",
       "  0.0008002492471113771,\n",
       "  0.00079564323907467527,\n",
       "  0.0010840751831215469,\n",
       "  0.00073501759042799352,\n",
       "  0.00085923553375117918,\n",
       "  0.0010626934790540993,\n",
       "  0.00091188268184524556,\n",
       "  0.00077630942812416392,\n",
       "  0.00091917266836576638,\n",
       "  0.00085772134138327688,\n",
       "  0.00084415030019392495,\n",
       "  0.0008721247745409592,\n",
       "  0.00082775510943461903,\n",
       "  0.00096935468332785672,\n",
       "  0.00075187186526240229,\n",
       "  0.00088550045501368233,\n",
       "  0.00082083917417970686,\n",
       "  0.00094980632732395991,\n",
       "  0.00082963627609048568,\n",
       "  0.00072331674351370514,\n",
       "  0.00098641215203408659,\n",
       "  0.00075866788024536789,\n",
       "  0.00085958072706843463,\n",
       "  0.0006940952875037254,\n",
       "  0.00087950016771190921,\n",
       "  0.0010400625262321119,\n",
       "  0.00077841330795388412,\n",
       "  0.00082293267439672723,\n",
       "  0.00091939201155021748,\n",
       "  0.00070176648912773738,\n",
       "  0.00094631865771403463,\n",
       "  0.00084911017052385993,\n",
       "  0.00077300056545002353,\n",
       "  0.00086830542332821134,\n",
       "  0.00079913267454084335,\n",
       "  0.00085282490346379257,\n",
       "  0.00077286740931916016,\n",
       "  0.00093570493206141718,\n",
       "  0.0007313798409034365,\n",
       "  0.00082958675631557407,\n",
       "  0.00082943811318068482,\n",
       "  0.00077954567823466275,\n",
       "  0.00075758452995612743,\n",
       "  0.00084902376175028998,\n",
       "  0.00085020951956699077,\n",
       "  0.00071125615269461407],\n",
       " 'val_acc': [0.5302,\n",
       "  0.63959999999999995,\n",
       "  0.69240000000000002,\n",
       "  0.74990000000000001,\n",
       "  0.76270000000000004,\n",
       "  0.78849999999999998,\n",
       "  0.79579999999999995,\n",
       "  0.84540000000000004,\n",
       "  0.84279999999999999,\n",
       "  0.83450000000000002,\n",
       "  0.83230000000000004,\n",
       "  0.84640000000000004,\n",
       "  0.82199999999999995,\n",
       "  0.83709999999999996,\n",
       "  0.85950000000000004,\n",
       "  0.85560000000000003,\n",
       "  0.82989999999999997,\n",
       "  0.85540000000000005,\n",
       "  0.87139999999999995,\n",
       "  0.8639,\n",
       "  0.85709999999999997,\n",
       "  0.86229999999999996,\n",
       "  0.88070000000000004,\n",
       "  0.87139999999999995,\n",
       "  0.88539999999999996,\n",
       "  0.86870000000000003,\n",
       "  0.88170000000000004,\n",
       "  0.88680000000000003,\n",
       "  0.86899999999999999,\n",
       "  0.89929999999999999,\n",
       "  0.89939999999999998,\n",
       "  0.88129999999999997,\n",
       "  0.88460000000000005,\n",
       "  0.87390000000000001,\n",
       "  0.90480000000000005,\n",
       "  0.90259999999999996,\n",
       "  0.88170000000000004,\n",
       "  0.90069999999999995,\n",
       "  0.8982,\n",
       "  0.90149999999999997,\n",
       "  0.89800000000000002,\n",
       "  0.9052,\n",
       "  0.90869999999999995,\n",
       "  0.88970000000000005,\n",
       "  0.9002,\n",
       "  0.90369999999999995,\n",
       "  0.90400000000000003,\n",
       "  0.90539999999999998,\n",
       "  0.90749999999999997,\n",
       "  0.90620000000000001,\n",
       "  0.91049999999999998,\n",
       "  0.88970000000000005,\n",
       "  0.89219999999999999,\n",
       "  0.89649999999999996,\n",
       "  0.90910000000000002,\n",
       "  0.90920000000000001,\n",
       "  0.90200000000000002,\n",
       "  0.90629999999999999,\n",
       "  0.89959999999999996,\n",
       "  0.90259999999999996,\n",
       "  0.92659999999999998,\n",
       "  0.92730000000000001,\n",
       "  0.92920000000000003,\n",
       "  0.92849999999999999,\n",
       "  0.93110000000000004,\n",
       "  0.92889999999999995,\n",
       "  0.92930000000000001,\n",
       "  0.92910000000000004,\n",
       "  0.92900000000000005,\n",
       "  0.92989999999999995,\n",
       "  0.93030000000000002,\n",
       "  0.92979999999999996,\n",
       "  0.92900000000000005,\n",
       "  0.92930000000000001,\n",
       "  0.92920000000000003,\n",
       "  0.9304,\n",
       "  0.93100000000000005,\n",
       "  0.93110000000000004,\n",
       "  0.93069999999999997,\n",
       "  0.93140000000000001,\n",
       "  0.93020000000000003,\n",
       "  0.93100000000000005,\n",
       "  0.93100000000000005,\n",
       "  0.93100000000000005,\n",
       "  0.93130000000000002,\n",
       "  0.93100000000000005,\n",
       "  0.93140000000000001,\n",
       "  0.92989999999999995,\n",
       "  0.93000000000000005,\n",
       "  0.93030000000000002,\n",
       "  0.93030000000000002,\n",
       "  0.93089999999999995,\n",
       "  0.93140000000000001,\n",
       "  0.93059999999999998,\n",
       "  0.93169999999999997,\n",
       "  0.93020000000000003,\n",
       "  0.93030000000000002,\n",
       "  0.93149999999999999,\n",
       "  0.93110000000000004,\n",
       "  0.93240000000000001,\n",
       "  0.9304,\n",
       "  0.93089999999999995,\n",
       "  0.93100000000000005,\n",
       "  0.93110000000000004,\n",
       "  0.93169999999999997,\n",
       "  0.93100000000000005,\n",
       "  0.93120000000000003,\n",
       "  0.93130000000000002,\n",
       "  0.93069999999999997,\n",
       "  0.93079999999999996,\n",
       "  0.93189999999999995,\n",
       "  0.93179999999999996,\n",
       "  0.93079999999999996,\n",
       "  0.93069999999999997,\n",
       "  0.9304,\n",
       "  0.93259999999999998,\n",
       "  0.93130000000000002,\n",
       "  0.93140000000000001,\n",
       "  0.93059999999999998,\n",
       "  0.9304,\n",
       "  0.93079999999999996,\n",
       "  0.93030000000000002,\n",
       "  0.9304,\n",
       "  0.92969999999999997,\n",
       "  0.93010000000000004,\n",
       "  0.93059999999999998,\n",
       "  0.93069999999999997,\n",
       "  0.93069999999999997,\n",
       "  0.93010000000000004,\n",
       "  0.93030000000000002,\n",
       "  0.93049999999999999,\n",
       "  0.9304,\n",
       "  0.93079999999999996,\n",
       "  0.9304,\n",
       "  0.93030000000000002,\n",
       "  0.93059999999999998,\n",
       "  0.93069999999999997,\n",
       "  0.93059999999999998,\n",
       "  0.93059999999999998,\n",
       "  0.93130000000000002,\n",
       "  0.93120000000000003,\n",
       "  0.93189999999999995,\n",
       "  0.93100000000000005,\n",
       "  0.93140000000000001,\n",
       "  0.93110000000000004,\n",
       "  0.93079999999999996,\n",
       "  0.93110000000000004,\n",
       "  0.93079999999999996,\n",
       "  0.93100000000000005,\n",
       "  0.93069999999999997,\n",
       "  0.93149999999999999,\n",
       "  0.93159999999999998,\n",
       "  0.93100000000000005,\n",
       "  0.93100000000000005,\n",
       "  0.93130000000000002,\n",
       "  0.93140000000000001,\n",
       "  0.93079999999999996,\n",
       "  0.93110000000000004,\n",
       "  0.93100000000000005,\n",
       "  0.93120000000000003,\n",
       "  0.93079999999999996,\n",
       "  0.9304,\n",
       "  0.93110000000000004,\n",
       "  0.93110000000000004,\n",
       "  0.93110000000000004,\n",
       "  0.9304,\n",
       "  0.93100000000000005,\n",
       "  0.93100000000000005,\n",
       "  0.93110000000000004,\n",
       "  0.93069999999999997,\n",
       "  0.93089999999999995,\n",
       "  0.93079999999999996,\n",
       "  0.93069999999999997,\n",
       "  0.93100000000000005,\n",
       "  0.93079999999999996,\n",
       "  0.93130000000000002,\n",
       "  0.93079999999999996,\n",
       "  0.93110000000000004,\n",
       "  0.93079999999999996,\n",
       "  0.93120000000000003,\n",
       "  0.93079999999999996,\n",
       "  0.93079999999999996,\n",
       "  0.93059999999999998,\n",
       "  0.93089999999999995,\n",
       "  0.93200000000000005,\n",
       "  0.93069999999999997,\n",
       "  0.93140000000000001,\n",
       "  0.93049999999999999,\n",
       "  0.93100000000000005,\n",
       "  0.93120000000000003,\n",
       "  0.93110000000000004,\n",
       "  0.93100000000000005,\n",
       "  0.93200000000000005,\n",
       "  0.93140000000000001,\n",
       "  0.93089999999999995,\n",
       "  0.93130000000000002,\n",
       "  0.93149999999999999,\n",
       "  0.93149999999999999,\n",
       "  0.93110000000000004,\n",
       "  0.93100000000000005],\n",
       " 'val_loss': [1.5331891643524169,\n",
       "  1.1898214042663575,\n",
       "  0.89580021114349362,\n",
       "  0.76373467102050785,\n",
       "  0.72177741746902468,\n",
       "  0.65120959939956669,\n",
       "  0.62355886383056636,\n",
       "  0.45687409687042235,\n",
       "  0.47613129539489746,\n",
       "  0.51820390262603755,\n",
       "  0.50988347206115725,\n",
       "  0.48956357479095458,\n",
       "  0.56279831180572515,\n",
       "  0.50332908191680903,\n",
       "  0.42151539011001588,\n",
       "  0.47405318832397458,\n",
       "  0.63313326301574702,\n",
       "  0.4997737225532532,\n",
       "  0.43056681041717532,\n",
       "  0.5074576047897339,\n",
       "  0.53147386732101443,\n",
       "  0.47501532363891602,\n",
       "  0.44475707678794862,\n",
       "  0.44605023975372315,\n",
       "  0.42684454650878906,\n",
       "  0.47948061876296999,\n",
       "  0.43888839082717895,\n",
       "  0.4108235420227051,\n",
       "  0.53453350491523743,\n",
       "  0.36514791364669802,\n",
       "  0.39481929044723513,\n",
       "  0.5090801068305969,\n",
       "  0.44553938798904419,\n",
       "  0.54302408256530765,\n",
       "  0.39079011116027834,\n",
       "  0.39822897958755493,\n",
       "  0.53350080900192265,\n",
       "  0.41017224731445312,\n",
       "  0.4369791127204895,\n",
       "  0.41382304306030271,\n",
       "  0.47255212779045103,\n",
       "  0.38822202596664429,\n",
       "  0.39518536338806154,\n",
       "  0.46897975492477417,\n",
       "  0.45085573139190671,\n",
       "  0.46042697763442991,\n",
       "  0.43112707748413087,\n",
       "  0.41060457572937009,\n",
       "  0.40577825412750246,\n",
       "  0.44674341459274292,\n",
       "  0.39168561515808104,\n",
       "  0.56944017835855487,\n",
       "  0.54542245750427243,\n",
       "  0.51859165554046627,\n",
       "  0.44471184854507445,\n",
       "  0.40784555196762085,\n",
       "  0.46752697916030883,\n",
       "  0.44084387867450714,\n",
       "  0.49619859380722048,\n",
       "  0.48105568466186521,\n",
       "  0.33161912918090819,\n",
       "  0.33616851987838747,\n",
       "  0.33287070055007933,\n",
       "  0.33647112302780152,\n",
       "  0.33452913951873781,\n",
       "  0.33627153244018554,\n",
       "  0.33973195486068725,\n",
       "  0.34140329351425169,\n",
       "  0.34435432872772215,\n",
       "  0.34054055433273317,\n",
       "  0.3461036082267761,\n",
       "  0.34572747220993044,\n",
       "  0.34869038391113283,\n",
       "  0.34853731470108035,\n",
       "  0.34856403226852417,\n",
       "  0.34721797151565553,\n",
       "  0.34538197717666624,\n",
       "  0.34500877637863159,\n",
       "  0.34465638132095339,\n",
       "  0.35050230712890623,\n",
       "  0.35175072412490843,\n",
       "  0.34995216560363768,\n",
       "  0.34995159931182862,\n",
       "  0.35087408714294432,\n",
       "  0.35109290056228637,\n",
       "  0.35104395093917845,\n",
       "  0.35244492387771609,\n",
       "  0.35711162328720092,\n",
       "  0.35786398801803587,\n",
       "  0.35984035005569459,\n",
       "  0.36384128465652466,\n",
       "  0.36154878797531126,\n",
       "  0.36084973182678221,\n",
       "  0.36415518312454226,\n",
       "  0.36093462562561035,\n",
       "  0.36469076795578004,\n",
       "  0.3619468189239502,\n",
       "  0.35740743551254273,\n",
       "  0.36269870567321777,\n",
       "  0.36096559925079347,\n",
       "  0.36152607297897338,\n",
       "  0.3646086501121521,\n",
       "  0.36355143833160403,\n",
       "  0.3683331120491028,\n",
       "  0.36570913267135619,\n",
       "  0.36657622919082644,\n",
       "  0.37082031183242797,\n",
       "  0.36845822629928587,\n",
       "  0.37276764974594118,\n",
       "  0.37296383171081543,\n",
       "  0.37039564685821535,\n",
       "  0.37420791320800784,\n",
       "  0.37008118839263915,\n",
       "  0.37842775812149049,\n",
       "  0.37959349422454836,\n",
       "  0.37802066602706907,\n",
       "  0.38186451215744016,\n",
       "  0.37808922071456907,\n",
       "  0.37853816719055178,\n",
       "  0.3761211660385132,\n",
       "  0.37729204225540164,\n",
       "  0.3777048666000366,\n",
       "  0.37818607463836668,\n",
       "  0.37866782178878783,\n",
       "  0.37883987579345701,\n",
       "  0.37890461645126344,\n",
       "  0.37896386661529541,\n",
       "  0.37887107963562011,\n",
       "  0.37959022550582888,\n",
       "  0.37989895915985106,\n",
       "  0.37922385892868044,\n",
       "  0.37848504104614256,\n",
       "  0.37949915971755982,\n",
       "  0.37890056161880492,\n",
       "  0.37962710523605347,\n",
       "  0.38049924707412719,\n",
       "  0.38021789817810059,\n",
       "  0.37967879276275635,\n",
       "  0.37926750955581667,\n",
       "  0.38021194686889648,\n",
       "  0.37894750194549559,\n",
       "  0.37840765399932863,\n",
       "  0.37920223360061645,\n",
       "  0.37919028100967406,\n",
       "  0.37949523525238038,\n",
       "  0.38000416173934937,\n",
       "  0.37882432365417479,\n",
       "  0.37963730535507201,\n",
       "  0.37923703794479369,\n",
       "  0.37904362373352052,\n",
       "  0.37905228366851806,\n",
       "  0.3799889759063721,\n",
       "  0.38072412710189818,\n",
       "  0.38039012403488159,\n",
       "  0.37888699226379396,\n",
       "  0.38006678581237791,\n",
       "  0.38017985610961913,\n",
       "  0.37990101718902586,\n",
       "  0.37938810043334958,\n",
       "  0.38128434457778931,\n",
       "  0.38031468734741208,\n",
       "  0.37991172723770139,\n",
       "  0.38084347486495973,\n",
       "  0.38086237440109255,\n",
       "  0.3794483430862427,\n",
       "  0.38086937446594238,\n",
       "  0.38015368328094484,\n",
       "  0.37988611984252929,\n",
       "  0.37984223356246949,\n",
       "  0.38088471174240113,\n",
       "  0.38074554672241212,\n",
       "  0.38025354890823365,\n",
       "  0.38058104553222655,\n",
       "  0.37993108453750613,\n",
       "  0.38092233324050906,\n",
       "  0.37998929672241211,\n",
       "  0.38119682569503782,\n",
       "  0.38082470216751096,\n",
       "  0.380948047542572,\n",
       "  0.38067457056045534,\n",
       "  0.38029530944824219,\n",
       "  0.38140963678359985,\n",
       "  0.38084284305572508,\n",
       "  0.38073707752227781,\n",
       "  0.38055063571929931,\n",
       "  0.38145521440505981,\n",
       "  0.38049152765274047,\n",
       "  0.3807650526046753,\n",
       "  0.38074719886779784,\n",
       "  0.38037763414382936,\n",
       "  0.38131639842987058,\n",
       "  0.38074513511657715,\n",
       "  0.38017935829162597,\n",
       "  0.38074854936599734,\n",
       "  0.38057672796249392,\n",
       "  0.38114513530731203,\n",
       "  0.37974793853759764,\n",
       "  0.38077914381027222,\n",
       "  0.38041740751266478,\n",
       "  0.38161190509796145]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16-4 - no dropout\n",
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 1.5553 - acc: 0.4277 - val_loss: 2.6719 - val_acc: 0.3779\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 1.0312 - acc: 0.6317 - val_loss: 1.1908 - val_acc: 0.6051\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.8176 - acc: 0.7095 - val_loss: 1.5804 - val_acc: 0.5409\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.6790 - acc: 0.7633 - val_loss: 0.8938 - val_acc: 0.7101\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 70s 181ms/step - loss: 0.5883 - acc: 0.7950 - val_loss: 0.9713 - val_acc: 0.6967\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.5241 - acc: 0.8199 - val_loss: 0.7080 - val_acc: 0.7622\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.4772 - acc: 0.8358 - val_loss: 0.6977 - val_acc: 0.7877\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.4331 - acc: 0.8491 - val_loss: 0.6823 - val_acc: 0.7973\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 70s 181ms/step - loss: 0.3972 - acc: 0.8624 - val_loss: 0.8889 - val_acc: 0.7569\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 71s 181ms/step - loss: 0.3736 - acc: 0.8703 - val_loss: 0.6395 - val_acc: 0.8012\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.3524 - acc: 0.8787 - val_loss: 0.5963 - val_acc: 0.8244\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.3266 - acc: 0.8860 - val_loss: 0.6120 - val_acc: 0.8165\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.3102 - acc: 0.8909 - val_loss: 0.5673 - val_acc: 0.8423\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.2843 - acc: 0.9012 - val_loss: 0.6078 - val_acc: 0.8261\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 70s 181ms/step - loss: 0.2769 - acc: 0.9042 - val_loss: 0.9822 - val_acc: 0.7684\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.2573 - acc: 0.9093 - val_loss: 0.6500 - val_acc: 0.8211\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.2529 - acc: 0.9115 - val_loss: 0.4621 - val_acc: 0.8668\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.2280 - acc: 0.9208 - val_loss: 0.4689 - val_acc: 0.8670\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.2252 - acc: 0.9217 - val_loss: 0.4986 - val_acc: 0.8568\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.2089 - acc: 0.9261 - val_loss: 0.5519 - val_acc: 0.8520\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.2007 - acc: 0.9287 - val_loss: 0.4436 - val_acc: 0.8712\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1938 - acc: 0.9304 - val_loss: 0.4751 - val_acc: 0.8654\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1825 - acc: 0.9356 - val_loss: 0.4551 - val_acc: 0.8694\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1697 - acc: 0.9400 - val_loss: 0.5251 - val_acc: 0.8585\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1652 - acc: 0.9425 - val_loss: 0.4744 - val_acc: 0.8728\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 70s 181ms/step - loss: 0.1628 - acc: 0.9424 - val_loss: 0.4530 - val_acc: 0.8812\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.1497 - acc: 0.9470 - val_loss: 0.4386 - val_acc: 0.8792\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.1472 - acc: 0.9475 - val_loss: 0.4155 - val_acc: 0.8826\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.1385 - acc: 0.9508 - val_loss: 0.5678 - val_acc: 0.8604\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 70s 180ms/step - loss: 0.1352 - acc: 0.9523 - val_loss: 0.5481 - val_acc: 0.8693\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1311 - acc: 0.9530 - val_loss: 0.4554 - val_acc: 0.8776\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 71s 182ms/step - loss: 0.1226 - acc: 0.9560 - val_loss: 0.6402 - val_acc: 0.8551\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1194 - acc: 0.9571 - val_loss: 0.5120 - val_acc: 0.8687\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1159 - acc: 0.9601 - val_loss: 0.5309 - val_acc: 0.8718\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1115 - acc: 0.9597 - val_loss: 0.5260 - val_acc: 0.8717\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1062 - acc: 0.9626 - val_loss: 0.5275 - val_acc: 0.8625\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.1020 - acc: 0.9642 - val_loss: 0.6261 - val_acc: 0.8459\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0963 - acc: 0.9654 - val_loss: 0.5383 - val_acc: 0.8752\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0931 - acc: 0.9665 - val_loss: 0.4882 - val_acc: 0.8867\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0896 - acc: 0.9682 - val_loss: 0.5124 - val_acc: 0.8714\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0867 - acc: 0.9696 - val_loss: 0.4302 - val_acc: 0.8928\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0856 - acc: 0.9698 - val_loss: 0.4407 - val_acc: 0.8954\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0789 - acc: 0.9728 - val_loss: 0.4567 - val_acc: 0.8929\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0783 - acc: 0.9716 - val_loss: 0.4857 - val_acc: 0.8812\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0797 - acc: 0.9720 - val_loss: 0.5257 - val_acc: 0.8853\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0729 - acc: 0.9735 - val_loss: 0.4932 - val_acc: 0.8892\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0710 - acc: 0.9752 - val_loss: 0.5300 - val_acc: 0.8815\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0663 - acc: 0.9767 - val_loss: 0.4540 - val_acc: 0.9015\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0715 - acc: 0.9751 - val_loss: 0.6166 - val_acc: 0.8691\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0692 - acc: 0.9760 - val_loss: 0.4296 - val_acc: 0.9007\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0641 - acc: 0.9773 - val_loss: 0.4052 - val_acc: 0.9046\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0600 - acc: 0.9791 - val_loss: 0.4567 - val_acc: 0.8930\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0611 - acc: 0.9787 - val_loss: 0.5493 - val_acc: 0.8815\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0529 - acc: 0.9815 - val_loss: 0.6481 - val_acc: 0.8710\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0557 - acc: 0.9803 - val_loss: 0.5394 - val_acc: 0.8804\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0564 - acc: 0.9794 - val_loss: 0.4989 - val_acc: 0.8890\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0544 - acc: 0.9808 - val_loss: 0.5139 - val_acc: 0.8908\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0547 - acc: 0.9813 - val_loss: 0.4914 - val_acc: 0.8920\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0485 - acc: 0.9834 - val_loss: 0.5181 - val_acc: 0.8887\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 70s 179ms/step - loss: 0.0484 - acc: 0.9832 - val_loss: 0.4809 - val_acc: 0.8973\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0313 - acc: 0.9898 - val_loss: 0.3555 - val_acc: 0.9195\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0222 - acc: 0.9929 - val_loss: 0.3429 - val_acc: 0.9212\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0181 - acc: 0.9944 - val_loss: 0.3489 - val_acc: 0.9199\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.3611 - val_acc: 0.9186\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0167 - acc: 0.9952 - val_loss: 0.3509 - val_acc: 0.9210\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.3583 - val_acc: 0.9200\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0135 - acc: 0.9963 - val_loss: 0.3551 - val_acc: 0.9204\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0140 - acc: 0.9960 - val_loss: 0.3549 - val_acc: 0.9208\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.3672 - val_acc: 0.9192\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0134 - acc: 0.9963 - val_loss: 0.3586 - val_acc: 0.9215\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0119 - acc: 0.9969 - val_loss: 0.3537 - val_acc: 0.9235\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0119 - acc: 0.9967 - val_loss: 0.3650 - val_acc: 0.9230\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0129 - acc: 0.9966 - val_loss: 0.3698 - val_acc: 0.9205\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 0.3767 - val_acc: 0.9194\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.3663 - val_acc: 0.9214\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.3732 - val_acc: 0.9223\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.3656 - val_acc: 0.9220\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.3761 - val_acc: 0.9214\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0105 - acc: 0.9971 - val_loss: 0.3640 - val_acc: 0.9217\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.3800 - val_acc: 0.9201\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.3686 - val_acc: 0.9223\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.3844 - val_acc: 0.9196\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.3754 - val_acc: 0.9237\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0095 - acc: 0.9974 - val_loss: 0.3804 - val_acc: 0.9207\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0093 - acc: 0.9974 - val_loss: 0.3764 - val_acc: 0.9221\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.3826 - val_acc: 0.9211\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.3865 - val_acc: 0.9212\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0083 - acc: 0.9978 - val_loss: 0.3788 - val_acc: 0.9229\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0087 - acc: 0.9974 - val_loss: 0.3899 - val_acc: 0.9195\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.3811 - val_acc: 0.9211\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.3844 - val_acc: 0.9205\n",
      "Epoch 92/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.3921 - val_acc: 0.9202\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.3835 - val_acc: 0.9211\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.4001 - val_acc: 0.9214\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.3911 - val_acc: 0.9218\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.3984 - val_acc: 0.9206\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.3926 - val_acc: 0.9223\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.3920 - val_acc: 0.9202\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.4096 - val_acc: 0.9200\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0065 - acc: 0.9984 - val_loss: 0.3996 - val_acc: 0.9212\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.4010 - val_acc: 0.9198\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.3957 - val_acc: 0.9228\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.4093 - val_acc: 0.9199\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.4013 - val_acc: 0.9222\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.4070 - val_acc: 0.9196\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.3996 - val_acc: 0.9212\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.4118 - val_acc: 0.9194\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.4045 - val_acc: 0.9195\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.3984 - val_acc: 0.9208\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.4022 - val_acc: 0.9220\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.4161 - val_acc: 0.9188\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.4063 - val_acc: 0.9219\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.4081 - val_acc: 0.9200\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.4061 - val_acc: 0.9223\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.4066 - val_acc: 0.9223\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0058 - acc: 0.9986 - val_loss: 0.4149 - val_acc: 0.9217\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.4173 - val_acc: 0.9220\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.4110 - val_acc: 0.9205\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.4132 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.4422 - val_acc: 0.9174\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.4201 - val_acc: 0.9217\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.4166 - val_acc: 0.9225\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.4135 - val_acc: 0.9228\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.4116 - val_acc: 0.9232\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0048 - acc: 0.9989 - val_loss: 0.4092 - val_acc: 0.9223\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.4110 - val_acc: 0.9226\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.4101 - val_acc: 0.9226\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.4138 - val_acc: 0.9211\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.4096 - val_acc: 0.9218\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.4120 - val_acc: 0.9219\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.4116 - val_acc: 0.9219\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.4089 - val_acc: 0.9240\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.4113 - val_acc: 0.9232\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4139 - val_acc: 0.9223\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4113 - val_acc: 0.9228\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.4073 - val_acc: 0.9229\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.4086 - val_acc: 0.9225\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.4086 - val_acc: 0.9226\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.4078 - val_acc: 0.9236\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4085 - val_acc: 0.9231\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.4078 - val_acc: 0.9230\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.4064 - val_acc: 0.9236\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.4084 - val_acc: 0.9234\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.4089 - val_acc: 0.9226\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.4097 - val_acc: 0.9220\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.4092 - val_acc: 0.9232\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.4090 - val_acc: 0.9220\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.4059 - val_acc: 0.9234\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4109 - val_acc: 0.9226\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4088 - val_acc: 0.9234\n",
      "Epoch 151/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.4094 - val_acc: 0.9234\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.4118 - val_acc: 0.9223\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.4105 - val_acc: 0.9223\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4084 - val_acc: 0.9224\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.4050 - val_acc: 0.9230\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.4074 - val_acc: 0.9237\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.4073 - val_acc: 0.9229\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.4064 - val_acc: 0.9231\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.4070 - val_acc: 0.9230\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.4058 - val_acc: 0.9228\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.4069 - val_acc: 0.9226\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.4070 - val_acc: 0.9227\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.4073 - val_acc: 0.9225\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.4059 - val_acc: 0.9232\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.4063 - val_acc: 0.9228\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.4065 - val_acc: 0.9225\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.4074 - val_acc: 0.9227\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.4067 - val_acc: 0.9232\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.4074 - val_acc: 0.9231\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.4074 - val_acc: 0.9232\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.4091 - val_acc: 0.9228\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.4067 - val_acc: 0.9234\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.4085 - val_acc: 0.9228\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.4074 - val_acc: 0.9233\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.4077 - val_acc: 0.9235\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.4093 - val_acc: 0.9234\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.4096 - val_acc: 0.9232\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.4081 - val_acc: 0.9232\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.4096 - val_acc: 0.9228\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.4075 - val_acc: 0.9237\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.4098 - val_acc: 0.9237\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.4084 - val_acc: 0.9232\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.4083 - val_acc: 0.9236\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.4094 - val_acc: 0.9229\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.4093 - val_acc: 0.9231\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.4108 - val_acc: 0.9232\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.4097 - val_acc: 0.9236\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.4089 - val_acc: 0.9229\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.4096 - val_acc: 0.9230\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.4079 - val_acc: 0.9241\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.4100 - val_acc: 0.9232\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.4093 - val_acc: 0.9235\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.4091 - val_acc: 0.9239\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.4094 - val_acc: 0.9234\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.4086 - val_acc: 0.9235\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.4105 - val_acc: 0.9231\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.4099 - val_acc: 0.9235\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 70s 178ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.4103 - val_acc: 0.9237\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.4099 - val_acc: 0.9233\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 69s 178ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.4116 - val_acc: 0.9237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.4276146936347785,\n",
       "  0.63169070512820513,\n",
       "  0.70975834933242965,\n",
       "  0.76321382739813926,\n",
       "  0.7950152390309928,\n",
       "  0.81981873594494559,\n",
       "  0.83577959574603633,\n",
       "  0.84915383379544285,\n",
       "  0.86234760988129611,\n",
       "  0.87024783443708542,\n",
       "  0.87874959899249427,\n",
       "  0.88600818096233702,\n",
       "  0.89092075710002905,\n",
       "  0.90116698744972434,\n",
       "  0.90419473849239351,\n",
       "  0.9092476740646791,\n",
       "  0.91151347451382891,\n",
       "  0.92083734363785985,\n",
       "  0.92167949953789052,\n",
       "  0.92606169871794874,\n",
       "  0.92874919711663007,\n",
       "  0.93040182871966937,\n",
       "  0.93563522619814077,\n",
       "  0.93998636513288714,\n",
       "  0.94249278153981542,\n",
       "  0.94237247351312015,\n",
       "  0.94696422840564498,\n",
       "  0.94748556306050835,\n",
       "  0.95078125000000002,\n",
       "  0.95229206813706291,\n",
       "  0.95301973047186095,\n",
       "  0.95602743022136671,\n",
       "  0.95711020213654308,\n",
       "  0.96007779916586466,\n",
       "  0.9596955128205128,\n",
       "  0.9626083815411719,\n",
       "  0.96420837339775123,\n",
       "  0.96541145333962297,\n",
       "  0.96659448183535157,\n",
       "  0.96824919871794868,\n",
       "  0.96959296730212396,\n",
       "  0.96982274626859455,\n",
       "  0.97275024063509929,\n",
       "  0.9715945512820513,\n",
       "  0.97206165701361447,\n",
       "  0.97351219124774124,\n",
       "  0.97519650304780237,\n",
       "  0.97674045560449452,\n",
       "  0.9750360923965351,\n",
       "  0.97599855634238353,\n",
       "  0.97734199547025691,\n",
       "  0.9791265640038499,\n",
       "  0.97868543471286495,\n",
       "  0.98145032051282055,\n",
       "  0.98031069369904111,\n",
       "  0.97942733399409843,\n",
       "  0.98076923076923073,\n",
       "  0.98129415548452603,\n",
       "  0.98339749755559536,\n",
       "  0.98317693296747022,\n",
       "  0.98979387233224403,\n",
       "  0.99294193134424125,\n",
       "  0.99442572988758571,\n",
       "  0.99520773181251354,\n",
       "  0.99524783447533038,\n",
       "  0.99531250000000004,\n",
       "  0.99634714195876839,\n",
       "  0.9959696824060329,\n",
       "  0.99544834773204705,\n",
       "  0.99631055505909827,\n",
       "  0.99693214629451399,\n",
       "  0.99665142765479631,\n",
       "  0.99657122232916262,\n",
       "  0.99671158164902152,\n",
       "  0.99649101700352904,\n",
       "  0.99689204363169714,\n",
       "  0.99741337824831566,\n",
       "  0.99693214629451399,\n",
       "  0.99707250563349525,\n",
       "  0.99739332693602967,\n",
       "  0.99735322425409045,\n",
       "  0.99757378891870541,\n",
       "  0.99741337826743814,\n",
       "  0.99743342957972414,\n",
       "  0.99741337824831566,\n",
       "  0.99747353226166335,\n",
       "  0.997676282051282,\n",
       "  0.99785244062916167,\n",
       "  0.99741337824831566,\n",
       "  0.99741586538461535,\n",
       "  0.99817357096981374,\n",
       "  0.99795476419634266,\n",
       "  0.99775425088225855,\n",
       "  0.99815705128205123,\n",
       "  0.99805502085338471,\n",
       "  0.99781229929351312,\n",
       "  0.99777430221366703,\n",
       "  0.99785450755842309,\n",
       "  0.99775425088225855,\n",
       "  0.99837584215591912,\n",
       "  0.99803496952197623,\n",
       "  0.9979948668591595,\n",
       "  0.99813522617901829,\n",
       "  0.99815527751042665,\n",
       "  0.99809512353532392,\n",
       "  0.9981753288609575,\n",
       "  0.99815527752954913,\n",
       "  0.99817708333333333,\n",
       "  0.99803496952197623,\n",
       "  0.99815350032113037,\n",
       "  0.99857635547000323,\n",
       "  0.99823717948717949,\n",
       "  0.99845456007052169,\n",
       "  0.99829563683028555,\n",
       "  0.9982171474358974,\n",
       "  0.99855630413859475,\n",
       "  0.99803307644817107,\n",
       "  0.99849615014436954,\n",
       "  0.99865656079563681,\n",
       "  0.99855630413859475,\n",
       "  0.99845753205128207,\n",
       "  0.99853484264611436,\n",
       "  0.99887820512820513,\n",
       "  0.99861645813282007,\n",
       "  0.99887604367373151,\n",
       "  0.99877804487179489,\n",
       "  0.99865656079563681,\n",
       "  0.99887712544112928,\n",
       "  0.99883590239550568,\n",
       "  0.99865656079563681,\n",
       "  0.99869791666666663,\n",
       "  0.99885597304418905,\n",
       "  0.99887712546025176,\n",
       "  0.99889717677253764,\n",
       "  0.99889717677253764,\n",
       "  0.99871794871794872,\n",
       "  0.99895632626846498,\n",
       "  0.99881697144690407,\n",
       "  0.99903753611151902,\n",
       "  0.99893727943535449,\n",
       "  0.99875681745267886,\n",
       "  0.99883702277831243,\n",
       "  0.99895733076676296,\n",
       "  0.99911774141803011,\n",
       "  0.99903753609239654,\n",
       "  0.99875681745267886,\n",
       "  0.99901842948717945,\n",
       "  0.99905667951188182,\n",
       "  0.99889717677253764,\n",
       "  0.99889717677253764,\n",
       "  0.99871671478986201,\n",
       "  0.99889823717948723,\n",
       "  0.99921799807507217,\n",
       "  0.99887604369287242,\n",
       "  0.99891722810394612,\n",
       "  0.99905849358974363,\n",
       "  0.99885707414796576,\n",
       "  0.99873676612127049,\n",
       "  0.99887604367373151,\n",
       "  0.99885817307692304,\n",
       "  0.99905667951188182,\n",
       "  0.99911774141803011,\n",
       "  0.99913862179487178,\n",
       "  0.99893625561978161,\n",
       "  0.99881810897435896,\n",
       "  0.99887604367373151,\n",
       "  0.99899743344870218,\n",
       "  0.99903753609239654,\n",
       "  0.99907852564102562,\n",
       "  0.99917789541225532,\n",
       "  0.9992573860178563,\n",
       "  0.99883702277831243,\n",
       "  0.9989383012820513,\n",
       "  0.99911774141803011,\n",
       "  0.99893625561978161,\n",
       "  0.99909769008662175,\n",
       "  0.99897738209817133,\n",
       "  0.9987969201154957,\n",
       "  0.99861778846153848,\n",
       "  0.99901653821451508,\n",
       "  0.99877804487179489,\n",
       "  0.99915784408084696,\n",
       "  0.99905667951188182,\n",
       "  0.9991979467436638,\n",
       "  0.99891722812306849,\n",
       "  0.99909769008662175,\n",
       "  0.99889717679166012,\n",
       "  0.99921875000000004,\n",
       "  0.99903753609239654,\n",
       "  0.99897639693628926,\n",
       "  0.99897836538461537,\n",
       "  0.99897639691714835,\n",
       "  0.99907763875521338,\n",
       "  0.99895833333333328,\n",
       "  0.9992573859987155,\n",
       "  0.99881697144690407,\n",
       "  0.99885707410972091,\n",
       "  0.99903846153846154,\n",
       "  0.99907763875521338,\n",
       "  0.99909769008662175],\n",
       " 'loss': [1.5554920665185956,\n",
       "  1.0312292756178441,\n",
       "  0.81682759676518624,\n",
       "  0.67911604718975349,\n",
       "  0.58830405065636238,\n",
       "  0.52407167925068232,\n",
       "  0.47721792125992568,\n",
       "  0.43310761516737489,\n",
       "  0.39733181376085475,\n",
       "  0.37365815094123866,\n",
       "  0.35235785715160056,\n",
       "  0.32649515709281923,\n",
       "  0.31024914274316667,\n",
       "  0.28426052340720759,\n",
       "  0.27690391527428471,\n",
       "  0.25737280123005091,\n",
       "  0.25290623948785945,\n",
       "  0.22794008441448671,\n",
       "  0.22520935189314747,\n",
       "  0.20893284672727952,\n",
       "  0.20070831349398283,\n",
       "  0.19376879612107595,\n",
       "  0.18249465267953485,\n",
       "  0.16973093555529684,\n",
       "  0.16529437797284485,\n",
       "  0.16279908129332882,\n",
       "  0.14973169660334942,\n",
       "  0.14713186177642903,\n",
       "  0.13852827852735153,\n",
       "  0.13525927853970934,\n",
       "  0.13112290623547904,\n",
       "  0.12261844874575691,\n",
       "  0.11922180948060103,\n",
       "  0.1159406330772306,\n",
       "  0.11146955664914387,\n",
       "  0.10616251234594415,\n",
       "  0.10196947116365332,\n",
       "  0.096368481789619681,\n",
       "  0.0930169237407691,\n",
       "  0.089557609616372824,\n",
       "  0.086701607377093121,\n",
       "  0.085537741404716344,\n",
       "  0.078916040181941552,\n",
       "  0.078274157485709747,\n",
       "  0.079648736859543171,\n",
       "  0.072940949956537784,\n",
       "  0.071062841404553295,\n",
       "  0.06623754723497717,\n",
       "  0.071530153480396375,\n",
       "  0.069170909469130334,\n",
       "  0.064042719501140294,\n",
       "  0.060027201816841201,\n",
       "  0.061191772454964127,\n",
       "  0.052942030061370667,\n",
       "  0.055713660588551386,\n",
       "  0.056426240764092209,\n",
       "  0.054417544054106259,\n",
       "  0.054665734011658355,\n",
       "  0.048480732663438074,\n",
       "  0.048439249009094749,\n",
       "  0.031274317431792181,\n",
       "  0.022247588384166277,\n",
       "  0.018111795678801709,\n",
       "  0.017553664261058696,\n",
       "  0.016642387495051984,\n",
       "  0.015785930098559803,\n",
       "  0.013489421407555158,\n",
       "  0.014035792404622613,\n",
       "  0.014773248109212374,\n",
       "  0.013376021570381749,\n",
       "  0.011914040645770309,\n",
       "  0.011898992894791642,\n",
       "  0.012862086133466108,\n",
       "  0.011390164669604304,\n",
       "  0.01224181270582394,\n",
       "  0.01080477965220613,\n",
       "  0.009792374280152288,\n",
       "  0.010443928346448998,\n",
       "  0.010479507086404745,\n",
       "  0.0094191103949270404,\n",
       "  0.0098386401450844107,\n",
       "  0.0092794366612944379,\n",
       "  0.0091098626740312664,\n",
       "  0.0094935651105925098,\n",
       "  0.0093082435458078667,\n",
       "  0.0092728966740992323,\n",
       "  0.0087839854514757641,\n",
       "  0.0082988602910178851,\n",
       "  0.0086938700033889604,\n",
       "  0.009692588619266947,\n",
       "  0.0079189557452041918,\n",
       "  0.0083113201460772689,\n",
       "  0.008172274767001388,\n",
       "  0.0072962429143118264,\n",
       "  0.0073904220857878592,\n",
       "  0.0076399377550045713,\n",
       "  0.0076684104806791671,\n",
       "  0.0075264742376386189,\n",
       "  0.0073980515963049231,\n",
       "  0.0065382285964462215,\n",
       "  0.0076671057737937075,\n",
       "  0.0072524685499742074,\n",
       "  0.0066211627888393739,\n",
       "  0.0069799340490812305,\n",
       "  0.0064001125492690984,\n",
       "  0.006941776842259547,\n",
       "  0.0069071848222129533,\n",
       "  0.0068173092229512691,\n",
       "  0.0069304751496693916,\n",
       "  0.0067865885428168527,\n",
       "  0.0060005053864907922,\n",
       "  0.0063421190492450614,\n",
       "  0.0062478282487387158,\n",
       "  0.006422305242557355,\n",
       "  0.0066960496894376449,\n",
       "  0.0057991335782789253,\n",
       "  0.0066153429009411522,\n",
       "  0.0058218102830763637,\n",
       "  0.0053213363082946202,\n",
       "  0.0053231146963140519,\n",
       "  0.0057089165632929414,\n",
       "  0.0055500042863135144,\n",
       "  0.0047029139968053175,\n",
       "  0.0052740381258463065,\n",
       "  0.0048433975261255053,\n",
       "  0.0049410744986977456,\n",
       "  0.0047742566753377234,\n",
       "  0.0046719250893776055,\n",
       "  0.0045600023050209318,\n",
       "  0.0048743360353927173,\n",
       "  0.0046364373039418398,\n",
       "  0.0048284500114926738,\n",
       "  0.0044638839385833474,\n",
       "  0.0043995160431835505,\n",
       "  0.0043720997256369663,\n",
       "  0.0046376839382738138,\n",
       "  0.0045567123694453026,\n",
       "  0.0046362947580130594,\n",
       "  0.004337151024077072,\n",
       "  0.0043876249613597415,\n",
       "  0.0043965735159235304,\n",
       "  0.0048400610282731517,\n",
       "  0.0042843295902239061,\n",
       "  0.0042453373464575546,\n",
       "  0.0039703613481782245,\n",
       "  0.0047144735051826347,\n",
       "  0.0042271168521927811,\n",
       "  0.004037680330438935,\n",
       "  0.0043980379956486083,\n",
       "  0.0043893850528738017,\n",
       "  0.005375369609550794,\n",
       "  0.0043369710602116987,\n",
       "  0.0038636336608780597,\n",
       "  0.004358032139663584,\n",
       "  0.0041425796869555397,\n",
       "  0.0040942556179988272,\n",
       "  0.0045856729973801367,\n",
       "  0.004541773507852594,\n",
       "  0.0045339152119973315,\n",
       "  0.004217019294055018,\n",
       "  0.0043118362959208749,\n",
       "  0.00371312055881543,\n",
       "  0.0037070485462139076,\n",
       "  0.0043943757171860456,\n",
       "  0.004324874325949424,\n",
       "  0.0041338982363066986,\n",
       "  0.0042908376442518876,\n",
       "  0.0041736688351041717,\n",
       "  0.0040993245109581409,\n",
       "  0.0037768133692616668,\n",
       "  0.0036176411082397479,\n",
       "  0.0043766327046726181,\n",
       "  0.0042214198693937555,\n",
       "  0.0039298525938166939,\n",
       "  0.0038539078418588935,\n",
       "  0.0039631556056792146,\n",
       "  0.0040206187517111205,\n",
       "  0.0041369414721150397,\n",
       "  0.0046286845453626787,\n",
       "  0.0041537422563595961,\n",
       "  0.0042050446555666173,\n",
       "  0.0037299666627237612,\n",
       "  0.0043092711238945994,\n",
       "  0.0034792434053776684,\n",
       "  0.0041600012337791332,\n",
       "  0.0038887255181288623,\n",
       "  0.0042171458305715129,\n",
       "  0.0034628818265889259,\n",
       "  0.0038807722213634738,\n",
       "  0.0041372505473912091,\n",
       "  0.0041138652707256064,\n",
       "  0.003949391734991199,\n",
       "  0.0038124887308719538,\n",
       "  0.0041322158741352796,\n",
       "  0.0038283905108230801,\n",
       "  0.0042973094046361475,\n",
       "  0.004028571581881509,\n",
       "  0.0040663442714736825,\n",
       "  0.0038787511151105007,\n",
       "  0.0040137479789877761],\n",
       " 'val_acc': [0.37790000000000001,\n",
       "  0.60509999999999997,\n",
       "  0.54090000000000005,\n",
       "  0.71009999999999995,\n",
       "  0.69669999999999999,\n",
       "  0.76219999999999999,\n",
       "  0.78769999999999996,\n",
       "  0.79730000000000001,\n",
       "  0.75690000000000002,\n",
       "  0.80120000000000002,\n",
       "  0.82440000000000002,\n",
       "  0.8165,\n",
       "  0.84230000000000005,\n",
       "  0.82609999999999995,\n",
       "  0.76839999999999997,\n",
       "  0.82110000000000005,\n",
       "  0.86680000000000001,\n",
       "  0.86699999999999999,\n",
       "  0.85680000000000001,\n",
       "  0.85199999999999998,\n",
       "  0.87119999999999997,\n",
       "  0.86539999999999995,\n",
       "  0.86939999999999995,\n",
       "  0.85850000000000004,\n",
       "  0.87280000000000002,\n",
       "  0.88119999999999998,\n",
       "  0.87919999999999998,\n",
       "  0.88260000000000005,\n",
       "  0.86040000000000005,\n",
       "  0.86929999999999996,\n",
       "  0.87760000000000005,\n",
       "  0.85509999999999997,\n",
       "  0.86870000000000003,\n",
       "  0.87180000000000002,\n",
       "  0.87170000000000003,\n",
       "  0.86250000000000004,\n",
       "  0.84589999999999999,\n",
       "  0.87519999999999998,\n",
       "  0.88670000000000004,\n",
       "  0.87139999999999995,\n",
       "  0.89280000000000004,\n",
       "  0.89539999999999997,\n",
       "  0.89290000000000003,\n",
       "  0.88119999999999998,\n",
       "  0.88529999999999998,\n",
       "  0.88919999999999999,\n",
       "  0.88149999999999995,\n",
       "  0.90149999999999997,\n",
       "  0.86909999999999998,\n",
       "  0.90069999999999995,\n",
       "  0.90459999999999996,\n",
       "  0.89300000000000002,\n",
       "  0.88149999999999995,\n",
       "  0.871,\n",
       "  0.88039999999999996,\n",
       "  0.88900000000000001,\n",
       "  0.89080000000000004,\n",
       "  0.89200000000000002,\n",
       "  0.88870000000000005,\n",
       "  0.89729999999999999,\n",
       "  0.91949999999999998,\n",
       "  0.92120000000000002,\n",
       "  0.91990000000000005,\n",
       "  0.91859999999999997,\n",
       "  0.92100000000000004,\n",
       "  0.92000000000000004,\n",
       "  0.9204,\n",
       "  0.92079999999999995,\n",
       "  0.91920000000000002,\n",
       "  0.92149999999999999,\n",
       "  0.92349999999999999,\n",
       "  0.92300000000000004,\n",
       "  0.92049999999999998,\n",
       "  0.9194,\n",
       "  0.9214,\n",
       "  0.92230000000000001,\n",
       "  0.92200000000000004,\n",
       "  0.9214,\n",
       "  0.92169999999999996,\n",
       "  0.92010000000000003,\n",
       "  0.92230000000000001,\n",
       "  0.91959999999999997,\n",
       "  0.92369999999999997,\n",
       "  0.92069999999999996,\n",
       "  0.92210000000000003,\n",
       "  0.92110000000000003,\n",
       "  0.92120000000000002,\n",
       "  0.92290000000000005,\n",
       "  0.91949999999999998,\n",
       "  0.92110000000000003,\n",
       "  0.92049999999999998,\n",
       "  0.92020000000000002,\n",
       "  0.92110000000000003,\n",
       "  0.9214,\n",
       "  0.92179999999999995,\n",
       "  0.92059999999999997,\n",
       "  0.92230000000000001,\n",
       "  0.92020000000000002,\n",
       "  0.92000000000000004,\n",
       "  0.92120000000000002,\n",
       "  0.91979999999999995,\n",
       "  0.92279999999999995,\n",
       "  0.91990000000000005,\n",
       "  0.92220000000000002,\n",
       "  0.91959999999999997,\n",
       "  0.92120000000000002,\n",
       "  0.9194,\n",
       "  0.91949999999999998,\n",
       "  0.92079999999999995,\n",
       "  0.92200000000000004,\n",
       "  0.91879999999999995,\n",
       "  0.92190000000000005,\n",
       "  0.92000000000000004,\n",
       "  0.92230000000000001,\n",
       "  0.92230000000000001,\n",
       "  0.92169999999999996,\n",
       "  0.92200000000000004,\n",
       "  0.92049999999999998,\n",
       "  0.92220000000000002,\n",
       "  0.91739999999999999,\n",
       "  0.92169999999999996,\n",
       "  0.92249999999999999,\n",
       "  0.92279999999999995,\n",
       "  0.92320000000000002,\n",
       "  0.92230000000000001,\n",
       "  0.92259999999999998,\n",
       "  0.92259999999999998,\n",
       "  0.92110000000000003,\n",
       "  0.92179999999999995,\n",
       "  0.92190000000000005,\n",
       "  0.92190000000000005,\n",
       "  0.92400000000000004,\n",
       "  0.92320000000000002,\n",
       "  0.92230000000000001,\n",
       "  0.92279999999999995,\n",
       "  0.92290000000000005,\n",
       "  0.92249999999999999,\n",
       "  0.92259999999999998,\n",
       "  0.92359999999999998,\n",
       "  0.92310000000000003,\n",
       "  0.92300000000000004,\n",
       "  0.92359999999999998,\n",
       "  0.9234,\n",
       "  0.92259999999999998,\n",
       "  0.92200000000000004,\n",
       "  0.92320000000000002,\n",
       "  0.92200000000000004,\n",
       "  0.9234,\n",
       "  0.92259999999999998,\n",
       "  0.9234,\n",
       "  0.9234,\n",
       "  0.92230000000000001,\n",
       "  0.92230000000000001,\n",
       "  0.9224,\n",
       "  0.92300000000000004,\n",
       "  0.92369999999999997,\n",
       "  0.92290000000000005,\n",
       "  0.92310000000000003,\n",
       "  0.92300000000000004,\n",
       "  0.92279999999999995,\n",
       "  0.92259999999999998,\n",
       "  0.92269999999999996,\n",
       "  0.92249999999999999,\n",
       "  0.92320000000000002,\n",
       "  0.92279999999999995,\n",
       "  0.92249999999999999,\n",
       "  0.92269999999999996,\n",
       "  0.92320000000000002,\n",
       "  0.92310000000000003,\n",
       "  0.92320000000000002,\n",
       "  0.92279999999999995,\n",
       "  0.9234,\n",
       "  0.92279999999999995,\n",
       "  0.92330000000000001,\n",
       "  0.92349999999999999,\n",
       "  0.9234,\n",
       "  0.92320000000000002,\n",
       "  0.92320000000000002,\n",
       "  0.92279999999999995,\n",
       "  0.92369999999999997,\n",
       "  0.92369999999999997,\n",
       "  0.92320000000000002,\n",
       "  0.92359999999999998,\n",
       "  0.92290000000000005,\n",
       "  0.92310000000000003,\n",
       "  0.92320000000000002,\n",
       "  0.92359999999999998,\n",
       "  0.92290000000000005,\n",
       "  0.92300000000000004,\n",
       "  0.92410000000000003,\n",
       "  0.92320000000000002,\n",
       "  0.92349999999999999,\n",
       "  0.92390000000000005,\n",
       "  0.9234,\n",
       "  0.92349999999999999,\n",
       "  0.92310000000000003,\n",
       "  0.92349999999999999,\n",
       "  0.92369999999999997,\n",
       "  0.92330000000000001,\n",
       "  0.92369999999999997],\n",
       " 'val_loss': [2.6718517719268799,\n",
       "  1.1908255887985228,\n",
       "  1.5803811138153077,\n",
       "  0.89379863629341127,\n",
       "  0.97128923263549805,\n",
       "  0.70803624076843263,\n",
       "  0.69774013872146612,\n",
       "  0.68229546575546263,\n",
       "  0.88888572082519535,\n",
       "  0.63953773794174196,\n",
       "  0.59632205152511597,\n",
       "  0.61199794349670411,\n",
       "  0.56730555481910705,\n",
       "  0.60776862702369694,\n",
       "  0.98215742418766017,\n",
       "  0.64995058231353764,\n",
       "  0.46212291612625123,\n",
       "  0.46888252613544462,\n",
       "  0.49858634796142576,\n",
       "  0.55191521005630495,\n",
       "  0.44356223592758176,\n",
       "  0.47506009616851808,\n",
       "  0.45512662982940671,\n",
       "  0.52505833759307863,\n",
       "  0.47436577119827272,\n",
       "  0.45304311978816986,\n",
       "  0.43863940353393555,\n",
       "  0.41545259883403779,\n",
       "  0.56780660886764522,\n",
       "  0.54807588100433347,\n",
       "  0.45537460651397704,\n",
       "  0.64017211136817931,\n",
       "  0.51196086645126337,\n",
       "  0.53089978561401363,\n",
       "  0.52595645565986637,\n",
       "  0.52748600773811338,\n",
       "  0.62605938577651976,\n",
       "  0.538266028547287,\n",
       "  0.48817643728256227,\n",
       "  0.51235282673835758,\n",
       "  0.43024569730758666,\n",
       "  0.44071310545206072,\n",
       "  0.45665513309240341,\n",
       "  0.48565789008140564,\n",
       "  0.52566834268569951,\n",
       "  0.49323937015533448,\n",
       "  0.53001688320636753,\n",
       "  0.45401287174224852,\n",
       "  0.61657550115585325,\n",
       "  0.42956270031929017,\n",
       "  0.40519322977066041,\n",
       "  0.45671843051910399,\n",
       "  0.54929728660583499,\n",
       "  0.64807311496734621,\n",
       "  0.53937318801879885,\n",
       "  0.49889945302009581,\n",
       "  0.51386734037399295,\n",
       "  0.49140605907440188,\n",
       "  0.51811047134399413,\n",
       "  0.48086942534446714,\n",
       "  0.35548748283386228,\n",
       "  0.34285682280063629,\n",
       "  0.34893946337699888,\n",
       "  0.36112505588531496,\n",
       "  0.35094484124183656,\n",
       "  0.35834487357139588,\n",
       "  0.35513356573581695,\n",
       "  0.35490513367652893,\n",
       "  0.36718595867156983,\n",
       "  0.35855014872550967,\n",
       "  0.35365398983955382,\n",
       "  0.36499137284755706,\n",
       "  0.36983265044689179,\n",
       "  0.37672408626079562,\n",
       "  0.36630014137029648,\n",
       "  0.37322333745956421,\n",
       "  0.3656014687895775,\n",
       "  0.3760841016769409,\n",
       "  0.36401510474681853,\n",
       "  0.37996459944248201,\n",
       "  0.36860192337036135,\n",
       "  0.38438401732444766,\n",
       "  0.37535804290771485,\n",
       "  0.38037115216255191,\n",
       "  0.37638693170547488,\n",
       "  0.3826321974039078,\n",
       "  0.38654890744686127,\n",
       "  0.37881328125000002,\n",
       "  0.38988256475925448,\n",
       "  0.3810602177143097,\n",
       "  0.38436533589363098,\n",
       "  0.3921102421283722,\n",
       "  0.38345872855186464,\n",
       "  0.40014856781959535,\n",
       "  0.3910915773630142,\n",
       "  0.39842606935501096,\n",
       "  0.39255073933601381,\n",
       "  0.39196937570571899,\n",
       "  0.4096097840309143,\n",
       "  0.39958878226280214,\n",
       "  0.40100315232276917,\n",
       "  0.39574430379867553,\n",
       "  0.40933525366783141,\n",
       "  0.40129287197589875,\n",
       "  0.40699689188003541,\n",
       "  0.39962643737792969,\n",
       "  0.41184571046829221,\n",
       "  0.40454527478218077,\n",
       "  0.39841163306236266,\n",
       "  0.4022380084037781,\n",
       "  0.41610363121032717,\n",
       "  0.40630917210578921,\n",
       "  0.40806263136863707,\n",
       "  0.4061400471687317,\n",
       "  0.40661696238517764,\n",
       "  0.41487922477722167,\n",
       "  0.41730213093757629,\n",
       "  0.41099963302612302,\n",
       "  0.41321171345710755,\n",
       "  0.44224258594512939,\n",
       "  0.42005871009826662,\n",
       "  0.41663589899539949,\n",
       "  0.41348185005187987,\n",
       "  0.41156049599647521,\n",
       "  0.40921954684257505,\n",
       "  0.41096807041168215,\n",
       "  0.41012511940002444,\n",
       "  0.4137584841489792,\n",
       "  0.40961964035034182,\n",
       "  0.41203095936775208,\n",
       "  0.41155415663719175,\n",
       "  0.40894508457183837,\n",
       "  0.41130053086280821,\n",
       "  0.41386853828430176,\n",
       "  0.41129863326549532,\n",
       "  0.40729371757507327,\n",
       "  0.40860923113822939,\n",
       "  0.40864375948905945,\n",
       "  0.40779102087020874,\n",
       "  0.40849987936019899,\n",
       "  0.40781525769233706,\n",
       "  0.40643498125076294,\n",
       "  0.40843672699928285,\n",
       "  0.4089022850036621,\n",
       "  0.40965545501708983,\n",
       "  0.40921025724411009,\n",
       "  0.40902138876914979,\n",
       "  0.40594275131225588,\n",
       "  0.41091131687164306,\n",
       "  0.40883750929832458,\n",
       "  0.40941970419883728,\n",
       "  0.41184230456352233,\n",
       "  0.41045755820274354,\n",
       "  0.40844148583412171,\n",
       "  0.4049852472782135,\n",
       "  0.4074371727466583,\n",
       "  0.40729559397697451,\n",
       "  0.40639665927886964,\n",
       "  0.40702938609123229,\n",
       "  0.40582195487022399,\n",
       "  0.40686765327453611,\n",
       "  0.40696291627883913,\n",
       "  0.40733078818321228,\n",
       "  0.40586067237854007,\n",
       "  0.4062950397014618,\n",
       "  0.40654663052558898,\n",
       "  0.40743280377388003,\n",
       "  0.40666296110153199,\n",
       "  0.40744909715652466,\n",
       "  0.40741498126983644,\n",
       "  0.40905747790336611,\n",
       "  0.40668791909217833,\n",
       "  0.40846955590248107,\n",
       "  0.40738729038238525,\n",
       "  0.40768678369522093,\n",
       "  0.40930512628555299,\n",
       "  0.40958523693084714,\n",
       "  0.40814374499320982,\n",
       "  0.40957420139312745,\n",
       "  0.40750363783836363,\n",
       "  0.40977412943840025,\n",
       "  0.40838636064529421,\n",
       "  0.40834701638221743,\n",
       "  0.40942210855484007,\n",
       "  0.40933882799148558,\n",
       "  0.41075253820419311,\n",
       "  0.40971396470069887,\n",
       "  0.40891203069686888,\n",
       "  0.40961463637351991,\n",
       "  0.407903772687912,\n",
       "  0.41001614012718202,\n",
       "  0.4093302333831787,\n",
       "  0.40911433444023132,\n",
       "  0.40936685914993287,\n",
       "  0.40863603086471556,\n",
       "  0.41046725478172302,\n",
       "  0.40993083200454711,\n",
       "  0.41029921202659608,\n",
       "  0.40988498697280884,\n",
       "  0.41161193232536314]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16-4 - dropout\n",
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 413s 1s/step - loss: 1.6013 - acc: 0.4157 - val_loss: 1.8229 - val_acc: 0.4576\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 1.0809 - acc: 0.6130 - val_loss: 1.6867 - val_acc: 0.5268\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.8150 - acc: 0.7119 - val_loss: 2.9243 - val_acc: 0.4185\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.6578 - acc: 0.7715 - val_loss: 4.6879 - val_acc: 0.3590\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.5648 - acc: 0.8032 - val_loss: 1.0208 - val_acc: 0.7188\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.4891 - acc: 0.8300 - val_loss: 1.3008 - val_acc: 0.6749\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.4436 - acc: 0.8458 - val_loss: 0.9580 - val_acc: 0.7518\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.4075 - acc: 0.8585 - val_loss: 1.3144 - val_acc: 0.7132\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.3662 - acc: 0.8735 - val_loss: 0.7828 - val_acc: 0.7964\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.3341 - acc: 0.8838 - val_loss: 1.6359 - val_acc: 0.6711\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.3058 - acc: 0.8936 - val_loss: 0.6686 - val_acc: 0.8185\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.2780 - acc: 0.9034 - val_loss: 1.1242 - val_acc: 0.7648\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.2580 - acc: 0.9108 - val_loss: 0.9508 - val_acc: 0.7807\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.2431 - acc: 0.9167 - val_loss: 0.9817 - val_acc: 0.7816\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.2245 - acc: 0.9218 - val_loss: 0.7679 - val_acc: 0.8209\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.2107 - acc: 0.9264 - val_loss: 0.9010 - val_acc: 0.8074\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1918 - acc: 0.9326 - val_loss: 0.6168 - val_acc: 0.8534\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1767 - acc: 0.9388 - val_loss: 0.4855 - val_acc: 0.8823\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1695 - acc: 0.9400 - val_loss: 0.6403 - val_acc: 0.8528\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1542 - acc: 0.9454 - val_loss: 1.2104 - val_acc: 0.7880\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 411s 1s/step - loss: 0.1428 - acc: 0.9493 - val_loss: 0.8826 - val_acc: 0.8210\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 411s 1s/step - loss: 0.1371 - acc: 0.9521 - val_loss: 1.2832 - val_acc: 0.7873\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 411s 1s/step - loss: 0.1269 - acc: 0.9556 - val_loss: 0.9688 - val_acc: 0.8222\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1204 - acc: 0.9577 - val_loss: 0.6590 - val_acc: 0.8683\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1145 - acc: 0.9589 - val_loss: 0.6485 - val_acc: 0.8662\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.1009 - acc: 0.9646 - val_loss: 1.0147 - val_acc: 0.8335\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0942 - acc: 0.9659 - val_loss: 0.8510 - val_acc: 0.8507\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0885 - acc: 0.9686 - val_loss: 0.7999 - val_acc: 0.8549\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0852 - acc: 0.9701 - val_loss: 0.7698 - val_acc: 0.8637\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0810 - acc: 0.9709 - val_loss: 1.1277 - val_acc: 0.8238\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0810 - acc: 0.9715 - val_loss: 0.8717 - val_acc: 0.8486\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0701 - acc: 0.9750 - val_loss: 0.8335 - val_acc: 0.8618\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0660 - acc: 0.9772 - val_loss: 0.7944 - val_acc: 0.8658\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0632 - acc: 0.9784 - val_loss: 0.9783 - val_acc: 0.8464\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0594 - acc: 0.9790 - val_loss: 0.9441 - val_acc: 0.8559\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0529 - acc: 0.9814 - val_loss: 0.6445 - val_acc: 0.8898\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0574 - acc: 0.9794 - val_loss: 1.1816 - val_acc: 0.8285\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0541 - acc: 0.9813 - val_loss: 0.9019 - val_acc: 0.8538\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0502 - acc: 0.9824 - val_loss: 0.7946 - val_acc: 0.8733\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0454 - acc: 0.9844 - val_loss: 0.7808 - val_acc: 0.8741\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0440 - acc: 0.9845 - val_loss: 0.9036 - val_acc: 0.8671\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0391 - acc: 0.9863 - val_loss: 0.6312 - val_acc: 0.8967\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0358 - acc: 0.9877 - val_loss: 1.2921 - val_acc: 0.8303\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0394 - acc: 0.9861 - val_loss: 0.9571 - val_acc: 0.8579\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0411 - acc: 0.9855 - val_loss: 0.9440 - val_acc: 0.8691\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0358 - acc: 0.9874 - val_loss: 0.8565 - val_acc: 0.8688\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0374 - acc: 0.9870 - val_loss: 0.7960 - val_acc: 0.8814\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0322 - acc: 0.9888 - val_loss: 0.7014 - val_acc: 0.8903\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0289 - acc: 0.9899 - val_loss: 0.9216 - val_acc: 0.8672\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0303 - acc: 0.9898 - val_loss: 0.8845 - val_acc: 0.8757\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0294 - acc: 0.9899 - val_loss: 0.7800 - val_acc: 0.8840\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.8499 - val_acc: 0.8793\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 409s 1s/step - loss: 0.0252 - acc: 0.9911 - val_loss: 1.1141 - val_acc: 0.8548\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0249 - acc: 0.9912 - val_loss: 0.9085 - val_acc: 0.8738\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0262 - acc: 0.9905 - val_loss: 0.7585 - val_acc: 0.8845\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0265 - acc: 0.9907 - val_loss: 0.6705 - val_acc: 0.8961\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0215 - acc: 0.9923 - val_loss: 0.8124 - val_acc: 0.8863\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0252 - acc: 0.9916 - val_loss: 0.8254 - val_acc: 0.8829\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0196 - acc: 0.9934 - val_loss: 0.8113 - val_acc: 0.8881\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0242 - acc: 0.9915 - val_loss: 0.7963 - val_acc: 0.8851\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 410s 1s/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.5060 - val_acc: 0.9210\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.5053 - val_acc: 0.9214\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.5129 - val_acc: 0.9210\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 409s 1s/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.5073 - val_acc: 0.9224\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.5211 - val_acc: 0.9186\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5077 - val_acc: 0.9222\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.5131 - val_acc: 0.9213\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 410s 1s/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.5067 - val_acc: 0.9206\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 409s 1s/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.5133 - val_acc: 0.9206\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 409s 1s/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.5161 - val_acc: 0.9206\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 409s 1s/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.5075 - val_acc: 0.9225\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 411s 1s/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.4904 - val_acc: 0.9259\n",
      "Epoch 73/200\n",
      "232/390 [================>.............] - ETA: 2:38 - loss: 0.0024 - acc: 0.9995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-335fb11dec73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gonna fit the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;31m#     print(his.history)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mhis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# WRN 28-10\n",
    "init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "# For WRN-16-8 put N = 2, k = 8\n",
    "# For WRN-28-10 put N = 4, k = 10\n",
    "# For WRN-40-4 put N = 6, k = 4\n",
    "model = build_model(init_shape, num_classes, 28, 10, dropout=0.3)\n",
    "\n",
    "print(\"Model Created\")\n",
    "batch_size  = 128\n",
    "epochs = 200\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "####################\n",
    "# Network training #\n",
    "####################\n",
    "\n",
    "print(\"Gonna fit the model\")\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "#     print(his.history)\n",
    "return his.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/125\n",
      "390/390 [==============================] - 35s 91ms/step - loss: 1.4636 - acc: 0.4623 - val_loss: 1.3533 - val_acc: 0.5576\n",
      "Epoch 2/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 1.0228 - acc: 0.6376 - val_loss: 1.0025 - val_acc: 0.6565\n",
      "Epoch 3/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.8504 - acc: 0.7041 - val_loss: 0.8529 - val_acc: 0.7124\n",
      "Epoch 4/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.7550 - acc: 0.7360 - val_loss: 1.2492 - val_acc: 0.6346\n",
      "Epoch 5/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6852 - acc: 0.7618 - val_loss: 0.6488 - val_acc: 0.7781\n",
      "Epoch 6/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6305 - acc: 0.7806 - val_loss: 0.8291 - val_acc: 0.7328\n",
      "Epoch 7/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5941 - acc: 0.7931 - val_loss: 0.6204 - val_acc: 0.7932\n",
      "Epoch 8/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5595 - acc: 0.8059 - val_loss: 0.6695 - val_acc: 0.7854\n",
      "Epoch 9/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5324 - acc: 0.8153 - val_loss: 0.8586 - val_acc: 0.7269\n",
      "Epoch 10/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5191 - acc: 0.8203 - val_loss: 0.6163 - val_acc: 0.7984\n",
      "Epoch 11/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4943 - acc: 0.8279 - val_loss: 0.8060 - val_acc: 0.7529\n",
      "Epoch 12/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4762 - acc: 0.8369 - val_loss: 0.5885 - val_acc: 0.8093\n",
      "Epoch 13/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4577 - acc: 0.8407 - val_loss: 0.7889 - val_acc: 0.7543\n",
      "Epoch 14/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4435 - acc: 0.8454 - val_loss: 0.6197 - val_acc: 0.8034\n",
      "Epoch 15/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4282 - acc: 0.8531 - val_loss: 0.4826 - val_acc: 0.8425\n",
      "Epoch 16/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4169 - acc: 0.8567 - val_loss: 0.5217 - val_acc: 0.8289\n",
      "Epoch 17/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4097 - acc: 0.8575 - val_loss: 0.5242 - val_acc: 0.8333\n",
      "Epoch 18/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3989 - acc: 0.8616 - val_loss: 0.5877 - val_acc: 0.8154\n",
      "Epoch 19/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3918 - acc: 0.8639 - val_loss: 0.4901 - val_acc: 0.8437\n",
      "Epoch 20/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3785 - acc: 0.8677 - val_loss: 0.5041 - val_acc: 0.8377\n",
      "Epoch 21/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3709 - acc: 0.8709 - val_loss: 0.4670 - val_acc: 0.8433\n",
      "Epoch 22/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3687 - acc: 0.8728 - val_loss: 0.6799 - val_acc: 0.8049\n",
      "Epoch 23/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3549 - acc: 0.8769 - val_loss: 0.4799 - val_acc: 0.8464\n",
      "Epoch 24/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3521 - acc: 0.8774 - val_loss: 0.5041 - val_acc: 0.8352\n",
      "Epoch 25/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3397 - acc: 0.8841 - val_loss: 0.5220 - val_acc: 0.8340\n",
      "Epoch 26/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3376 - acc: 0.8822 - val_loss: 0.4244 - val_acc: 0.8592\n",
      "Epoch 27/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3290 - acc: 0.8865 - val_loss: 0.4783 - val_acc: 0.8501\n",
      "Epoch 28/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3273 - acc: 0.8870 - val_loss: 0.4595 - val_acc: 0.8510\n",
      "Epoch 29/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3233 - acc: 0.8872 - val_loss: 0.5359 - val_acc: 0.8393\n",
      "Epoch 30/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3166 - acc: 0.8897 - val_loss: 0.5665 - val_acc: 0.8237\n",
      "Epoch 31/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3074 - acc: 0.8949 - val_loss: 0.6254 - val_acc: 0.8147\n",
      "Epoch 32/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3061 - acc: 0.8932 - val_loss: 0.4300 - val_acc: 0.8640\n",
      "Epoch 33/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3018 - acc: 0.8947 - val_loss: 0.5066 - val_acc: 0.8457\n",
      "Epoch 34/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2937 - acc: 0.8979 - val_loss: 0.4071 - val_acc: 0.8706\n",
      "Epoch 35/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2930 - acc: 0.8974 - val_loss: 0.8463 - val_acc: 0.7880\n",
      "Epoch 36/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2865 - acc: 0.8997 - val_loss: 0.4297 - val_acc: 0.8593\n",
      "Epoch 37/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2873 - acc: 0.8987 - val_loss: 0.6219 - val_acc: 0.8270\n",
      "Epoch 38/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2828 - acc: 0.9009 - val_loss: 0.4422 - val_acc: 0.8627\n",
      "Epoch 39/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2780 - acc: 0.9050 - val_loss: 0.4516 - val_acc: 0.8592\n",
      "Epoch 40/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2719 - acc: 0.9057 - val_loss: 0.5127 - val_acc: 0.8479\n",
      "Epoch 41/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2707 - acc: 0.9046 - val_loss: 0.4673 - val_acc: 0.8597\n",
      "Epoch 42/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2702 - acc: 0.9067 - val_loss: 0.4356 - val_acc: 0.8649\n",
      "Epoch 43/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2665 - acc: 0.9074 - val_loss: 0.4206 - val_acc: 0.8663\n",
      "Epoch 44/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2547 - acc: 0.9099 - val_loss: 0.3700 - val_acc: 0.8807\n",
      "Epoch 45/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2588 - acc: 0.9110 - val_loss: 0.4898 - val_acc: 0.8555\n",
      "Epoch 46/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2096 - acc: 0.9263 - val_loss: 0.3115 - val_acc: 0.8966\n",
      "Epoch 47/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1914 - acc: 0.9327 - val_loss: 0.3179 - val_acc: 0.8995\n",
      "Epoch 48/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1849 - acc: 0.9365 - val_loss: 0.3124 - val_acc: 0.8990\n",
      "Epoch 49/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1807 - acc: 0.9381 - val_loss: 0.3198 - val_acc: 0.8980\n",
      "Epoch 50/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1812 - acc: 0.9365 - val_loss: 0.3157 - val_acc: 0.8999\n",
      "Epoch 51/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1786 - acc: 0.9376 - val_loss: 0.3241 - val_acc: 0.8971\n",
      "Epoch 52/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1758 - acc: 0.9379 - val_loss: 0.3154 - val_acc: 0.9000\n",
      "Epoch 53/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1761 - acc: 0.9392 - val_loss: 0.3305 - val_acc: 0.8987\n",
      "Epoch 54/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1706 - acc: 0.9406 - val_loss: 0.3328 - val_acc: 0.8958\n",
      "Epoch 55/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1732 - acc: 0.9398 - val_loss: 0.3408 - val_acc: 0.8964\n",
      "Epoch 56/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1710 - acc: 0.9413 - val_loss: 0.3358 - val_acc: 0.8975\n",
      "Epoch 57/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1669 - acc: 0.9420 - val_loss: 0.3232 - val_acc: 0.9020\n",
      "Epoch 58/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1693 - acc: 0.9408 - val_loss: 0.3227 - val_acc: 0.8993\n",
      "Epoch 59/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1693 - acc: 0.9414 - val_loss: 0.3396 - val_acc: 0.8962\n",
      "Epoch 60/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1655 - acc: 0.9418 - val_loss: 0.3281 - val_acc: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1676 - acc: 0.9411 - val_loss: 0.3424 - val_acc: 0.8949\n",
      "Epoch 62/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1633 - acc: 0.9428 - val_loss: 0.3396 - val_acc: 0.8932\n",
      "Epoch 63/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1606 - acc: 0.9443 - val_loss: 0.3320 - val_acc: 0.9006\n",
      "Epoch 64/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1605 - acc: 0.9436 - val_loss: 0.3330 - val_acc: 0.9003\n",
      "Epoch 65/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1602 - acc: 0.9439 - val_loss: 0.3335 - val_acc: 0.9003\n",
      "Epoch 66/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1644 - acc: 0.9426 - val_loss: 0.3392 - val_acc: 0.8987\n",
      "Epoch 67/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1578 - acc: 0.9447 - val_loss: 0.3306 - val_acc: 0.8999\n",
      "Epoch 68/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1602 - acc: 0.9429 - val_loss: 0.3422 - val_acc: 0.9002\n",
      "Epoch 69/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1553 - acc: 0.9452 - val_loss: 0.3415 - val_acc: 0.9007\n",
      "Epoch 70/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1573 - acc: 0.9436 - val_loss: 0.3886 - val_acc: 0.8917\n",
      "Epoch 71/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1556 - acc: 0.9448 - val_loss: 0.3350 - val_acc: 0.9017\n",
      "Epoch 72/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1543 - acc: 0.9456 - val_loss: 0.3278 - val_acc: 0.8994\n",
      "Epoch 73/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1557 - acc: 0.9443 - val_loss: 0.3446 - val_acc: 0.8981\n",
      "Epoch 74/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1540 - acc: 0.9468 - val_loss: 0.3426 - val_acc: 0.8966\n",
      "Epoch 75/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1511 - acc: 0.9475 - val_loss: 0.3596 - val_acc: 0.8998\n",
      "Epoch 76/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1525 - acc: 0.9460 - val_loss: 0.3558 - val_acc: 0.8948\n",
      "Epoch 77/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1516 - acc: 0.9466 - val_loss: 0.3450 - val_acc: 0.8997\n",
      "Epoch 78/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1514 - acc: 0.9459 - val_loss: 0.3393 - val_acc: 0.9013\n",
      "Epoch 79/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1483 - acc: 0.9481 - val_loss: 0.3506 - val_acc: 0.8994\n",
      "Epoch 80/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1503 - acc: 0.9480 - val_loss: 0.3473 - val_acc: 0.8990\n",
      "Epoch 81/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1515 - acc: 0.9472 - val_loss: 0.3517 - val_acc: 0.8978\n",
      "Epoch 82/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1473 - acc: 0.9479 - val_loss: 0.3392 - val_acc: 0.8989\n",
      "Epoch 83/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1448 - acc: 0.9493 - val_loss: 0.3340 - val_acc: 0.8996\n",
      "Epoch 84/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1472 - acc: 0.9477 - val_loss: 0.3453 - val_acc: 0.8998\n",
      "Epoch 85/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1492 - acc: 0.9473 - val_loss: 0.3512 - val_acc: 0.9008\n",
      "Epoch 86/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1392 - acc: 0.9507 - val_loss: 0.3288 - val_acc: 0.9038\n",
      "Epoch 87/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1315 - acc: 0.9539 - val_loss: 0.3271 - val_acc: 0.9049\n",
      "Epoch 88/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1311 - acc: 0.9541 - val_loss: 0.3233 - val_acc: 0.9042\n",
      "Epoch 89/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1271 - acc: 0.9543 - val_loss: 0.3257 - val_acc: 0.9059\n",
      "Epoch 90/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1225 - acc: 0.9575 - val_loss: 0.3227 - val_acc: 0.9043\n",
      "Epoch 91/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1249 - acc: 0.9571 - val_loss: 0.3264 - val_acc: 0.9059\n",
      "Epoch 92/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1255 - acc: 0.9560 - val_loss: 0.3272 - val_acc: 0.9048\n",
      "Epoch 93/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1274 - acc: 0.9548 - val_loss: 0.3300 - val_acc: 0.9047\n",
      "Epoch 94/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1261 - acc: 0.9559 - val_loss: 0.3291 - val_acc: 0.9058\n",
      "Epoch 95/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1242 - acc: 0.9569 - val_loss: 0.3309 - val_acc: 0.9043\n",
      "Epoch 96/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1234 - acc: 0.9569 - val_loss: 0.3361 - val_acc: 0.9045\n",
      "Epoch 97/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1298 - acc: 0.9546 - val_loss: 0.3281 - val_acc: 0.9061\n",
      "Epoch 98/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1272 - acc: 0.9546 - val_loss: 0.3384 - val_acc: 0.9037\n",
      "Epoch 99/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1235 - acc: 0.9569 - val_loss: 0.3337 - val_acc: 0.9044\n",
      "Epoch 100/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1251 - acc: 0.9558 - val_loss: 0.3340 - val_acc: 0.9062\n",
      "Epoch 101/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1261 - acc: 0.9560 - val_loss: 0.3340 - val_acc: 0.9044\n",
      "Epoch 102/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1240 - acc: 0.9576 - val_loss: 0.3324 - val_acc: 0.9049\n",
      "Epoch 103/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1232 - acc: 0.9567 - val_loss: 0.3337 - val_acc: 0.9056\n",
      "Epoch 104/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1229 - acc: 0.9566 - val_loss: 0.3354 - val_acc: 0.9050\n",
      "Epoch 105/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1259 - acc: 0.9551 - val_loss: 0.3349 - val_acc: 0.9050\n",
      "Epoch 106/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1208 - acc: 0.9576 - val_loss: 0.3335 - val_acc: 0.9058\n",
      "Epoch 107/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1219 - acc: 0.9575 - val_loss: 0.3343 - val_acc: 0.9052\n",
      "Epoch 108/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1207 - acc: 0.9581 - val_loss: 0.3316 - val_acc: 0.9055\n",
      "Epoch 109/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1189 - acc: 0.9582 - val_loss: 0.3331 - val_acc: 0.9045\n",
      "Epoch 110/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1225 - acc: 0.9575 - val_loss: 0.3328 - val_acc: 0.9049\n",
      "Epoch 111/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1218 - acc: 0.9567 - val_loss: 0.3339 - val_acc: 0.9048\n",
      "Epoch 112/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1218 - acc: 0.9564 - val_loss: 0.3353 - val_acc: 0.9050\n",
      "Epoch 113/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1180 - acc: 0.9587 - val_loss: 0.3351 - val_acc: 0.9050\n",
      "Epoch 114/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1213 - acc: 0.9576 - val_loss: 0.3334 - val_acc: 0.9046\n",
      "Epoch 115/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1226 - acc: 0.9562 - val_loss: 0.3348 - val_acc: 0.9053\n",
      "Epoch 116/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1196 - acc: 0.9577 - val_loss: 0.3349 - val_acc: 0.9054\n",
      "Epoch 117/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1208 - acc: 0.9575 - val_loss: 0.3343 - val_acc: 0.9053\n",
      "Epoch 118/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1198 - acc: 0.9578 - val_loss: 0.3325 - val_acc: 0.9056\n",
      "Epoch 119/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1180 - acc: 0.9585 - val_loss: 0.3331 - val_acc: 0.9051\n",
      "Epoch 120/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1205 - acc: 0.9572 - val_loss: 0.3336 - val_acc: 0.9056\n",
      "Epoch 121/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1195 - acc: 0.9581 - val_loss: 0.3327 - val_acc: 0.9056\n",
      "Epoch 122/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1193 - acc: 0.9580 - val_loss: 0.3332 - val_acc: 0.9050\n",
      "Epoch 123/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1192 - acc: 0.9578 - val_loss: 0.3338 - val_acc: 0.9059\n",
      "Epoch 124/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1206 - acc: 0.9593 - val_loss: 0.3325 - val_acc: 0.9055\n",
      "Epoch 125/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1200 - acc: 0.9585 - val_loss: 0.3344 - val_acc: 0.9054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.46216313761321637,\n",
       "  0.63755213342341699,\n",
       "  0.70414260504985415,\n",
       "  0.73594401664446285,\n",
       "  0.761810234161306,\n",
       "  0.7805983317101044,\n",
       "  0.79299005453962146,\n",
       "  0.80596326592261491,\n",
       "  0.81534728905999354,\n",
       "  0.82023981392364453,\n",
       "  0.82789942248341053,\n",
       "  0.836822264960151,\n",
       "  0.84069217199846302,\n",
       "  0.84540423487943839,\n",
       "  0.8530838947323679,\n",
       "  0.85669313446237061,\n",
       "  0.85741498239307323,\n",
       "  0.86166586457516547,\n",
       "  0.86383140838639572,\n",
       "  0.86762111000346187,\n",
       "  0.87088947704215447,\n",
       "  0.87289461020211745,\n",
       "  0.87690487648379856,\n",
       "  0.87738610841847786,\n",
       "  0.88408325310888525,\n",
       "  0.8821984280138625,\n",
       "  0.88652951555983317,\n",
       "  0.88701074747538999,\n",
       "  0.88729146611510767,\n",
       "  0.8897577799548313,\n",
       "  0.894951074732241,\n",
       "  0.89326676287481255,\n",
       "  0.89467035609252332,\n",
       "  0.89783846651241872,\n",
       "  0.89739733720231141,\n",
       "  0.89968318892550236,\n",
       "  0.89878087903124648,\n",
       "  0.90096647419300757,\n",
       "  0.90499679178697467,\n",
       "  0.90565848568520713,\n",
       "  0.90461581649021494,\n",
       "  0.90674125758126101,\n",
       "  0.9074230028491469,\n",
       "  0.90988931661238071,\n",
       "  0.91099759615384612,\n",
       "  0.9263206487025063,\n",
       "  0.93266762913057422,\n",
       "  0.93647738213641618,\n",
       "  0.93814164262419142,\n",
       "  0.93647836538461537,\n",
       "  0.93762042393038192,\n",
       "  0.93784087261482041,\n",
       "  0.93924446583253129,\n",
       "  0.94058790503689449,\n",
       "  0.93984374999999998,\n",
       "  0.941353564566347,\n",
       "  0.94197144688495205,\n",
       "  0.94078525641025645,\n",
       "  0.94137363515760786,\n",
       "  0.94173083096541843,\n",
       "  0.94108573717948718,\n",
       "  0.94275850993590093,\n",
       "  0.94429086538461537,\n",
       "  0.94362154782928565,\n",
       "  0.94387632340699534,\n",
       "  0.94255293549579577,\n",
       "  0.94469842795649517,\n",
       "  0.94295396216220873,\n",
       "  0.94519971124170532,\n",
       "  0.9435555021235833,\n",
       "  0.94481873594494559,\n",
       "  0.94564084055181263,\n",
       "  0.94431745271710277,\n",
       "  0.94683493589743595,\n",
       "  0.94749518306345681,\n",
       "  0.94602181581032752,\n",
       "  0.94654315048431337,\n",
       "  0.94594161056118364,\n",
       "  0.94813701923076921,\n",
       "  0.94797687857443502,\n",
       "  0.94720484438254582,\n",
       "  0.94785657051282046,\n",
       "  0.9493015413875372,\n",
       "  0.94770612770600082,\n",
       "  0.94726499835764866,\n",
       "  0.95067372473532241,\n",
       "  0.95394631410256414,\n",
       "  0.95409842640372067,\n",
       "  0.95426291301918209,\n",
       "  0.95751122870734384,\n",
       "  0.95707131410256407,\n",
       "  0.95602520875388719,\n",
       "  0.95482435031773993,\n",
       "  0.95596727626538636,\n",
       "  0.95684953477086643,\n",
       "  0.9568695861022749,\n",
       "  0.95462383698453346,\n",
       "  0.95462383700365583,\n",
       "  0.95693108974358976,\n",
       "  0.95582691686903776,\n",
       "  0.95602520865818286,\n",
       "  0.95757138272069153,\n",
       "  0.95672922685890582,\n",
       "  0.95663060897435892,\n",
       "  0.95516217082222077,\n",
       "  0.95763153671491674,\n",
       "  0.95751122870734384,\n",
       "  0.95813281998100441,\n",
       "  0.95819297397522962,\n",
       "  0.95745192307692306,\n",
       "  0.95672922680153849,\n",
       "  0.95646676306320288,\n",
       "  0.95869425731780711,\n",
       "  0.9575520833333333,\n",
       "  0.95622591524072087,\n",
       "  0.95765158802720263,\n",
       "  0.95751201923076923,\n",
       "  0.95777189603477553,\n",
       "  0.95855389799794821,\n",
       "  0.95720937702620568,\n",
       "  0.95807291666666672,\n",
       "  0.95805234427090713,\n",
       "  0.95783253205128205,\n",
       "  0.95927665387887406,\n",
       "  0.95849374402284548],\n",
       " 'loss': [1.4637850606751281,\n",
       "  1.022853697287107,\n",
       "  0.85040882458318146,\n",
       "  0.7548379952761467,\n",
       "  0.68522286753567285,\n",
       "  0.63046443923222628,\n",
       "  0.59428436083215375,\n",
       "  0.5594987142609984,\n",
       "  0.53247503826814768,\n",
       "  0.51910470601531,\n",
       "  0.49417498884653871,\n",
       "  0.47637221874011265,\n",
       "  0.4577709803663082,\n",
       "  0.4435638748073486,\n",
       "  0.42815701424941671,\n",
       "  0.41700976465541584,\n",
       "  0.40979637767322408,\n",
       "  0.39876040247414485,\n",
       "  0.39187849402657143,\n",
       "  0.37852329115168498,\n",
       "  0.37092561787009276,\n",
       "  0.36870900242038745,\n",
       "  0.35506887304159473,\n",
       "  0.35211222184021806,\n",
       "  0.33973918193075797,\n",
       "  0.33768626086636161,\n",
       "  0.3289045834621635,\n",
       "  0.32744704040151629,\n",
       "  0.32312447610940159,\n",
       "  0.31660585071476827,\n",
       "  0.30742851367240004,\n",
       "  0.30593002567239247,\n",
       "  0.30186402790049999,\n",
       "  0.29373607787138994,\n",
       "  0.29310486062926133,\n",
       "  0.28641225005098747,\n",
       "  0.28715001026100939,\n",
       "  0.28268345155928581,\n",
       "  0.27792457606611171,\n",
       "  0.27188415323880993,\n",
       "  0.27079175423172219,\n",
       "  0.2700812175826916,\n",
       "  0.26645050435675083,\n",
       "  0.25461124015190983,\n",
       "  0.25880478534560936,\n",
       "  0.20962696421506241,\n",
       "  0.191370237024694,\n",
       "  0.18484975150429289,\n",
       "  0.18073940295265928,\n",
       "  0.18120651701704049,\n",
       "  0.17864463255814275,\n",
       "  0.17586125280465459,\n",
       "  0.17597390114209344,\n",
       "  0.17051122845041472,\n",
       "  0.17315691330302985,\n",
       "  0.17081997420114819,\n",
       "  0.16691586868595457,\n",
       "  0.1693160629329773,\n",
       "  0.16935801832872702,\n",
       "  0.16558522400964634,\n",
       "  0.16762345866897169,\n",
       "  0.16335938376594064,\n",
       "  0.16056662853329609,\n",
       "  0.16057503872772938,\n",
       "  0.16031846337864175,\n",
       "  0.16442252912122085,\n",
       "  0.15784589603022195,\n",
       "  0.16016685286698756,\n",
       "  0.1553648645569202,\n",
       "  0.15735672724325381,\n",
       "  0.15558156916584231,\n",
       "  0.15427047933069682,\n",
       "  0.15570541613944111,\n",
       "  0.15399280480849437,\n",
       "  0.15109629242387274,\n",
       "  0.15250765579822242,\n",
       "  0.15167647013832142,\n",
       "  0.15131359994602692,\n",
       "  0.14834866789289011,\n",
       "  0.15023507141701364,\n",
       "  0.15149130409521899,\n",
       "  0.1472517385123632,\n",
       "  0.14484511455072374,\n",
       "  0.14721934805227696,\n",
       "  0.14927082635727609,\n",
       "  0.13913205794734301,\n",
       "  0.13152427510000192,\n",
       "  0.13110435765306003,\n",
       "  0.12708085832264623,\n",
       "  0.12253825537164814,\n",
       "  0.1248794339501705,\n",
       "  0.12551930856364005,\n",
       "  0.12743092166486367,\n",
       "  0.12603972200632937,\n",
       "  0.12428944492410841,\n",
       "  0.12341557237097649,\n",
       "  0.12979378732668884,\n",
       "  0.12715816088582524,\n",
       "  0.12349592056603004,\n",
       "  0.12513667694509584,\n",
       "  0.12616498180944413,\n",
       "  0.12404200271187893,\n",
       "  0.1232327869640622,\n",
       "  0.12285564391372296,\n",
       "  0.12586565843532693,\n",
       "  0.1207776241623556,\n",
       "  0.12186530962428643,\n",
       "  0.12069485869819169,\n",
       "  0.11897066743721134,\n",
       "  0.12250452128549418,\n",
       "  0.12171072590071917,\n",
       "  0.1216634850070962,\n",
       "  0.1180473185602407,\n",
       "  0.12130041124346928,\n",
       "  0.12257694290542955,\n",
       "  0.11958450064144342,\n",
       "  0.12077165143803144,\n",
       "  0.11983913090946509,\n",
       "  0.11786805634931528,\n",
       "  0.12054318854765932,\n",
       "  0.11949313582900242,\n",
       "  0.11924638967140408,\n",
       "  0.11916041559515855,\n",
       "  0.12048553100708745,\n",
       "  0.12004688821247766],\n",
       " 'val_acc': [0.55759999999999998,\n",
       "  0.65649999999999997,\n",
       "  0.71240000000000003,\n",
       "  0.63460000000000005,\n",
       "  0.77810000000000001,\n",
       "  0.73280000000000001,\n",
       "  0.79320000000000002,\n",
       "  0.78539999999999999,\n",
       "  0.72689999999999999,\n",
       "  0.7984,\n",
       "  0.75290000000000001,\n",
       "  0.80930000000000002,\n",
       "  0.75429999999999997,\n",
       "  0.8034,\n",
       "  0.84250000000000003,\n",
       "  0.82889999999999997,\n",
       "  0.83330000000000004,\n",
       "  0.81540000000000001,\n",
       "  0.84370000000000001,\n",
       "  0.8377,\n",
       "  0.84330000000000005,\n",
       "  0.80489999999999995,\n",
       "  0.84640000000000004,\n",
       "  0.83520000000000005,\n",
       "  0.83399999999999996,\n",
       "  0.85919999999999996,\n",
       "  0.85009999999999997,\n",
       "  0.85099999999999998,\n",
       "  0.83930000000000005,\n",
       "  0.82369999999999999,\n",
       "  0.81469999999999998,\n",
       "  0.86399999999999999,\n",
       "  0.84570000000000001,\n",
       "  0.87060000000000004,\n",
       "  0.78800000000000003,\n",
       "  0.85929999999999995,\n",
       "  0.82699999999999996,\n",
       "  0.86270000000000002,\n",
       "  0.85919999999999996,\n",
       "  0.84789999999999999,\n",
       "  0.85970000000000002,\n",
       "  0.8649,\n",
       "  0.86629999999999996,\n",
       "  0.88070000000000004,\n",
       "  0.85550000000000004,\n",
       "  0.89659999999999995,\n",
       "  0.89949999999999997,\n",
       "  0.89900000000000002,\n",
       "  0.89800000000000002,\n",
       "  0.89990000000000003,\n",
       "  0.89710000000000001,\n",
       "  0.90000000000000002,\n",
       "  0.89870000000000005,\n",
       "  0.89580000000000004,\n",
       "  0.89639999999999997,\n",
       "  0.89749999999999996,\n",
       "  0.90200000000000002,\n",
       "  0.89929999999999999,\n",
       "  0.8962,\n",
       "  0.89990000000000003,\n",
       "  0.89490000000000003,\n",
       "  0.89319999999999999,\n",
       "  0.90059999999999996,\n",
       "  0.90029999999999999,\n",
       "  0.90029999999999999,\n",
       "  0.89870000000000005,\n",
       "  0.89990000000000003,\n",
       "  0.9002,\n",
       "  0.90069999999999995,\n",
       "  0.89170000000000005,\n",
       "  0.90169999999999995,\n",
       "  0.89939999999999998,\n",
       "  0.89810000000000001,\n",
       "  0.89659999999999995,\n",
       "  0.89980000000000004,\n",
       "  0.89480000000000004,\n",
       "  0.89970000000000006,\n",
       "  0.90129999999999999,\n",
       "  0.89939999999999998,\n",
       "  0.89900000000000002,\n",
       "  0.89780000000000004,\n",
       "  0.89890000000000003,\n",
       "  0.89959999999999996,\n",
       "  0.89980000000000004,\n",
       "  0.90080000000000005,\n",
       "  0.90380000000000005,\n",
       "  0.90490000000000004,\n",
       "  0.9042,\n",
       "  0.90590000000000004,\n",
       "  0.90429999999999999,\n",
       "  0.90590000000000004,\n",
       "  0.90480000000000005,\n",
       "  0.90469999999999995,\n",
       "  0.90580000000000005,\n",
       "  0.90429999999999999,\n",
       "  0.90449999999999997,\n",
       "  0.90610000000000002,\n",
       "  0.90369999999999995,\n",
       "  0.90439999999999998,\n",
       "  0.90620000000000001,\n",
       "  0.90439999999999998,\n",
       "  0.90490000000000004,\n",
       "  0.90559999999999996,\n",
       "  0.90500000000000003,\n",
       "  0.90500000000000003,\n",
       "  0.90580000000000005,\n",
       "  0.9052,\n",
       "  0.90549999999999997,\n",
       "  0.90449999999999997,\n",
       "  0.90490000000000004,\n",
       "  0.90480000000000005,\n",
       "  0.90500000000000003,\n",
       "  0.90500000000000003,\n",
       "  0.90459999999999996,\n",
       "  0.90529999999999999,\n",
       "  0.90539999999999998,\n",
       "  0.90529999999999999,\n",
       "  0.90559999999999996,\n",
       "  0.90510000000000002,\n",
       "  0.90559999999999996,\n",
       "  0.90559999999999996,\n",
       "  0.90500000000000003,\n",
       "  0.90590000000000004,\n",
       "  0.90549999999999997,\n",
       "  0.90539999999999998],\n",
       " 'val_loss': [1.353323163986206,\n",
       "  1.0025371114730834,\n",
       "  0.85287384414672851,\n",
       "  1.2491707580566407,\n",
       "  0.64875788822174074,\n",
       "  0.82910852098464971,\n",
       "  0.62035962400436406,\n",
       "  0.66949315161705014,\n",
       "  0.85864888162612918,\n",
       "  0.616349549806118,\n",
       "  0.80599965224266057,\n",
       "  0.5884666858196258,\n",
       "  0.7888578023910523,\n",
       "  0.61974483306407924,\n",
       "  0.48262069468498231,\n",
       "  0.52170121026039129,\n",
       "  0.52422588205337528,\n",
       "  0.58774777512550358,\n",
       "  0.49010205001831053,\n",
       "  0.50408467121124267,\n",
       "  0.46701718142032622,\n",
       "  0.67988276298046113,\n",
       "  0.47992829566001893,\n",
       "  0.5041404960393906,\n",
       "  0.52200938162803645,\n",
       "  0.42438960990905761,\n",
       "  0.47829693975448606,\n",
       "  0.45946596815586088,\n",
       "  0.53589100656509403,\n",
       "  0.56646907691955561,\n",
       "  0.6253511852264404,\n",
       "  0.43003951573371885,\n",
       "  0.50656242337226864,\n",
       "  0.40708639755249021,\n",
       "  0.84626994872093197,\n",
       "  0.4296520199418068,\n",
       "  0.62192248890399937,\n",
       "  0.44219406123161314,\n",
       "  0.45161027297973633,\n",
       "  0.51273603758811948,\n",
       "  0.46728022584915163,\n",
       "  0.43564885373115542,\n",
       "  0.42056695747375489,\n",
       "  0.36998703331947325,\n",
       "  0.48977787570953368,\n",
       "  0.31154110217094422,\n",
       "  0.31785920433998111,\n",
       "  0.31240086607933043,\n",
       "  0.3197853605270386,\n",
       "  0.31571834406852722,\n",
       "  0.32406212854385374,\n",
       "  0.31535026581287384,\n",
       "  0.33046883597373961,\n",
       "  0.33277676885128021,\n",
       "  0.34078011355400084,\n",
       "  0.33582431688308717,\n",
       "  0.32317621958255766,\n",
       "  0.32274720501899717,\n",
       "  0.33955457658767702,\n",
       "  0.32808970525264741,\n",
       "  0.34242585325241087,\n",
       "  0.33955400536060332,\n",
       "  0.33196146869659426,\n",
       "  0.33302259116172789,\n",
       "  0.3334626545906067,\n",
       "  0.33918341860771178,\n",
       "  0.33055404357910156,\n",
       "  0.34215825214385986,\n",
       "  0.341463890004158,\n",
       "  0.38860306124687194,\n",
       "  0.33500632696151733,\n",
       "  0.32784482145309446,\n",
       "  0.34459880020618439,\n",
       "  0.3426031925201416,\n",
       "  0.35955334429740904,\n",
       "  0.35584086959362032,\n",
       "  0.34499460163116458,\n",
       "  0.33933509571552278,\n",
       "  0.35058630285263059,\n",
       "  0.3473234972476959,\n",
       "  0.351678400850296,\n",
       "  0.3391634175300598,\n",
       "  0.3339948905944824,\n",
       "  0.34530498552322386,\n",
       "  0.35118101353645326,\n",
       "  0.32875078520774842,\n",
       "  0.32710770835876463,\n",
       "  0.32326627120971679,\n",
       "  0.32573785974979402,\n",
       "  0.3226978904724121,\n",
       "  0.32640173878669737,\n",
       "  0.32715398092269898,\n",
       "  0.32998115315437315,\n",
       "  0.32914018716812132,\n",
       "  0.33085027027130126,\n",
       "  0.33610503754615784,\n",
       "  0.3280963866472244,\n",
       "  0.33836231398582456,\n",
       "  0.33367895352840421,\n",
       "  0.33398548111915588,\n",
       "  0.33399665021896363,\n",
       "  0.33238976860046388,\n",
       "  0.33368272321224213,\n",
       "  0.33536807582378386,\n",
       "  0.33490030291080475,\n",
       "  0.33350346159934996,\n",
       "  0.33427589402198793,\n",
       "  0.33157565414905549,\n",
       "  0.33311553215980527,\n",
       "  0.332827321100235,\n",
       "  0.33392744333744051,\n",
       "  0.33527452092170718,\n",
       "  0.33514246401786807,\n",
       "  0.33335576860904692,\n",
       "  0.33481943032741546,\n",
       "  0.33485469567775727,\n",
       "  0.33427840008735654,\n",
       "  0.33250251610279086,\n",
       "  0.33306731452941896,\n",
       "  0.33361223428249359,\n",
       "  0.33270741171836854,\n",
       "  0.33319993264675141,\n",
       "  0.33376401503086089,\n",
       "  0.33247149395942688,\n",
       "  0.334407484126091]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/125\n",
      "390/390 [==============================] - 35s 91ms/step - loss: 1.4812 - acc: 0.4551 - val_loss: 1.6290 - val_acc: 0.4853\n",
      "Epoch 2/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 1.0406 - acc: 0.6292 - val_loss: 1.0975 - val_acc: 0.6407\n",
      "Epoch 3/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.8597 - acc: 0.6945 - val_loss: 0.8614 - val_acc: 0.7006\n",
      "Epoch 4/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.7603 - acc: 0.7325 - val_loss: 1.1223 - val_acc: 0.6097\n",
      "Epoch 5/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6915 - acc: 0.7601 - val_loss: 1.2396 - val_acc: 0.6442\n",
      "Epoch 6/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6447 - acc: 0.7758 - val_loss: 0.8798 - val_acc: 0.7202\n",
      "Epoch 7/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5972 - acc: 0.7934 - val_loss: 0.8105 - val_acc: 0.7370\n",
      "Epoch 8/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5708 - acc: 0.8006 - val_loss: 0.6330 - val_acc: 0.7879\n",
      "Epoch 9/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5383 - acc: 0.8153 - val_loss: 0.5286 - val_acc: 0.8173\n",
      "Epoch 10/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5156 - acc: 0.8218 - val_loss: 0.9276 - val_acc: 0.7294\n",
      "Epoch 11/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5100 - acc: 0.8230 - val_loss: 0.7743 - val_acc: 0.7694\n",
      "Epoch 12/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4827 - acc: 0.8329 - val_loss: 0.6828 - val_acc: 0.7873\n",
      "Epoch 13/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4651 - acc: 0.8395 - val_loss: 0.6341 - val_acc: 0.7905\n",
      "Epoch 14/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4494 - acc: 0.8430 - val_loss: 0.6020 - val_acc: 0.8132\n",
      "Epoch 15/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4379 - acc: 0.8480 - val_loss: 0.7650 - val_acc: 0.7725\n",
      "Epoch 16/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4289 - acc: 0.8515 - val_loss: 0.7475 - val_acc: 0.7679\n",
      "Epoch 17/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4127 - acc: 0.8567 - val_loss: 0.5240 - val_acc: 0.8258\n",
      "Epoch 18/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4069 - acc: 0.8575 - val_loss: 0.5361 - val_acc: 0.8278\n",
      "Epoch 19/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3967 - acc: 0.8624 - val_loss: 0.5815 - val_acc: 0.8214\n",
      "Epoch 20/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3867 - acc: 0.8639 - val_loss: 0.4864 - val_acc: 0.8331\n",
      "Epoch 21/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3844 - acc: 0.8650 - val_loss: 0.5604 - val_acc: 0.8314\n",
      "Epoch 22/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3661 - acc: 0.8711 - val_loss: 0.4894 - val_acc: 0.8339\n",
      "Epoch 23/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3637 - acc: 0.8722 - val_loss: 0.6215 - val_acc: 0.8130\n",
      "Epoch 24/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3587 - acc: 0.8738 - val_loss: 0.5624 - val_acc: 0.8284\n",
      "Epoch 25/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3460 - acc: 0.8785 - val_loss: 0.4197 - val_acc: 0.8640\n",
      "Epoch 26/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3430 - acc: 0.8818 - val_loss: 0.4923 - val_acc: 0.8517\n",
      "Epoch 27/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3377 - acc: 0.8823 - val_loss: 0.5082 - val_acc: 0.8428\n",
      "Epoch 28/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3282 - acc: 0.8863 - val_loss: 0.5683 - val_acc: 0.8328\n",
      "Epoch 29/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3272 - acc: 0.8847 - val_loss: 0.4319 - val_acc: 0.8615\n",
      "Epoch 30/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3234 - acc: 0.8889 - val_loss: 0.4011 - val_acc: 0.8716\n",
      "Epoch 31/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3111 - acc: 0.8914 - val_loss: 0.5128 - val_acc: 0.8467\n",
      "Epoch 32/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3076 - acc: 0.8927 - val_loss: 0.6458 - val_acc: 0.8153\n",
      "Epoch 33/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3105 - acc: 0.8908 - val_loss: 0.4952 - val_acc: 0.8515\n",
      "Epoch 34/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3042 - acc: 0.8948 - val_loss: 0.5307 - val_acc: 0.8467\n",
      "Epoch 35/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2924 - acc: 0.8981 - val_loss: 0.4523 - val_acc: 0.8560\n",
      "Epoch 36/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2894 - acc: 0.9002 - val_loss: 0.4656 - val_acc: 0.8581\n",
      "Epoch 37/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2898 - acc: 0.9004 - val_loss: 0.4934 - val_acc: 0.8523\n",
      "Epoch 38/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2832 - acc: 0.9014 - val_loss: 0.4768 - val_acc: 0.8539\n",
      "Epoch 39/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2796 - acc: 0.9015 - val_loss: 0.6547 - val_acc: 0.8126\n",
      "Epoch 40/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2809 - acc: 0.9022 - val_loss: 0.4037 - val_acc: 0.8713\n",
      "Epoch 41/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2750 - acc: 0.9034 - val_loss: 0.4243 - val_acc: 0.8664\n",
      "Epoch 42/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2719 - acc: 0.9054 - val_loss: 0.5326 - val_acc: 0.8442\n",
      "Epoch 43/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2713 - acc: 0.9059 - val_loss: 0.4361 - val_acc: 0.8580\n",
      "Epoch 44/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2721 - acc: 0.9053 - val_loss: 0.6206 - val_acc: 0.8190\n",
      "Epoch 45/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2592 - acc: 0.9088 - val_loss: 0.4519 - val_acc: 0.8566\n",
      "Epoch 46/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2140 - acc: 0.9262 - val_loss: 0.3231 - val_acc: 0.8944\n",
      "Epoch 47/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1964 - acc: 0.9323 - val_loss: 0.3303 - val_acc: 0.8904\n",
      "Epoch 48/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1914 - acc: 0.9335 - val_loss: 0.3410 - val_acc: 0.8905\n",
      "Epoch 49/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1862 - acc: 0.9347 - val_loss: 0.3311 - val_acc: 0.8945\n",
      "Epoch 50/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1829 - acc: 0.9357 - val_loss: 0.3337 - val_acc: 0.8974\n",
      "Epoch 51/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1800 - acc: 0.9377 - val_loss: 0.3455 - val_acc: 0.8927\n",
      "Epoch 52/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1775 - acc: 0.9384 - val_loss: 0.3383 - val_acc: 0.8948\n",
      "Epoch 53/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1805 - acc: 0.9365 - val_loss: 0.3369 - val_acc: 0.8965\n",
      "Epoch 54/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1760 - acc: 0.9374 - val_loss: 0.3408 - val_acc: 0.8982\n",
      "Epoch 55/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1746 - acc: 0.9379 - val_loss: 0.3395 - val_acc: 0.8941\n",
      "Epoch 56/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1702 - acc: 0.9405 - val_loss: 0.3388 - val_acc: 0.8951\n",
      "Epoch 57/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1728 - acc: 0.9392 - val_loss: 0.3297 - val_acc: 0.8965\n",
      "Epoch 58/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1731 - acc: 0.9393 - val_loss: 0.3529 - val_acc: 0.8911\n",
      "Epoch 59/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1661 - acc: 0.9424 - val_loss: 0.3518 - val_acc: 0.8942\n",
      "Epoch 60/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.3523 - val_acc: 0.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1675 - acc: 0.9411 - val_loss: 0.3609 - val_acc: 0.8927\n",
      "Epoch 62/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1629 - acc: 0.9416 - val_loss: 0.3461 - val_acc: 0.8896\n",
      "Epoch 63/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1661 - acc: 0.9417 - val_loss: 0.3618 - val_acc: 0.8902\n",
      "Epoch 64/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1638 - acc: 0.9429 - val_loss: 0.3723 - val_acc: 0.8899\n",
      "Epoch 65/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1620 - acc: 0.9430 - val_loss: 0.3712 - val_acc: 0.8901\n",
      "Epoch 66/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1634 - acc: 0.9423 - val_loss: 0.3391 - val_acc: 0.8973\n",
      "Epoch 67/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1619 - acc: 0.9420 - val_loss: 0.3369 - val_acc: 0.8946\n",
      "Epoch 68/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1576 - acc: 0.9447 - val_loss: 0.3551 - val_acc: 0.8919\n",
      "Epoch 69/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1567 - acc: 0.9439 - val_loss: 0.3647 - val_acc: 0.8932\n",
      "Epoch 70/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1591 - acc: 0.9429 - val_loss: 0.3682 - val_acc: 0.8906\n",
      "Epoch 71/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1565 - acc: 0.9444 - val_loss: 0.3612 - val_acc: 0.8917\n",
      "Epoch 72/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1577 - acc: 0.9451 - val_loss: 0.3401 - val_acc: 0.8961\n",
      "Epoch 73/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1545 - acc: 0.9454 - val_loss: 0.3501 - val_acc: 0.8938\n",
      "Epoch 74/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1586 - acc: 0.9446 - val_loss: 0.3916 - val_acc: 0.8864\n",
      "Epoch 75/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1533 - acc: 0.9447 - val_loss: 0.3526 - val_acc: 0.8972\n",
      "Epoch 76/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1548 - acc: 0.9453 - val_loss: 0.3593 - val_acc: 0.8903\n",
      "Epoch 77/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1537 - acc: 0.9454 - val_loss: 0.3556 - val_acc: 0.8933\n",
      "Epoch 78/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1536 - acc: 0.9454 - val_loss: 0.3657 - val_acc: 0.8952\n",
      "Epoch 79/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1484 - acc: 0.9469 - val_loss: 0.3568 - val_acc: 0.8952\n",
      "Epoch 80/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1538 - acc: 0.9460 - val_loss: 0.3745 - val_acc: 0.8877\n",
      "Epoch 81/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1497 - acc: 0.9464 - val_loss: 0.3733 - val_acc: 0.8891\n",
      "Epoch 82/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1502 - acc: 0.9466 - val_loss: 0.3588 - val_acc: 0.8950\n",
      "Epoch 83/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1483 - acc: 0.9473 - val_loss: 0.3662 - val_acc: 0.8924\n",
      "Epoch 84/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1519 - acc: 0.9463 - val_loss: 0.3660 - val_acc: 0.8937\n",
      "Epoch 85/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1460 - acc: 0.9486 - val_loss: 0.3696 - val_acc: 0.8948\n",
      "Epoch 86/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1345 - acc: 0.9526 - val_loss: 0.3505 - val_acc: 0.8949\n",
      "Epoch 87/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1307 - acc: 0.9538 - val_loss: 0.3440 - val_acc: 0.8987\n",
      "Epoch 88/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1314 - acc: 0.9542 - val_loss: 0.3410 - val_acc: 0.9005\n",
      "Epoch 89/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1306 - acc: 0.9540 - val_loss: 0.3407 - val_acc: 0.8986\n",
      "Epoch 90/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1284 - acc: 0.9559 - val_loss: 0.3412 - val_acc: 0.8986\n",
      "Epoch 91/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1286 - acc: 0.9553 - val_loss: 0.3426 - val_acc: 0.8991\n",
      "Epoch 92/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1264 - acc: 0.9548 - val_loss: 0.3488 - val_acc: 0.8987\n",
      "Epoch 93/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1306 - acc: 0.9531 - val_loss: 0.3432 - val_acc: 0.8999\n",
      "Epoch 94/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1289 - acc: 0.9550 - val_loss: 0.3414 - val_acc: 0.8996\n",
      "Epoch 95/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1284 - acc: 0.9545 - val_loss: 0.3464 - val_acc: 0.8989\n",
      "Epoch 96/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1286 - acc: 0.9547 - val_loss: 0.3480 - val_acc: 0.8990\n",
      "Epoch 97/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1270 - acc: 0.9562 - val_loss: 0.3509 - val_acc: 0.8980\n",
      "Epoch 98/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1291 - acc: 0.9540 - val_loss: 0.3442 - val_acc: 0.8997\n",
      "Epoch 99/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1242 - acc: 0.9567 - val_loss: 0.3480 - val_acc: 0.8989\n",
      "Epoch 100/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1278 - acc: 0.9543 - val_loss: 0.3494 - val_acc: 0.8992\n",
      "Epoch 101/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1258 - acc: 0.9566 - val_loss: 0.3508 - val_acc: 0.8981\n",
      "Epoch 102/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1260 - acc: 0.9565 - val_loss: 0.3498 - val_acc: 0.8994\n",
      "Epoch 103/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1255 - acc: 0.9555 - val_loss: 0.3499 - val_acc: 0.8986\n",
      "Epoch 104/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1260 - acc: 0.9537 - val_loss: 0.3533 - val_acc: 0.8961\n",
      "Epoch 105/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1233 - acc: 0.9568 - val_loss: 0.3520 - val_acc: 0.8974\n",
      "Epoch 106/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1223 - acc: 0.9562 - val_loss: 0.3497 - val_acc: 0.8995\n",
      "Epoch 107/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1205 - acc: 0.9582 - val_loss: 0.3492 - val_acc: 0.8991\n",
      "Epoch 108/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1193 - acc: 0.9586 - val_loss: 0.3517 - val_acc: 0.8989\n",
      "Epoch 109/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1236 - acc: 0.9575 - val_loss: 0.3505 - val_acc: 0.9000\n",
      "Epoch 110/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1225 - acc: 0.9565 - val_loss: 0.3505 - val_acc: 0.8984\n",
      "Epoch 111/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1274 - acc: 0.9554 - val_loss: 0.3519 - val_acc: 0.8992\n",
      "Epoch 112/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1218 - acc: 0.9577 - val_loss: 0.3504 - val_acc: 0.8989\n",
      "Epoch 113/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1233 - acc: 0.9575 - val_loss: 0.3508 - val_acc: 0.8990\n",
      "Epoch 114/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1199 - acc: 0.9574 - val_loss: 0.3504 - val_acc: 0.8993\n",
      "Epoch 115/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1235 - acc: 0.9570 - val_loss: 0.3515 - val_acc: 0.8989\n",
      "Epoch 116/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1206 - acc: 0.9585 - val_loss: 0.3493 - val_acc: 0.8994\n",
      "Epoch 117/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1205 - acc: 0.9581 - val_loss: 0.3492 - val_acc: 0.8985\n",
      "Epoch 118/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1235 - acc: 0.9569 - val_loss: 0.3520 - val_acc: 0.8994\n",
      "Epoch 119/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1224 - acc: 0.9577 - val_loss: 0.3514 - val_acc: 0.8987\n",
      "Epoch 120/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1208 - acc: 0.9571 - val_loss: 0.3498 - val_acc: 0.8989\n",
      "Epoch 121/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1205 - acc: 0.9564 - val_loss: 0.3502 - val_acc: 0.8990\n",
      "Epoch 122/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1227 - acc: 0.9577 - val_loss: 0.3520 - val_acc: 0.8989\n",
      "Epoch 123/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1201 - acc: 0.9576 - val_loss: 0.3520 - val_acc: 0.8988\n",
      "Epoch 124/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1177 - acc: 0.9587 - val_loss: 0.3515 - val_acc: 0.8993\n",
      "Epoch 125/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1235 - acc: 0.9568 - val_loss: 0.3517 - val_acc: 0.8999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.45516522298036649,\n",
       "  0.62925088225858195,\n",
       "  0.69449791462328869,\n",
       "  0.73243503368623675,\n",
       "  0.76016602504318409,\n",
       "  0.77584134615384615,\n",
       "  0.79337267174733017,\n",
       "  0.80068976580044915,\n",
       "  0.81526708369611511,\n",
       "  0.82174366376015251,\n",
       "  0.82296679497606529,\n",
       "  0.8328721526726951,\n",
       "  0.83948909207571387,\n",
       "  0.84311838307975762,\n",
       "  0.84805101056798049,\n",
       "  0.8515800449149824,\n",
       "  0.85663298042990055,\n",
       "  0.85753205128205123,\n",
       "  0.86237556199730403,\n",
       "  0.86385145975604893,\n",
       "  0.8649943855889608,\n",
       "  0.87113009303817779,\n",
       "  0.87223291624651755,\n",
       "  0.87381697142778159,\n",
       "  0.87850898301559344,\n",
       "  0.88177735003516355,\n",
       "  0.88223853065755686,\n",
       "  0.8864092076096276,\n",
       "  0.88472489573307667,\n",
       "  0.88890224358974357,\n",
       "  0.89141779060385207,\n",
       "  0.89274542831556136,\n",
       "  0.89080044913070111,\n",
       "  0.89471045875534017,\n",
       "  0.89823949312146445,\n",
       "  0.90012431825473216,\n",
       "  0.90038498558216384,\n",
       "  0.90136750076380834,\n",
       "  0.90154796276560645,\n",
       "  0.90218349358974359,\n",
       "  0.9033397560174754,\n",
       "  0.90537776712197926,\n",
       "  0.90593920438229214,\n",
       "  0.90527751046493721,\n",
       "  0.90880654475457168,\n",
       "  0.92615094642284246,\n",
       "  0.93231169871794872,\n",
       "  0.93350594089286942,\n",
       "  0.93469281358370082,\n",
       "  0.93567708333333333,\n",
       "  0.93766056515118523,\n",
       "  0.9384223612639091,\n",
       "  0.93653753613064139,\n",
       "  0.93739974336208043,\n",
       "  0.93792107797869895,\n",
       "  0.94052775104266928,\n",
       "  0.93918431187655094,\n",
       "  0.93932467119640983,\n",
       "  0.94240785256410253,\n",
       "  0.94097222226050392,\n",
       "  0.94116939360949337,\n",
       "  0.94153031765133444,\n",
       "  0.9417308309080511,\n",
       "  0.94285370544779934,\n",
       "  0.94309432146294514,\n",
       "  0.94235242221995652,\n",
       "  0.94205165221058562,\n",
       "  0.94469842799474002,\n",
       "  0.94391025641025639,\n",
       "  0.94295396214308635,\n",
       "  0.94437760671132798,\n",
       "  0.94514691714836219,\n",
       "  0.94536012191209495,\n",
       "  0.94459817135682045,\n",
       "  0.94473157051282053,\n",
       "  0.94536769430302015,\n",
       "  0.94542027588719779,\n",
       "  0.94542027592544264,\n",
       "  0.9468439204745619,\n",
       "  0.94600176449804152,\n",
       "  0.94632258585794327,\n",
       "  0.94658325310888525,\n",
       "  0.94732515235187387,\n",
       "  0.9463025344500452,\n",
       "  0.94870869423817628,\n",
       "  0.9525841346153846,\n",
       "  0.95381743736043523,\n",
       "  0.95420673076923079,\n",
       "  0.95393786119511281,\n",
       "  0.95590712227116115,\n",
       "  0.9553256335838276,\n",
       "  0.95482435037510727,\n",
       "  0.95306490384615383,\n",
       "  0.95498476098812957,\n",
       "  0.95441955687835878,\n",
       "  0.95466393966647267,\n",
       "  0.95620789216579749,\n",
       "  0.95402644230769229,\n",
       "  0.95664902145678243,\n",
       "  0.95431920355837851,\n",
       "  0.95662897016361892,\n",
       "  0.95645032051282053,\n",
       "  0.95550337183069711,\n",
       "  0.95370147575886921,\n",
       "  0.95678938077664122,\n",
       "  0.95620789224228719,\n",
       "  0.95819297401347447,\n",
       "  0.95859400066076506,\n",
       "  0.95751122870734384,\n",
       "  0.9565287134683319,\n",
       "  0.95536573630401178,\n",
       "  0.95773237179487181,\n",
       "  0.95753050736685785,\n",
       "  0.9574109721267916,\n",
       "  0.95694979142790848,\n",
       "  0.95847355769230769,\n",
       "  0.95813262684649969,\n",
       "  0.95687099358974359,\n",
       "  0.95771174202142784,\n",
       "  0.95702999679178702,\n",
       "  0.95642662168927273,\n",
       "  0.95767163937773347,\n",
       "  0.9575520833333333,\n",
       "  0.95877488758522655,\n",
       "  0.95682948351594777],\n",
       " 'loss': [1.4813120110049476,\n",
       "  1.0403141976167729,\n",
       "  0.85956479178279643,\n",
       "  0.76022433270848178,\n",
       "  0.69139731848205477,\n",
       "  0.64467381116671441,\n",
       "  0.59732312306656465,\n",
       "  0.57039315982778216,\n",
       "  0.53836898120848609,\n",
       "  0.51574222116773238,\n",
       "  0.51015765551462133,\n",
       "  0.48278464023639345,\n",
       "  0.46531790307518911,\n",
       "  0.44922772123525262,\n",
       "  0.437860643706874,\n",
       "  0.42892005369377934,\n",
       "  0.41277124540791282,\n",
       "  0.4068806836238274,\n",
       "  0.39668925515182829,\n",
       "  0.38678621807884705,\n",
       "  0.38437463575959169,\n",
       "  0.36612124642118798,\n",
       "  0.36364997636059521,\n",
       "  0.35874457892065137,\n",
       "  0.34594391726667312,\n",
       "  0.34297686042715275,\n",
       "  0.33783724046418939,\n",
       "  0.32799287300519847,\n",
       "  0.32717970829190524,\n",
       "  0.32338162973905221,\n",
       "  0.3110756593766546,\n",
       "  0.30756837634425305,\n",
       "  0.31051623691225955,\n",
       "  0.30430259194403153,\n",
       "  0.29228423089792915,\n",
       "  0.28956445962678251,\n",
       "  0.28979998239921689,\n",
       "  0.28313478858072405,\n",
       "  0.27957354400415702,\n",
       "  0.28087348093589148,\n",
       "  0.2750593353692124,\n",
       "  0.27195854033745365,\n",
       "  0.27121036396716547,\n",
       "  0.27212804803050478,\n",
       "  0.25929349257389933,\n",
       "  0.21404060058804861,\n",
       "  0.19644997337689765,\n",
       "  0.19130246805872914,\n",
       "  0.18623547071461347,\n",
       "  0.18291184008121492,\n",
       "  0.17998680740346032,\n",
       "  0.17741469101413229,\n",
       "  0.18039937001851111,\n",
       "  0.17608449066922494,\n",
       "  0.17450573375908121,\n",
       "  0.17017093411251485,\n",
       "  0.17293261038184279,\n",
       "  0.17303618917795646,\n",
       "  0.16606754878392585,\n",
       "  0.16941548830320532,\n",
       "  0.167355487418932,\n",
       "  0.16293209573665718,\n",
       "  0.16593908460468371,\n",
       "  0.16374923677485761,\n",
       "  0.16188172508900006,\n",
       "  0.16337082943053516,\n",
       "  0.16186374163432576,\n",
       "  0.15751173944571939,\n",
       "  0.15665945426011696,\n",
       "  0.15912349542670423,\n",
       "  0.15652225865723424,\n",
       "  0.15765755665210401,\n",
       "  0.15450282088926981,\n",
       "  0.15856451078661368,\n",
       "  0.15333927139066733,\n",
       "  0.15464034916769639,\n",
       "  0.15353110090333144,\n",
       "  0.15356064648784273,\n",
       "  0.14850181039036717,\n",
       "  0.15381825531967031,\n",
       "  0.14973472839924706,\n",
       "  0.15024350632315536,\n",
       "  0.14819422568845031,\n",
       "  0.15184703506565186,\n",
       "  0.14580291503891563,\n",
       "  0.13453791478696542,\n",
       "  0.13071957527969078,\n",
       "  0.13137078757087389,\n",
       "  0.13065335597883607,\n",
       "  0.12830011171811967,\n",
       "  0.1286145425881183,\n",
       "  0.12628790938896287,\n",
       "  0.1306037365912627,\n",
       "  0.12883551376711913,\n",
       "  0.12852474042216247,\n",
       "  0.12862150354820603,\n",
       "  0.12699074760667398,\n",
       "  0.12906059564497227,\n",
       "  0.12420257573895321,\n",
       "  0.12772267301762494,\n",
       "  0.12574559312946032,\n",
       "  0.12597427117900969,\n",
       "  0.12544186639389551,\n",
       "  0.12598697271318746,\n",
       "  0.12331639487448127,\n",
       "  0.12232640212091606,\n",
       "  0.12050042062146869,\n",
       "  0.11939596453724016,\n",
       "  0.12357399752019957,\n",
       "  0.12244850885688321,\n",
       "  0.12744832744258391,\n",
       "  0.12179957691293497,\n",
       "  0.12320697905738613,\n",
       "  0.11991197146645027,\n",
       "  0.12350882591729291,\n",
       "  0.12062978165654036,\n",
       "  0.12034863133555279,\n",
       "  0.12352355685180579,\n",
       "  0.1224036834151614,\n",
       "  0.12089138562537977,\n",
       "  0.12037721098273246,\n",
       "  0.12267191710000165,\n",
       "  0.12010274997983987,\n",
       "  0.11769683339185032,\n",
       "  0.12352201359314838],\n",
       " 'val_acc': [0.48530000000000001,\n",
       "  0.64070000000000005,\n",
       "  0.7006,\n",
       "  0.60970000000000002,\n",
       "  0.64419999999999999,\n",
       "  0.72019999999999995,\n",
       "  0.73699999999999999,\n",
       "  0.78790000000000004,\n",
       "  0.81730000000000003,\n",
       "  0.72940000000000005,\n",
       "  0.76939999999999997,\n",
       "  0.7873,\n",
       "  0.79049999999999998,\n",
       "  0.81320000000000003,\n",
       "  0.77249999999999996,\n",
       "  0.76790000000000003,\n",
       "  0.82579999999999998,\n",
       "  0.82779999999999998,\n",
       "  0.82140000000000002,\n",
       "  0.83309999999999995,\n",
       "  0.83140000000000003,\n",
       "  0.83389999999999997,\n",
       "  0.81299999999999994,\n",
       "  0.82840000000000003,\n",
       "  0.86399999999999999,\n",
       "  0.85170000000000001,\n",
       "  0.84279999999999999,\n",
       "  0.83279999999999998,\n",
       "  0.86150000000000004,\n",
       "  0.87160000000000004,\n",
       "  0.84670000000000001,\n",
       "  0.81530000000000002,\n",
       "  0.85150000000000003,\n",
       "  0.84670000000000001,\n",
       "  0.85599999999999998,\n",
       "  0.85809999999999997,\n",
       "  0.85229999999999995,\n",
       "  0.85389999999999999,\n",
       "  0.81259999999999999,\n",
       "  0.87129999999999996,\n",
       "  0.86639999999999995,\n",
       "  0.84419999999999995,\n",
       "  0.85799999999999998,\n",
       "  0.81899999999999995,\n",
       "  0.85660000000000003,\n",
       "  0.89439999999999997,\n",
       "  0.89039999999999997,\n",
       "  0.89049999999999996,\n",
       "  0.89449999999999996,\n",
       "  0.89739999999999998,\n",
       "  0.89270000000000005,\n",
       "  0.89480000000000004,\n",
       "  0.89649999999999996,\n",
       "  0.8982,\n",
       "  0.89410000000000001,\n",
       "  0.89510000000000001,\n",
       "  0.89649999999999996,\n",
       "  0.8911,\n",
       "  0.89419999999999999,\n",
       "  0.8931,\n",
       "  0.89270000000000005,\n",
       "  0.88959999999999995,\n",
       "  0.89019999999999999,\n",
       "  0.88990000000000002,\n",
       "  0.8901,\n",
       "  0.89729999999999999,\n",
       "  0.89459999999999995,\n",
       "  0.89190000000000003,\n",
       "  0.89319999999999999,\n",
       "  0.89059999999999995,\n",
       "  0.89170000000000005,\n",
       "  0.89610000000000001,\n",
       "  0.89380000000000004,\n",
       "  0.88639999999999997,\n",
       "  0.8972,\n",
       "  0.89029999999999998,\n",
       "  0.89329999999999998,\n",
       "  0.8952,\n",
       "  0.8952,\n",
       "  0.88770000000000004,\n",
       "  0.8891,\n",
       "  0.89500000000000002,\n",
       "  0.89239999999999997,\n",
       "  0.89370000000000005,\n",
       "  0.89480000000000004,\n",
       "  0.89490000000000003,\n",
       "  0.89870000000000005,\n",
       "  0.90049999999999997,\n",
       "  0.89859999999999995,\n",
       "  0.89859999999999995,\n",
       "  0.89910000000000001,\n",
       "  0.89870000000000005,\n",
       "  0.89990000000000003,\n",
       "  0.89959999999999996,\n",
       "  0.89890000000000003,\n",
       "  0.89900000000000002,\n",
       "  0.89800000000000002,\n",
       "  0.89970000000000006,\n",
       "  0.89890000000000003,\n",
       "  0.8992,\n",
       "  0.89810000000000001,\n",
       "  0.89939999999999998,\n",
       "  0.89859999999999995,\n",
       "  0.89610000000000001,\n",
       "  0.89739999999999998,\n",
       "  0.89949999999999997,\n",
       "  0.89910000000000001,\n",
       "  0.89890000000000003,\n",
       "  0.90000000000000002,\n",
       "  0.89839999999999998,\n",
       "  0.8992,\n",
       "  0.89890000000000003,\n",
       "  0.89900000000000002,\n",
       "  0.89930000114440922,\n",
       "  0.89890000000000003,\n",
       "  0.89939999999999998,\n",
       "  0.89849999999999997,\n",
       "  0.89939999999999998,\n",
       "  0.89870000000000005,\n",
       "  0.89890000000000003,\n",
       "  0.89900000000000002,\n",
       "  0.89890000000000003,\n",
       "  0.89880000000000004,\n",
       "  0.89929999999999999,\n",
       "  0.89990000000000003],\n",
       " 'val_loss': [1.6290396179199218,\n",
       "  1.0974550788879394,\n",
       "  0.86135747814178465,\n",
       "  1.1223259395599365,\n",
       "  1.2395962468147277,\n",
       "  0.87976081390380856,\n",
       "  0.81047158575057987,\n",
       "  0.63298860225677489,\n",
       "  0.52857776603698725,\n",
       "  0.92763197197914127,\n",
       "  0.77429340996742246,\n",
       "  0.68281930875778196,\n",
       "  0.63407366619110106,\n",
       "  0.60200386724472044,\n",
       "  0.76501079711914066,\n",
       "  0.7474588916778564,\n",
       "  0.52402661724090571,\n",
       "  0.53610511274337769,\n",
       "  0.58145004634857178,\n",
       "  0.48641567630767824,\n",
       "  0.56039879498481748,\n",
       "  0.48942719583511352,\n",
       "  0.62148433609008791,\n",
       "  0.56241670172214508,\n",
       "  0.41965029315948488,\n",
       "  0.49231105213165283,\n",
       "  0.50819765148162843,\n",
       "  0.56833849725723262,\n",
       "  0.43189923782348633,\n",
       "  0.40109563689231875,\n",
       "  0.51280583248138423,\n",
       "  0.64576169157028196,\n",
       "  0.49519560213088987,\n",
       "  0.53073125658035281,\n",
       "  0.45228718818426134,\n",
       "  0.46560015492439272,\n",
       "  0.49338647675514219,\n",
       "  0.47678253412246702,\n",
       "  0.65471650757789612,\n",
       "  0.40371660242080687,\n",
       "  0.42426775546073914,\n",
       "  0.53260400042533873,\n",
       "  0.43607266554832458,\n",
       "  0.62061083221435542,\n",
       "  0.45194998140335085,\n",
       "  0.32305340332984922,\n",
       "  0.3303196862578392,\n",
       "  0.34101229732036592,\n",
       "  0.33106837985515597,\n",
       "  0.33366652307510375,\n",
       "  0.34546662316322324,\n",
       "  0.33834450850486758,\n",
       "  0.33690602059364316,\n",
       "  0.34080143032073973,\n",
       "  0.33954188179969785,\n",
       "  0.3387676114797592,\n",
       "  0.32974318882226944,\n",
       "  0.35294038805961608,\n",
       "  0.35183924467563626,\n",
       "  0.35234433805942533,\n",
       "  0.36093835463523866,\n",
       "  0.34609980778694155,\n",
       "  0.36184190621376039,\n",
       "  0.37232997276782992,\n",
       "  0.37119639310836794,\n",
       "  0.33913208079338075,\n",
       "  0.33688098983764647,\n",
       "  0.35509206253290176,\n",
       "  0.36466729483604432,\n",
       "  0.36816293374300002,\n",
       "  0.36118263096809389,\n",
       "  0.3401462980747223,\n",
       "  0.3500927647590637,\n",
       "  0.3915828004360199,\n",
       "  0.35259134049415586,\n",
       "  0.35926256318092348,\n",
       "  0.35556262004375455,\n",
       "  0.36568643736839296,\n",
       "  0.35683698694705962,\n",
       "  0.37452842769622802,\n",
       "  0.37332688910961154,\n",
       "  0.35875577859878538,\n",
       "  0.36615411338806153,\n",
       "  0.36596272597312929,\n",
       "  0.36961054763793944,\n",
       "  0.3505131151199341,\n",
       "  0.34397228994369505,\n",
       "  0.34096820192337035,\n",
       "  0.34071859006881716,\n",
       "  0.34120446641445162,\n",
       "  0.34256885862350461,\n",
       "  0.34877846250534056,\n",
       "  0.34315791301727294,\n",
       "  0.34139988021850587,\n",
       "  0.34640620980262754,\n",
       "  0.34804005122184756,\n",
       "  0.35090694966316222,\n",
       "  0.3441815207481384,\n",
       "  0.34795212798118591,\n",
       "  0.34943905436992645,\n",
       "  0.35082969417572024,\n",
       "  0.3497772210121155,\n",
       "  0.34987991034984589,\n",
       "  0.35331451468467712,\n",
       "  0.35202934346199033,\n",
       "  0.34967165331840516,\n",
       "  0.34916613383293149,\n",
       "  0.35167178483009337,\n",
       "  0.35047896625995634,\n",
       "  0.35052124693393705,\n",
       "  0.35193892741203309,\n",
       "  0.35040552048683166,\n",
       "  0.35077358243465423,\n",
       "  0.35042806297540663,\n",
       "  0.35145947589874266,\n",
       "  0.34928653752803801,\n",
       "  0.3492220881462097,\n",
       "  0.35201773662567137,\n",
       "  0.35137236166000368,\n",
       "  0.34977285642623901,\n",
       "  0.35015249371528623,\n",
       "  0.35195190677642824,\n",
       "  0.35197277195453641,\n",
       "  0.35149095153808596,\n",
       "  0.351701328086853]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/125\n",
      "390/390 [==============================] - 35s 91ms/step - loss: 1.4940 - acc: 0.4544 - val_loss: 1.4668 - val_acc: 0.5072\n",
      "Epoch 2/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 1.0675 - acc: 0.6198 - val_loss: 1.4434 - val_acc: 0.5801\n",
      "Epoch 3/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.8773 - acc: 0.6913 - val_loss: 1.0260 - val_acc: 0.6340\n",
      "Epoch 4/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.7745 - acc: 0.7304 - val_loss: 0.7134 - val_acc: 0.7496\n",
      "Epoch 5/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.7033 - acc: 0.7547 - val_loss: 1.0043 - val_acc: 0.6880\n",
      "Epoch 6/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6596 - acc: 0.7692 - val_loss: 0.7626 - val_acc: 0.7510\n",
      "Epoch 7/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.6146 - acc: 0.7872 - val_loss: 0.9213 - val_acc: 0.7130\n",
      "Epoch 8/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5855 - acc: 0.7989 - val_loss: 0.6410 - val_acc: 0.7880\n",
      "Epoch 9/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5542 - acc: 0.8080 - val_loss: 0.6857 - val_acc: 0.7756\n",
      "Epoch 10/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5342 - acc: 0.8142 - val_loss: 0.5552 - val_acc: 0.8056\n",
      "Epoch 11/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.5097 - acc: 0.8227 - val_loss: 0.5380 - val_acc: 0.8174\n",
      "Epoch 12/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4950 - acc: 0.8301 - val_loss: 0.9062 - val_acc: 0.7365\n",
      "Epoch 13/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4731 - acc: 0.8357 - val_loss: 0.5462 - val_acc: 0.8130\n",
      "Epoch 14/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4660 - acc: 0.8387 - val_loss: 0.6795 - val_acc: 0.7935\n",
      "Epoch 15/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4525 - acc: 0.8426 - val_loss: 0.6226 - val_acc: 0.8047\n",
      "Epoch 16/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4352 - acc: 0.8495 - val_loss: 0.8488 - val_acc: 0.7426\n",
      "Epoch 17/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4201 - acc: 0.8533 - val_loss: 0.4966 - val_acc: 0.8326\n",
      "Epoch 18/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4152 - acc: 0.8560 - val_loss: 0.5314 - val_acc: 0.8291\n",
      "Epoch 19/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.4062 - acc: 0.8597 - val_loss: 0.5018 - val_acc: 0.8347\n",
      "Epoch 20/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3916 - acc: 0.8635 - val_loss: 0.4955 - val_acc: 0.8357\n",
      "Epoch 21/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3904 - acc: 0.8658 - val_loss: 0.5916 - val_acc: 0.8194\n",
      "Epoch 22/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3759 - acc: 0.8687 - val_loss: 0.7434 - val_acc: 0.7830\n",
      "Epoch 23/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3739 - acc: 0.8701 - val_loss: 0.5775 - val_acc: 0.8179\n",
      "Epoch 24/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3641 - acc: 0.8746 - val_loss: 0.4382 - val_acc: 0.8557\n",
      "Epoch 25/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3493 - acc: 0.8778 - val_loss: 0.5093 - val_acc: 0.8445\n",
      "Epoch 26/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3498 - acc: 0.8786 - val_loss: 0.3935 - val_acc: 0.8675\n",
      "Epoch 27/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3450 - acc: 0.8814 - val_loss: 0.4646 - val_acc: 0.8496\n",
      "Epoch 28/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3379 - acc: 0.8833 - val_loss: 0.5722 - val_acc: 0.8183\n",
      "Epoch 29/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3367 - acc: 0.8834 - val_loss: 0.4647 - val_acc: 0.8549\n",
      "Epoch 30/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3233 - acc: 0.8877 - val_loss: 0.4972 - val_acc: 0.8361\n",
      "Epoch 31/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3256 - acc: 0.8868 - val_loss: 0.4281 - val_acc: 0.8583\n",
      "Epoch 32/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3179 - acc: 0.8904 - val_loss: 0.4584 - val_acc: 0.8557\n",
      "Epoch 33/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3130 - acc: 0.8906 - val_loss: 0.3678 - val_acc: 0.8787\n",
      "Epoch 34/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3077 - acc: 0.8918 - val_loss: 0.4817 - val_acc: 0.8495\n",
      "Epoch 35/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3054 - acc: 0.8931 - val_loss: 0.4569 - val_acc: 0.8573\n",
      "Epoch 36/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.3002 - acc: 0.8954 - val_loss: 0.5151 - val_acc: 0.8433\n",
      "Epoch 37/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2942 - acc: 0.8989 - val_loss: 0.4414 - val_acc: 0.8565\n",
      "Epoch 38/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2899 - acc: 0.8985 - val_loss: 0.4592 - val_acc: 0.8559\n",
      "Epoch 39/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2867 - acc: 0.8997 - val_loss: 0.4689 - val_acc: 0.8621\n",
      "Epoch 40/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2824 - acc: 0.9031 - val_loss: 0.5055 - val_acc: 0.8519\n",
      "Epoch 41/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2847 - acc: 0.9003 - val_loss: 0.4960 - val_acc: 0.8507\n",
      "Epoch 42/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2754 - acc: 0.9044 - val_loss: 0.4004 - val_acc: 0.8709\n",
      "Epoch 43/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2758 - acc: 0.9042 - val_loss: 0.6340 - val_acc: 0.8115\n",
      "Epoch 44/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2699 - acc: 0.9056 - val_loss: 0.4916 - val_acc: 0.8547\n",
      "Epoch 45/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2699 - acc: 0.9056 - val_loss: 0.4899 - val_acc: 0.8410\n",
      "Epoch 46/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2183 - acc: 0.9236 - val_loss: 0.3440 - val_acc: 0.8891\n",
      "Epoch 47/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.2024 - acc: 0.9295 - val_loss: 0.3353 - val_acc: 0.8911\n",
      "Epoch 48/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1937 - acc: 0.9340 - val_loss: 0.3382 - val_acc: 0.8948\n",
      "Epoch 49/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1915 - acc: 0.9334 - val_loss: 0.3200 - val_acc: 0.8991\n",
      "Epoch 50/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1855 - acc: 0.9359 - val_loss: 0.3382 - val_acc: 0.8968\n",
      "Epoch 51/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1885 - acc: 0.9339 - val_loss: 0.3409 - val_acc: 0.8952\n",
      "Epoch 52/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1839 - acc: 0.9344 - val_loss: 0.3460 - val_acc: 0.8971\n",
      "Epoch 53/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1804 - acc: 0.9373 - val_loss: 0.3294 - val_acc: 0.8998\n",
      "Epoch 54/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1819 - acc: 0.9354 - val_loss: 0.3318 - val_acc: 0.8990\n",
      "Epoch 55/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1788 - acc: 0.9373 - val_loss: 0.3343 - val_acc: 0.8997\n",
      "Epoch 56/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1746 - acc: 0.9392 - val_loss: 0.3411 - val_acc: 0.8963\n",
      "Epoch 57/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1793 - acc: 0.9364 - val_loss: 0.3240 - val_acc: 0.9013\n",
      "Epoch 58/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1755 - acc: 0.9379 - val_loss: 0.3527 - val_acc: 0.8946\n",
      "Epoch 59/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1721 - acc: 0.9396 - val_loss: 0.3325 - val_acc: 0.9003\n",
      "Epoch 60/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1723 - acc: 0.9407 - val_loss: 0.3421 - val_acc: 0.8974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1706 - acc: 0.9395 - val_loss: 0.3379 - val_acc: 0.8975\n",
      "Epoch 62/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1709 - acc: 0.9402 - val_loss: 0.3501 - val_acc: 0.8959\n",
      "Epoch 63/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1704 - acc: 0.9392 - val_loss: 0.3442 - val_acc: 0.8973\n",
      "Epoch 64/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1682 - acc: 0.9407 - val_loss: 0.3307 - val_acc: 0.9019\n",
      "Epoch 65/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1658 - acc: 0.9410 - val_loss: 0.3424 - val_acc: 0.8994\n",
      "Epoch 66/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1659 - acc: 0.9415 - val_loss: 0.3350 - val_acc: 0.8998\n",
      "Epoch 67/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1647 - acc: 0.9423 - val_loss: 0.3375 - val_acc: 0.8990\n",
      "Epoch 68/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1660 - acc: 0.9405 - val_loss: 0.3521 - val_acc: 0.8967\n",
      "Epoch 69/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1638 - acc: 0.9413 - val_loss: 0.3403 - val_acc: 0.9018\n",
      "Epoch 70/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1628 - acc: 0.9421 - val_loss: 0.3489 - val_acc: 0.8983\n",
      "Epoch 71/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1637 - acc: 0.9420 - val_loss: 0.3417 - val_acc: 0.8957\n",
      "Epoch 72/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1623 - acc: 0.9438 - val_loss: 0.3616 - val_acc: 0.8942\n",
      "Epoch 73/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1634 - acc: 0.9424 - val_loss: 0.3488 - val_acc: 0.8994\n",
      "Epoch 74/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1560 - acc: 0.9449 - val_loss: 0.3448 - val_acc: 0.8972\n",
      "Epoch 75/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1584 - acc: 0.9432 - val_loss: 0.3559 - val_acc: 0.8957\n",
      "Epoch 76/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1549 - acc: 0.9457 - val_loss: 0.3442 - val_acc: 0.8986\n",
      "Epoch 77/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1576 - acc: 0.9449 - val_loss: 0.3879 - val_acc: 0.8907\n",
      "Epoch 78/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1562 - acc: 0.9449 - val_loss: 0.3518 - val_acc: 0.8967\n",
      "Epoch 79/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1577 - acc: 0.9444 - val_loss: 0.3485 - val_acc: 0.8979\n",
      "Epoch 80/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1495 - acc: 0.9472 - val_loss: 0.3479 - val_acc: 0.8995\n",
      "Epoch 81/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1543 - acc: 0.9462 - val_loss: 0.3631 - val_acc: 0.8951\n",
      "Epoch 82/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1556 - acc: 0.9443 - val_loss: 0.3603 - val_acc: 0.8962\n",
      "Epoch 83/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1511 - acc: 0.9468 - val_loss: 0.3661 - val_acc: 0.8930\n",
      "Epoch 84/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1525 - acc: 0.9457 - val_loss: 0.3963 - val_acc: 0.8914\n",
      "Epoch 85/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1553 - acc: 0.9447 - val_loss: 0.3600 - val_acc: 0.8958\n",
      "Epoch 86/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1436 - acc: 0.9499 - val_loss: 0.3323 - val_acc: 0.9022\n",
      "Epoch 87/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1352 - acc: 0.9528 - val_loss: 0.3357 - val_acc: 0.9020\n",
      "Epoch 88/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1381 - acc: 0.9511 - val_loss: 0.3346 - val_acc: 0.9039\n",
      "Epoch 89/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1337 - acc: 0.9527 - val_loss: 0.3414 - val_acc: 0.9015\n",
      "Epoch 90/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1360 - acc: 0.9518 - val_loss: 0.3343 - val_acc: 0.9027\n",
      "Epoch 91/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1337 - acc: 0.9532 - val_loss: 0.3382 - val_acc: 0.9020\n",
      "Epoch 92/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1299 - acc: 0.9543 - val_loss: 0.3356 - val_acc: 0.9032\n",
      "Epoch 93/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1314 - acc: 0.9523 - val_loss: 0.3378 - val_acc: 0.9035\n",
      "Epoch 94/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1335 - acc: 0.9540 - val_loss: 0.3395 - val_acc: 0.9042\n",
      "Epoch 95/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1347 - acc: 0.9524 - val_loss: 0.3403 - val_acc: 0.9033\n",
      "Epoch 96/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1299 - acc: 0.9547 - val_loss: 0.3330 - val_acc: 0.9039\n",
      "Epoch 97/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1301 - acc: 0.9554 - val_loss: 0.3357 - val_acc: 0.9034\n",
      "Epoch 98/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1290 - acc: 0.9547 - val_loss: 0.3408 - val_acc: 0.9031\n",
      "Epoch 99/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1301 - acc: 0.9547 - val_loss: 0.3434 - val_acc: 0.9022\n",
      "Epoch 100/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1308 - acc: 0.9542 - val_loss: 0.3372 - val_acc: 0.9048\n",
      "Epoch 101/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1296 - acc: 0.9547 - val_loss: 0.3406 - val_acc: 0.9019\n",
      "Epoch 102/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1266 - acc: 0.9560 - val_loss: 0.3414 - val_acc: 0.9026\n",
      "Epoch 103/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1281 - acc: 0.9546 - val_loss: 0.3447 - val_acc: 0.9023\n",
      "Epoch 104/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1285 - acc: 0.9545 - val_loss: 0.3361 - val_acc: 0.9049\n",
      "Epoch 105/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1272 - acc: 0.9558 - val_loss: 0.3422 - val_acc: 0.9034\n",
      "Epoch 106/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1266 - acc: 0.9543 - val_loss: 0.3398 - val_acc: 0.9048\n",
      "Epoch 107/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1261 - acc: 0.9548 - val_loss: 0.3372 - val_acc: 0.9046\n",
      "Epoch 108/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1264 - acc: 0.9565 - val_loss: 0.3388 - val_acc: 0.9039\n",
      "Epoch 109/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1241 - acc: 0.9555 - val_loss: 0.3390 - val_acc: 0.9048\n",
      "Epoch 110/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1257 - acc: 0.9555 - val_loss: 0.3385 - val_acc: 0.9045\n",
      "Epoch 111/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1256 - acc: 0.9560 - val_loss: 0.3384 - val_acc: 0.9048\n",
      "Epoch 112/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1248 - acc: 0.9557 - val_loss: 0.3392 - val_acc: 0.9049\n",
      "Epoch 113/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1227 - acc: 0.9566 - val_loss: 0.3383 - val_acc: 0.9049\n",
      "Epoch 114/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1232 - acc: 0.9561 - val_loss: 0.3375 - val_acc: 0.9042\n",
      "Epoch 115/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1252 - acc: 0.9561 - val_loss: 0.3402 - val_acc: 0.9042\n",
      "Epoch 116/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1237 - acc: 0.9576 - val_loss: 0.3398 - val_acc: 0.9039\n",
      "Epoch 117/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1242 - acc: 0.9559 - val_loss: 0.3398 - val_acc: 0.9044\n",
      "Epoch 118/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1220 - acc: 0.9566 - val_loss: 0.3386 - val_acc: 0.9047\n",
      "Epoch 119/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1248 - acc: 0.9556 - val_loss: 0.3376 - val_acc: 0.9040\n",
      "Epoch 120/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1247 - acc: 0.9569 - val_loss: 0.3393 - val_acc: 0.9046\n",
      "Epoch 121/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1221 - acc: 0.9572 - val_loss: 0.3386 - val_acc: 0.9045\n",
      "Epoch 122/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1239 - acc: 0.9561 - val_loss: 0.3379 - val_acc: 0.9046\n",
      "Epoch 123/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1234 - acc: 0.9573 - val_loss: 0.3399 - val_acc: 0.9049\n",
      "Epoch 124/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1234 - acc: 0.9567 - val_loss: 0.3410 - val_acc: 0.9030\n",
      "Epoch 125/125\n",
      "390/390 [==============================] - 35s 90ms/step - loss: 0.1238 - acc: 0.9559 - val_loss: 0.3389 - val_acc: 0.9035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.45432306705165221,\n",
       "  0.61974655117099775,\n",
       "  0.69128970163618864,\n",
       "  0.73044871794871791,\n",
       "  0.75463631980757573,\n",
       "  0.76924927813294686,\n",
       "  0.78715511709977537,\n",
       "  0.79894529998704034,\n",
       "  0.80797275641025645,\n",
       "  0.81412572256249349,\n",
       "  0.82274623033057281,\n",
       "  0.83010506899570247,\n",
       "  0.83565928779583076,\n",
       "  0.83868703878113271,\n",
       "  0.84255694578119988,\n",
       "  0.84949470644850822,\n",
       "  0.85334456211716692,\n",
       "  0.85603144052589331,\n",
       "  0.85982114208559213,\n",
       "  0.86345043314700332,\n",
       "  0.86579643886441948,\n",
       "  0.86874398460057745,\n",
       "  0.870147577760921,\n",
       "  0.87459935897435892,\n",
       "  0.87785003211303791,\n",
       "  0.87862929098492137,\n",
       "  0.88141642603156733,\n",
       "  0.88326114855938553,\n",
       "  0.88346166187346953,\n",
       "  0.88767244148835722,\n",
       "  0.886810234161306,\n",
       "  0.89037937115200216,\n",
       "  0.89064003847943385,\n",
       "  0.89178685897435894,\n",
       "  0.89312379570365641,\n",
       "  0.89545235799832879,\n",
       "  0.89886108435688017,\n",
       "  0.89854026307346813,\n",
       "  0.89972329160744158,\n",
       "  0.90313201800423781,\n",
       "  0.90036493426987785,\n",
       "  0.90439525184472247,\n",
       "  0.90423484119345521,\n",
       "  0.90559833176747173,\n",
       "  0.90551812638447071,\n",
       "  0.92352422197009643,\n",
       "  0.92949951878716863,\n",
       "  0.93403111966634589,\n",
       "  0.93334937443670496,\n",
       "  0.93585579082451076,\n",
       "  0.93393086304754869,\n",
       "  0.93439204359345229,\n",
       "  0.93729948668591589,\n",
       "  0.93543471282668933,\n",
       "  0.93727943537363001,\n",
       "  0.93918431185742846,\n",
       "  0.93643727943535449,\n",
       "  0.93782082132165689,\n",
       "  0.93954523576541249,\n",
       "  0.94060795638742534,\n",
       "  0.93952518447224898,\n",
       "  0.94014677570766469,\n",
       "  0.93926451720218451,\n",
       "  0.94074519230769227,\n",
       "  0.94102903436612428,\n",
       "  0.94147398840102459,\n",
       "  0.94235242221995652,\n",
       "  0.94052775104266928,\n",
       "  0.94126965026653531,\n",
       "  0.9421318575362192,\n",
       "  0.94199149825460526,\n",
       "  0.94381616943189262,\n",
       "  0.94233237090767064,\n",
       "  0.94487888995829328,\n",
       "  0.94321462945139556,\n",
       "  0.945660891883221,\n",
       "  0.9448788899391708,\n",
       "  0.94491899260198764,\n",
       "  0.94443776064818585,\n",
       "  0.94724494704536266,\n",
       "  0.94614212379877793,\n",
       "  0.94433092948717945,\n",
       "  0.94683285167604669,\n",
       "  0.94566089190234348,\n",
       "  0.94469842799474002,\n",
       "  0.94991987179487181,\n",
       "  0.95281390496454865,\n",
       "  0.95114182692307692,\n",
       "  0.95269348105330764,\n",
       "  0.95177654798190714,\n",
       "  0.95318014116137306,\n",
       "  0.95434695512820511,\n",
       "  0.95233220935786622,\n",
       "  0.95396214312454586,\n",
       "  0.95249839585524243,\n",
       "  0.95476419638088206,\n",
       "  0.95540583896682851,\n",
       "  0.9547275641025641,\n",
       "  0.95474068716158833,\n",
       "  0.95424286172601858,\n",
       "  0.95470404236753437,\n",
       "  0.95594722485748818,\n",
       "  0.95464743589743595,\n",
       "  0.95456005138086064,\n",
       "  0.95574671154340418,\n",
       "  0.95428685897435894,\n",
       "  0.95488118179807624,\n",
       "  0.95650866215604602,\n",
       "  0.95554619826756493,\n",
       "  0.95552614691703408,\n",
       "  0.95600737888995835,\n",
       "  0.95574919871794872,\n",
       "  0.95656711628747892,\n",
       "  0.95602743024048908,\n",
       "  0.95604748159101993,\n",
       "  0.95763221153846156,\n",
       "  0.95586464346871591,\n",
       "  0.9566490214759048,\n",
       "  0.9556063523000351,\n",
       "  0.95687099358974359,\n",
       "  0.95720937694964225,\n",
       "  0.95612768685928629,\n",
       "  0.95725160256410258,\n",
       "  0.95670761080912159,\n",
       "  0.95594722489573303],\n",
       " 'loss': [1.4941449515667335,\n",
       "  1.0676496052611972,\n",
       "  0.87737322805935192,\n",
       "  0.77451441165728452,\n",
       "  0.70323767316364372,\n",
       "  0.65949667440915594,\n",
       "  0.61463906815084923,\n",
       "  0.58543997190302288,\n",
       "  0.55422323017548292,\n",
       "  0.53429599338873268,\n",
       "  0.50972246365513529,\n",
       "  0.4948595114384548,\n",
       "  0.4731375338712257,\n",
       "  0.46610657566244645,\n",
       "  0.45259230124250527,\n",
       "  0.43522043192008003,\n",
       "  0.42008788571512234,\n",
       "  0.41514647531899496,\n",
       "  0.40603918198557054,\n",
       "  0.39177627178350627,\n",
       "  0.39039441036764994,\n",
       "  0.37581925221769941,\n",
       "  0.37382129511658796,\n",
       "  0.36414319689457231,\n",
       "  0.34921508105848564,\n",
       "  0.34981506550430602,\n",
       "  0.34498734472461107,\n",
       "  0.33801505379295133,\n",
       "  0.33656767526038534,\n",
       "  0.32338332562902472,\n",
       "  0.32566671496520716,\n",
       "  0.31786590310714163,\n",
       "  0.31282908581065794,\n",
       "  0.30771068858030515,\n",
       "  0.30534750463377608,\n",
       "  0.30004104961368339,\n",
       "  0.29435665256746846,\n",
       "  0.28982381333821394,\n",
       "  0.28673705919013792,\n",
       "  0.28241329789544128,\n",
       "  0.28460031132075431,\n",
       "  0.27528687648984307,\n",
       "  0.27569420303447834,\n",
       "  0.26990710564595904,\n",
       "  0.27003113378077165,\n",
       "  0.21839788139229752,\n",
       "  0.20229583434619541,\n",
       "  0.1937020709442335,\n",
       "  0.19158002432766807,\n",
       "  0.18545767932623519,\n",
       "  0.18837902169790566,\n",
       "  0.18387745327011759,\n",
       "  0.18044384954485865,\n",
       "  0.18178415084290589,\n",
       "  0.17893662803976973,\n",
       "  0.17466064899404499,\n",
       "  0.17917931777833554,\n",
       "  0.17553495215822726,\n",
       "  0.17213094107302329,\n",
       "  0.17233938241199992,\n",
       "  0.17053347651989825,\n",
       "  0.17085941771226393,\n",
       "  0.17038083448408503,\n",
       "  0.16824150658570802,\n",
       "  0.16564103970552127,\n",
       "  0.16597579025311124,\n",
       "  0.16467234101248049,\n",
       "  0.16608593318184714,\n",
       "  0.163859992485552,\n",
       "  0.16282467847169496,\n",
       "  0.16365943158288623,\n",
       "  0.16228382774641548,\n",
       "  0.16342385924508795,\n",
       "  0.15599995710488732,\n",
       "  0.15846385317503509,\n",
       "  0.15493276506844031,\n",
       "  0.15759216021193043,\n",
       "  0.15625483829517336,\n",
       "  0.15775322807037873,\n",
       "  0.14954731279950054,\n",
       "  0.15434536252996653,\n",
       "  0.15558387954265643,\n",
       "  0.1509761233779966,\n",
       "  0.15255663380203699,\n",
       "  0.15528338312873252,\n",
       "  0.14358709715306758,\n",
       "  0.13513914089441759,\n",
       "  0.13808738282666758,\n",
       "  0.13370474371233657,\n",
       "  0.13594373887094657,\n",
       "  0.13367601140095403,\n",
       "  0.12989656678759134,\n",
       "  0.13120794664726246,\n",
       "  0.1334977022201578,\n",
       "  0.13468545696790454,\n",
       "  0.1298877412419232,\n",
       "  0.13017634226605032,\n",
       "  0.12897023669420143,\n",
       "  0.13011879490433631,\n",
       "  0.13071343939025928,\n",
       "  0.12972888478726616,\n",
       "  0.12655980224348545,\n",
       "  0.12811106555163859,\n",
       "  0.12845986494142503,\n",
       "  0.12724479428651742,\n",
       "  0.12663568482758142,\n",
       "  0.12607099790324672,\n",
       "  0.12644028759247453,\n",
       "  0.12407824414297293,\n",
       "  0.12566709398917369,\n",
       "  0.12554798058431654,\n",
       "  0.124843936222486,\n",
       "  0.1227482211852043,\n",
       "  0.12330609263495486,\n",
       "  0.1252122879754973,\n",
       "  0.12369959320968542,\n",
       "  0.12419329486504992,\n",
       "  0.12200565155228629,\n",
       "  0.1248202973837496,\n",
       "  0.1247186659811399,\n",
       "  0.12204709602430147,\n",
       "  0.12390446355509613,\n",
       "  0.12343660538586286,\n",
       "  0.12336832769669702,\n",
       "  0.12376571626169085],\n",
       " 'val_acc': [0.50719999999999998,\n",
       "  0.58009999999999995,\n",
       "  0.63400000000000001,\n",
       "  0.74960000000000004,\n",
       "  0.68799999999999994,\n",
       "  0.751,\n",
       "  0.71299999999999997,\n",
       "  0.78800000000000003,\n",
       "  0.77559999999999996,\n",
       "  0.80559999999999998,\n",
       "  0.81740000000000002,\n",
       "  0.73650000000000004,\n",
       "  0.81299999999999994,\n",
       "  0.79349999999999998,\n",
       "  0.80469999999999997,\n",
       "  0.74260000000000004,\n",
       "  0.83260000000000001,\n",
       "  0.82909999999999995,\n",
       "  0.8347,\n",
       "  0.8357,\n",
       "  0.81940000000000002,\n",
       "  0.78300000000000003,\n",
       "  0.81789999999999996,\n",
       "  0.85570000000000002,\n",
       "  0.84450000000000003,\n",
       "  0.86750000000000005,\n",
       "  0.84960000000000002,\n",
       "  0.81830000000000003,\n",
       "  0.85489999999999999,\n",
       "  0.83609999999999995,\n",
       "  0.85829999999999995,\n",
       "  0.85570000000000002,\n",
       "  0.87870000000000004,\n",
       "  0.84950000000000003,\n",
       "  0.85729999999999995,\n",
       "  0.84330000000000005,\n",
       "  0.85650000000000004,\n",
       "  0.85589999999999999,\n",
       "  0.86209999999999998,\n",
       "  0.85189999999999999,\n",
       "  0.85070000000000001,\n",
       "  0.87090000000000001,\n",
       "  0.8115,\n",
       "  0.85470000000000002,\n",
       "  0.84099999999999997,\n",
       "  0.8891,\n",
       "  0.8911,\n",
       "  0.89480000000000004,\n",
       "  0.89910000000000001,\n",
       "  0.89680000000000004,\n",
       "  0.8952,\n",
       "  0.89710000000000001,\n",
       "  0.89980000000000004,\n",
       "  0.89900000000000002,\n",
       "  0.89970000000000006,\n",
       "  0.89629999999999999,\n",
       "  0.90129999999999999,\n",
       "  0.89459999999999995,\n",
       "  0.90029999999999999,\n",
       "  0.89739999999999998,\n",
       "  0.89749999999999996,\n",
       "  0.89590000000000003,\n",
       "  0.89729999999999999,\n",
       "  0.90190000000000003,\n",
       "  0.89939999999999998,\n",
       "  0.89980000000000004,\n",
       "  0.89900000000000002,\n",
       "  0.89670000000000005,\n",
       "  0.90180000000000005,\n",
       "  0.89829999999999999,\n",
       "  0.89570000000000005,\n",
       "  0.89419999999999999,\n",
       "  0.89939999999999998,\n",
       "  0.8972,\n",
       "  0.89570000000000005,\n",
       "  0.89859999999999995,\n",
       "  0.89070000000000005,\n",
       "  0.89670000000000005,\n",
       "  0.89790000000000003,\n",
       "  0.89949999999999997,\n",
       "  0.89510000000000001,\n",
       "  0.8962,\n",
       "  0.89300000000000002,\n",
       "  0.89139999999999997,\n",
       "  0.89580000000000004,\n",
       "  0.9022,\n",
       "  0.90200000000000002,\n",
       "  0.90390000000000004,\n",
       "  0.90149999999999997,\n",
       "  0.90269999999999995,\n",
       "  0.90200000000000002,\n",
       "  0.9032,\n",
       "  0.90349999999999997,\n",
       "  0.9042,\n",
       "  0.90329999999999999,\n",
       "  0.90390000000000004,\n",
       "  0.90339999999999998,\n",
       "  0.90310000000000001,\n",
       "  0.9022,\n",
       "  0.90480000000000005,\n",
       "  0.90190000000000003,\n",
       "  0.90259999999999996,\n",
       "  0.90229999999999999,\n",
       "  0.90490000000000004,\n",
       "  0.90339999999999998,\n",
       "  0.90480000000000005,\n",
       "  0.90459999999999996,\n",
       "  0.90390000000000004,\n",
       "  0.90480000000000005,\n",
       "  0.90449999999999997,\n",
       "  0.90480000000000005,\n",
       "  0.90490000000000004,\n",
       "  0.90490000000000004,\n",
       "  0.9042,\n",
       "  0.9042,\n",
       "  0.90390000000000004,\n",
       "  0.90439999999999998,\n",
       "  0.90469999999999995,\n",
       "  0.90400000000000003,\n",
       "  0.90459999999999996,\n",
       "  0.90449999999999997,\n",
       "  0.90459999999999996,\n",
       "  0.90490000000000004,\n",
       "  0.90300000000000002,\n",
       "  0.90349999999999997],\n",
       " 'val_loss': [1.4667663772583008,\n",
       "  1.4433705123901368,\n",
       "  1.0259819515228272,\n",
       "  0.71340259609222412,\n",
       "  1.0043070800781251,\n",
       "  0.76262585430145269,\n",
       "  0.92126553878784179,\n",
       "  0.64095609598159786,\n",
       "  0.68571142463684087,\n",
       "  0.55520332651138304,\n",
       "  0.53804019470214848,\n",
       "  0.90619456920623775,\n",
       "  0.54617919979095464,\n",
       "  0.67949824085235599,\n",
       "  0.6226112384796143,\n",
       "  0.84877609062194825,\n",
       "  0.49660934629440306,\n",
       "  0.5313543676137924,\n",
       "  0.50176295971870422,\n",
       "  0.49551408371925354,\n",
       "  0.59159865789413457,\n",
       "  0.74343881616592411,\n",
       "  0.57750684919357298,\n",
       "  0.43816891000270841,\n",
       "  0.50934552006721501,\n",
       "  0.39350912289619444,\n",
       "  0.46455815782547,\n",
       "  0.57223698425292968,\n",
       "  0.46474908063411713,\n",
       "  0.49720368394851683,\n",
       "  0.42811489753723142,\n",
       "  0.45837286181449888,\n",
       "  0.36777081322669986,\n",
       "  0.4816590934753418,\n",
       "  0.45691875867843629,\n",
       "  0.51509282550811764,\n",
       "  0.44141390252113344,\n",
       "  0.45916364660263059,\n",
       "  0.46889755401611327,\n",
       "  0.50549774303436279,\n",
       "  0.49597130200862882,\n",
       "  0.40040699129104612,\n",
       "  0.63398381633758549,\n",
       "  0.4915886875152588,\n",
       "  0.48988865551948546,\n",
       "  0.34399163084030149,\n",
       "  0.33532468453645708,\n",
       "  0.33820859205722809,\n",
       "  0.32001571340560914,\n",
       "  0.33819579849243164,\n",
       "  0.34091283711194992,\n",
       "  0.34602269974946975,\n",
       "  0.3294181780576706,\n",
       "  0.3317686687231064,\n",
       "  0.33430084795951842,\n",
       "  0.34107609279155732,\n",
       "  0.32400547906160354,\n",
       "  0.35272951979637146,\n",
       "  0.33250422203540803,\n",
       "  0.34209424765110014,\n",
       "  0.33793554573059081,\n",
       "  0.35014632840156557,\n",
       "  0.34416548912525174,\n",
       "  0.33069564346075059,\n",
       "  0.3423736432790756,\n",
       "  0.33503563647270201,\n",
       "  0.33751976377964021,\n",
       "  0.35213136074543,\n",
       "  0.3402840262413025,\n",
       "  0.3488902460575104,\n",
       "  0.34169396427869797,\n",
       "  0.36157536764144899,\n",
       "  0.34883765169382097,\n",
       "  0.34483581571578981,\n",
       "  0.3559135093331337,\n",
       "  0.34424018225669861,\n",
       "  0.38793368656635285,\n",
       "  0.35180074222087859,\n",
       "  0.34852465331554411,\n",
       "  0.34794758148193361,\n",
       "  0.36313914645910261,\n",
       "  0.36028519200086595,\n",
       "  0.36613351924419402,\n",
       "  0.39626655142307282,\n",
       "  0.35995953938961028,\n",
       "  0.33226784056425096,\n",
       "  0.33568820017576217,\n",
       "  0.33458572243452073,\n",
       "  0.34142194805145265,\n",
       "  0.33427650340795517,\n",
       "  0.33824771113395691,\n",
       "  0.33562461107969283,\n",
       "  0.33779569162130357,\n",
       "  0.33946551039218903,\n",
       "  0.34029984579086303,\n",
       "  0.33295268938541411,\n",
       "  0.33570328483581541,\n",
       "  0.34084282279014588,\n",
       "  0.34336859846115114,\n",
       "  0.33721726210117342,\n",
       "  0.34064538643360137,\n",
       "  0.34141445600986481,\n",
       "  0.34466882096529006,\n",
       "  0.33614954862594604,\n",
       "  0.34219070270061491,\n",
       "  0.33978556051254272,\n",
       "  0.33715916492938997,\n",
       "  0.33881438276767728,\n",
       "  0.33902114129066468,\n",
       "  0.33850633695125582,\n",
       "  0.33837733142375948,\n",
       "  0.33917319144010544,\n",
       "  0.33826246463060378,\n",
       "  0.33754450323581697,\n",
       "  0.34022639796733856,\n",
       "  0.33976066454648973,\n",
       "  0.33982035071849825,\n",
       "  0.33862323629856111,\n",
       "  0.33757732980251315,\n",
       "  0.33934669398069384,\n",
       "  0.33860851526260377,\n",
       "  0.33787282441854477,\n",
       "  0.33992407149076465,\n",
       "  0.34103172414302824,\n",
       "  0.33887350826263429]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using He initialization\n",
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/125\n",
      "390/390 [==============================] - 28s 72ms/step - loss: 1.5196 - acc: 0.4377 - val_loss: 1.7876 - val_acc: 0.4188\n",
      "Epoch 2/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 1.0615 - acc: 0.6218 - val_loss: 1.5348 - val_acc: 0.5052\n",
      "Epoch 3/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.8887 - acc: 0.6855 - val_loss: 0.8636 - val_acc: 0.6911\n",
      "Epoch 4/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.7712 - acc: 0.7269 - val_loss: 0.9352 - val_acc: 0.7051\n",
      "Epoch 5/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.6919 - acc: 0.7570 - val_loss: 0.9088 - val_acc: 0.7029\n",
      "Epoch 6/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.6338 - acc: 0.7790 - val_loss: 0.8397 - val_acc: 0.7201\n",
      "Epoch 7/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.5915 - acc: 0.7944 - val_loss: 0.6710 - val_acc: 0.7718\n",
      "Epoch 8/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.5600 - acc: 0.8047 - val_loss: 0.6779 - val_acc: 0.7743\n",
      "Epoch 9/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.5291 - acc: 0.8165 - val_loss: 0.6916 - val_acc: 0.7699\n",
      "Epoch 10/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.5036 - acc: 0.8253 - val_loss: 0.8180 - val_acc: 0.7576\n",
      "Epoch 11/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4815 - acc: 0.8321 - val_loss: 0.7202 - val_acc: 0.7696\n",
      "Epoch 12/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4658 - acc: 0.8384 - val_loss: 0.5252 - val_acc: 0.8224\n",
      "Epoch 13/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4422 - acc: 0.8464 - val_loss: 0.5398 - val_acc: 0.8223\n",
      "Epoch 14/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4300 - acc: 0.8494 - val_loss: 0.5208 - val_acc: 0.8308\n",
      "Epoch 15/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4165 - acc: 0.8567 - val_loss: 0.5764 - val_acc: 0.8155\n",
      "Epoch 16/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.4021 - acc: 0.8583 - val_loss: 0.5225 - val_acc: 0.8265\n",
      "Epoch 17/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3900 - acc: 0.8638 - val_loss: 0.6044 - val_acc: 0.8114\n",
      "Epoch 18/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3771 - acc: 0.8688 - val_loss: 0.4385 - val_acc: 0.8550\n",
      "Epoch 19/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3712 - acc: 0.8703 - val_loss: 0.5122 - val_acc: 0.8358\n",
      "Epoch 20/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3591 - acc: 0.8746 - val_loss: 0.4685 - val_acc: 0.8442\n",
      "Epoch 21/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3531 - acc: 0.8774 - val_loss: 0.4835 - val_acc: 0.8396\n",
      "Epoch 22/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3399 - acc: 0.8812 - val_loss: 0.5117 - val_acc: 0.8377\n",
      "Epoch 23/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3302 - acc: 0.8847 - val_loss: 0.5145 - val_acc: 0.8337\n",
      "Epoch 24/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3216 - acc: 0.8879 - val_loss: 0.4674 - val_acc: 0.8490\n",
      "Epoch 25/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.3195 - acc: 0.8886 - val_loss: 0.5263 - val_acc: 0.8295\n",
      "Epoch 26/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3129 - acc: 0.8902 - val_loss: 0.5494 - val_acc: 0.8319\n",
      "Epoch 27/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3046 - acc: 0.8922 - val_loss: 0.5254 - val_acc: 0.8275\n",
      "Epoch 28/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2996 - acc: 0.8961 - val_loss: 0.4757 - val_acc: 0.8495\n",
      "Epoch 29/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2947 - acc: 0.8961 - val_loss: 0.4727 - val_acc: 0.8471\n",
      "Epoch 30/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2824 - acc: 0.9004 - val_loss: 0.4203 - val_acc: 0.8622\n",
      "Epoch 31/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2782 - acc: 0.9038 - val_loss: 0.3731 - val_acc: 0.8778\n",
      "Epoch 32/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2773 - acc: 0.9032 - val_loss: 0.4290 - val_acc: 0.8605\n",
      "Epoch 33/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2632 - acc: 0.9074 - val_loss: 0.4253 - val_acc: 0.8610\n",
      "Epoch 34/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2657 - acc: 0.9062 - val_loss: 0.4291 - val_acc: 0.8663\n",
      "Epoch 35/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2597 - acc: 0.9089 - val_loss: 0.4523 - val_acc: 0.8557\n",
      "Epoch 36/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2516 - acc: 0.9125 - val_loss: 0.4305 - val_acc: 0.8648\n",
      "Epoch 37/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2541 - acc: 0.9114 - val_loss: 0.4814 - val_acc: 0.8463\n",
      "Epoch 38/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2437 - acc: 0.9146 - val_loss: 0.5926 - val_acc: 0.8305\n",
      "Epoch 39/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2408 - acc: 0.9148 - val_loss: 0.4404 - val_acc: 0.8585\n",
      "Epoch 40/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2407 - acc: 0.9142 - val_loss: 0.5083 - val_acc: 0.8508\n",
      "Epoch 41/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2361 - acc: 0.9159 - val_loss: 0.4763 - val_acc: 0.8545\n",
      "Epoch 42/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2325 - acc: 0.9175 - val_loss: 0.3966 - val_acc: 0.8788\n",
      "Epoch 43/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2253 - acc: 0.9199 - val_loss: 0.4259 - val_acc: 0.8676\n",
      "Epoch 44/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.2239 - acc: 0.9205 - val_loss: 0.4871 - val_acc: 0.8545\n",
      "Epoch 45/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.2219 - acc: 0.9210 - val_loss: 0.3760 - val_acc: 0.8821\n",
      "Epoch 46/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1683 - acc: 0.9417 - val_loss: 0.3038 - val_acc: 0.9041\n",
      "Epoch 47/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1591 - acc: 0.9455 - val_loss: 0.3102 - val_acc: 0.9017\n",
      "Epoch 48/125\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.1521 - acc: 0.9479 - val_loss: 0.3096 - val_acc: 0.8995\n",
      "Epoch 49/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1532 - acc: 0.9466 - val_loss: 0.3138 - val_acc: 0.9017\n",
      "Epoch 50/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1511 - acc: 0.9481 - val_loss: 0.3166 - val_acc: 0.8976\n",
      "Epoch 51/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1457 - acc: 0.9500 - val_loss: 0.3050 - val_acc: 0.9021\n",
      "Epoch 52/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1430 - acc: 0.9503 - val_loss: 0.3140 - val_acc: 0.8997\n",
      "Epoch 53/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1432 - acc: 0.9497 - val_loss: 0.3101 - val_acc: 0.9036\n",
      "Epoch 54/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1414 - acc: 0.9511 - val_loss: 0.3148 - val_acc: 0.9009\n",
      "Epoch 55/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1415 - acc: 0.9510 - val_loss: 0.3099 - val_acc: 0.9043\n",
      "Epoch 56/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1363 - acc: 0.9534 - val_loss: 0.2985 - val_acc: 0.9044\n",
      "Epoch 57/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1365 - acc: 0.9520 - val_loss: 0.3110 - val_acc: 0.9050\n",
      "Epoch 58/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1324 - acc: 0.9542 - val_loss: 0.3070 - val_acc: 0.9055\n",
      "Epoch 59/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1347 - acc: 0.9530 - val_loss: 0.3127 - val_acc: 0.9034\n",
      "Epoch 60/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1325 - acc: 0.9542 - val_loss: 0.3007 - val_acc: 0.9031\n",
      "Epoch 61/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1306 - acc: 0.9537 - val_loss: 0.3043 - val_acc: 0.9067\n",
      "Epoch 62/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1287 - acc: 0.9557 - val_loss: 0.3247 - val_acc: 0.8998\n",
      "Epoch 63/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1274 - acc: 0.9557 - val_loss: 0.3304 - val_acc: 0.9018\n",
      "Epoch 64/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1258 - acc: 0.9556 - val_loss: 0.3138 - val_acc: 0.9043\n",
      "Epoch 65/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1285 - acc: 0.9555 - val_loss: 0.3195 - val_acc: 0.9020\n",
      "Epoch 66/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1285 - acc: 0.9546 - val_loss: 0.3169 - val_acc: 0.9011\n",
      "Epoch 67/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1277 - acc: 0.9550 - val_loss: 0.3178 - val_acc: 0.9031\n",
      "Epoch 68/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1229 - acc: 0.9566 - val_loss: 0.3111 - val_acc: 0.9030\n",
      "Epoch 69/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1222 - acc: 0.9576 - val_loss: 0.3166 - val_acc: 0.9056\n",
      "Epoch 70/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1220 - acc: 0.9565 - val_loss: 0.3155 - val_acc: 0.9050\n",
      "Epoch 71/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1192 - acc: 0.9583 - val_loss: 0.3270 - val_acc: 0.9009\n",
      "Epoch 72/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1196 - acc: 0.9586 - val_loss: 0.3476 - val_acc: 0.8983\n",
      "Epoch 73/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1176 - acc: 0.9594 - val_loss: 0.3187 - val_acc: 0.9043\n",
      "Epoch 74/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1156 - acc: 0.9588 - val_loss: 0.3247 - val_acc: 0.9038\n",
      "Epoch 75/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1179 - acc: 0.9580 - val_loss: 0.3177 - val_acc: 0.9031\n",
      "Epoch 76/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1165 - acc: 0.9593 - val_loss: 0.3262 - val_acc: 0.9023\n",
      "Epoch 77/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1178 - acc: 0.9583 - val_loss: 0.3576 - val_acc: 0.8959\n",
      "Epoch 78/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1142 - acc: 0.9597 - val_loss: 0.3239 - val_acc: 0.9008\n",
      "Epoch 79/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1153 - acc: 0.9600 - val_loss: 0.3393 - val_acc: 0.8996\n",
      "Epoch 80/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1134 - acc: 0.9603 - val_loss: 0.3336 - val_acc: 0.9011\n",
      "Epoch 81/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1119 - acc: 0.9606 - val_loss: 0.3439 - val_acc: 0.9020\n",
      "Epoch 82/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1134 - acc: 0.9600 - val_loss: 0.3379 - val_acc: 0.9003\n",
      "Epoch 83/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1099 - acc: 0.9612 - val_loss: 0.3461 - val_acc: 0.9021\n",
      "Epoch 84/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1127 - acc: 0.9603 - val_loss: 0.3383 - val_acc: 0.9014\n",
      "Epoch 85/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1144 - acc: 0.9604 - val_loss: 0.3186 - val_acc: 0.9045\n",
      "Epoch 86/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.1050 - acc: 0.9624 - val_loss: 0.3185 - val_acc: 0.9042\n",
      "Epoch 87/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0962 - acc: 0.9664 - val_loss: 0.3157 - val_acc: 0.9069\n",
      "Epoch 88/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0980 - acc: 0.9667 - val_loss: 0.3150 - val_acc: 0.9055\n",
      "Epoch 89/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0974 - acc: 0.9670 - val_loss: 0.3148 - val_acc: 0.9060\n",
      "Epoch 90/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0953 - acc: 0.9674 - val_loss: 0.3117 - val_acc: 0.9068\n",
      "Epoch 91/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0976 - acc: 0.9670 - val_loss: 0.3149 - val_acc: 0.9055\n",
      "Epoch 92/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0967 - acc: 0.9667 - val_loss: 0.3149 - val_acc: 0.9079\n",
      "Epoch 93/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0976 - acc: 0.9671 - val_loss: 0.3132 - val_acc: 0.9064\n",
      "Epoch 94/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0949 - acc: 0.9668 - val_loss: 0.3133 - val_acc: 0.9065\n",
      "Epoch 95/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0951 - acc: 0.9676 - val_loss: 0.3134 - val_acc: 0.9072\n",
      "Epoch 96/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0937 - acc: 0.9675 - val_loss: 0.3190 - val_acc: 0.9064\n",
      "Epoch 97/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0941 - acc: 0.9672 - val_loss: 0.3150 - val_acc: 0.9068\n",
      "Epoch 98/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0952 - acc: 0.9669 - val_loss: 0.3143 - val_acc: 0.9065\n",
      "Epoch 99/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0931 - acc: 0.9678 - val_loss: 0.3127 - val_acc: 0.9088\n",
      "Epoch 100/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0936 - acc: 0.9675 - val_loss: 0.3157 - val_acc: 0.9062\n",
      "Epoch 101/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0925 - acc: 0.9676 - val_loss: 0.3148 - val_acc: 0.9072\n",
      "Epoch 102/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0922 - acc: 0.9684 - val_loss: 0.3128 - val_acc: 0.9081\n",
      "Epoch 103/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0941 - acc: 0.9682 - val_loss: 0.3125 - val_acc: 0.9074\n",
      "Epoch 104/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0946 - acc: 0.9668 - val_loss: 0.3142 - val_acc: 0.9086\n",
      "Epoch 105/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0969 - acc: 0.9659 - val_loss: 0.3178 - val_acc: 0.9059\n",
      "Epoch 106/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0902 - acc: 0.9691 - val_loss: 0.3108 - val_acc: 0.9081\n",
      "Epoch 107/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0900 - acc: 0.9694 - val_loss: 0.3095 - val_acc: 0.9084\n",
      "Epoch 108/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0922 - acc: 0.9684 - val_loss: 0.3111 - val_acc: 0.9073\n",
      "Epoch 109/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0879 - acc: 0.9705 - val_loss: 0.3107 - val_acc: 0.9087\n",
      "Epoch 110/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0911 - acc: 0.9691 - val_loss: 0.3110 - val_acc: 0.9089\n",
      "Epoch 111/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0880 - acc: 0.9698 - val_loss: 0.3110 - val_acc: 0.9095\n",
      "Epoch 112/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0905 - acc: 0.9692 - val_loss: 0.3111 - val_acc: 0.9083\n",
      "Epoch 113/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0890 - acc: 0.9697 - val_loss: 0.3128 - val_acc: 0.9085\n",
      "Epoch 114/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0894 - acc: 0.9694 - val_loss: 0.3108 - val_acc: 0.9086\n",
      "Epoch 115/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0886 - acc: 0.9698 - val_loss: 0.3121 - val_acc: 0.9091: 3s - loss: 0.0886 - a\n",
      "Epoch 116/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0915 - acc: 0.9684 - val_loss: 0.3133 - val_acc: 0.9080\n",
      "Epoch 117/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0911 - acc: 0.9684 - val_loss: 0.3116 - val_acc: 0.9093\n",
      "Epoch 118/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0899 - acc: 0.9688 - val_loss: 0.3120 - val_acc: 0.9088\n",
      "Epoch 119/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0903 - acc: 0.9690 - val_loss: 0.3125 - val_acc: 0.9081\n",
      "Epoch 120/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0910 - acc: 0.9687 - val_loss: 0.3125 - val_acc: 0.9074\n",
      "Epoch 121/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0895 - acc: 0.9685 - val_loss: 0.3129 - val_acc: 0.9078\n",
      "Epoch 122/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0909 - acc: 0.9689 - val_loss: 0.3125 - val_acc: 0.9078\n",
      "Epoch 123/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0880 - acc: 0.9695 - val_loss: 0.3122 - val_acc: 0.9084\n",
      "Epoch 124/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0882 - acc: 0.9705 - val_loss: 0.3131 - val_acc: 0.9081\n",
      "Epoch 125/125\n",
      "390/390 [==============================] - 27s 70ms/step - loss: 0.0895 - acc: 0.9691 - val_loss: 0.3146 - val_acc: 0.9068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.43770051332364529,\n",
       "  0.62181183826781861,\n",
       "  0.68553496952197623,\n",
       "  0.72684071222329161,\n",
       "  0.75703801736259524,\n",
       "  0.77895412253461516,\n",
       "  0.79441369906961823,\n",
       "  0.80467998075072189,\n",
       "  0.81653031765133444,\n",
       "  0.82527269806890946,\n",
       "  0.83215030476111496,\n",
       "  0.83842637151106836,\n",
       "  0.84636669878704185,\n",
       "  0.84939444982971102,\n",
       "  0.85669313444324824,\n",
       "  0.85835739493102337,\n",
       "  0.86389156236149844,\n",
       "  0.86880413863304762,\n",
       "  0.87033253205128203,\n",
       "  0.87453837515684607,\n",
       "  0.87742621106217222,\n",
       "  0.88121591271748323,\n",
       "  0.88470484442079067,\n",
       "  0.88785290339454304,\n",
       "  0.88864182692307692,\n",
       "  0.89031390496454865,\n",
       "  0.89220404231016703,\n",
       "  0.89607394929111173,\n",
       "  0.89619425733692948,\n",
       "  0.90034488286197967,\n",
       "  0.9038538658775731,\n",
       "  0.90321222327250417,\n",
       "  0.90748315684337211,\n",
       "  0.90619987163323412,\n",
       "  0.90884664739826604,\n",
       "  0.91249598969547341,\n",
       "  0.91147337183188959,\n",
       "  0.91462143086300929,\n",
       "  0.91480189280744006,\n",
       "  0.91428055819082155,\n",
       "  0.91592476742367812,\n",
       "  0.91744866861071694,\n",
       "  0.91989493106166487,\n",
       "  0.9204162656591609,\n",
       "  0.92105790820686262,\n",
       "  0.94175088222033709,\n",
       "  0.9454927884615385,\n",
       "  0.94785645472061653,\n",
       "  0.94660330449766095,\n",
       "  0.94816730832839413,\n",
       "  0.95001203083708996,\n",
       "  0.95026041666666672,\n",
       "  0.94968288373166199,\n",
       "  0.951054700070327,\n",
       "  0.95109480265665403,\n",
       "  0.95340070576862068,\n",
       "  0.95199711262739961,\n",
       "  0.95420275908232421,\n",
       "  0.95295957655412555,\n",
       "  0.9541866987179487,\n",
       "  0.95369701344919422,\n",
       "  0.95564645496285183,\n",
       "  0.95566650623689298,\n",
       "  0.95566650625601535,\n",
       "  0.95548604431158468,\n",
       "  0.9545837343408391,\n",
       "  0.95496794871794877,\n",
       "  0.95664739882478977,\n",
       "  0.95759143409034475,\n",
       "  0.9565287134683319,\n",
       "  0.9582731793008632,\n",
       "  0.95855389801707069,\n",
       "  0.9594361565799181,\n",
       "  0.95885466794995189,\n",
       "  0.95795272435897438,\n",
       "  0.95927665374488802,\n",
       "  0.95835338470298659,\n",
       "  0.95965672124453294,\n",
       "  0.96001764520988431,\n",
       "  0.96033846643592902,\n",
       "  0.96055903115791119,\n",
       "  0.96007779920410952,\n",
       "  0.96120067368649043,\n",
       "  0.96025826116766266,\n",
       "  0.96037856915611308,\n",
       "  0.96236365096554533,\n",
       "  0.96639396854038995,\n",
       "  0.96669473851151599,\n",
       "  0.96700721153846159,\n",
       "  0.96737648383676911,\n",
       "  0.96698378285844122,\n",
       "  0.96673484115521036,\n",
       "  0.96711581647109246,\n",
       "  0.96675489254398606,\n",
       "  0.96758814102564106,\n",
       "  0.96751684309926067,\n",
       "  0.96720455999395827,\n",
       "  0.96692708333333333,\n",
       "  0.96778660886319845,\n",
       "  0.9675569457620774,\n",
       "  0.9676772537696503,\n",
       "  0.96843920440141462,\n",
       "  0.96823869101084081,\n",
       "  0.96679499522592538,\n",
       "  0.96593278795624149,\n",
       "  0.96912094965017792,\n",
       "  0.96939102564102564,\n",
       "  0.96844894023146777,\n",
       "  0.97050449153648033,\n",
       "  0.96905048076923073,\n",
       "  0.9698026949180637,\n",
       "  0.96917148364149153,\n",
       "  0.9697024382227768,\n",
       "  0.96940166823252827,\n",
       "  0.96978264354841048,\n",
       "  0.96843920440141462,\n",
       "  0.96838942307692311,\n",
       "  0.96881021192690919,\n",
       "  0.96894048766750229,\n",
       "  0.96869987163323412,\n",
       "  0.96849935835739498,\n",
       "  0.96894048762925744,\n",
       "  0.96950192492781517,\n",
       "  0.97047275641025643,\n",
       "  0.96908084692999386],\n",
       " 'loss': [1.5197409113745373,\n",
       "  1.0613998833244185,\n",
       "  0.88873882937362492,\n",
       "  0.77117979126860781,\n",
       "  0.69191730886764702,\n",
       "  0.63387538130787424,\n",
       "  0.59140467473548075,\n",
       "  0.56005582199059967,\n",
       "  0.52906201246804363,\n",
       "  0.50362471716504165,\n",
       "  0.48157870848858858,\n",
       "  0.46580850969035542,\n",
       "  0.44215431078948153,\n",
       "  0.43007741778142411,\n",
       "  0.41652619624543274,\n",
       "  0.40196457712704609,\n",
       "  0.389732922599757,\n",
       "  0.37716613093705986,\n",
       "  0.37120059017951673,\n",
       "  0.35919464511142452,\n",
       "  0.35309707878452101,\n",
       "  0.33982320620840012,\n",
       "  0.33035100756507674,\n",
       "  0.32158564249597987,\n",
       "  0.31945712929352738,\n",
       "  0.31241434986727623,\n",
       "  0.30446002211446827,\n",
       "  0.29963963691301748,\n",
       "  0.29460226213048579,\n",
       "  0.2824974431320762,\n",
       "  0.27821201003817819,\n",
       "  0.2773074006322947,\n",
       "  0.26308902806489176,\n",
       "  0.26558743158632031,\n",
       "  0.2598516621438976,\n",
       "  0.25174366931598613,\n",
       "  0.25415480189458045,\n",
       "  0.24357698452866608,\n",
       "  0.24083969205436243,\n",
       "  0.24049356768136609,\n",
       "  0.23616674659073947,\n",
       "  0.23252623305704717,\n",
       "  0.22537750681222232,\n",
       "  0.22403635225178992,\n",
       "  0.22186012349528,\n",
       "  0.16819412799305283,\n",
       "  0.1591131821847879,\n",
       "  0.15212309527029652,\n",
       "  0.15312337851463162,\n",
       "  0.151083866726484,\n",
       "  0.14564718339528876,\n",
       "  0.14296616034056897,\n",
       "  0.14318782257666249,\n",
       "  0.14144659224260606,\n",
       "  0.14148635160743253,\n",
       "  0.13629106392998477,\n",
       "  0.1363745560946662,\n",
       "  0.13250511432921691,\n",
       "  0.13468797853408351,\n",
       "  0.13247997219172808,\n",
       "  0.13058414647096078,\n",
       "  0.12872768920136063,\n",
       "  0.12741304056782887,\n",
       "  0.12582354988086519,\n",
       "  0.12854285361553025,\n",
       "  0.1284518390798324,\n",
       "  0.1276898677341449,\n",
       "  0.12283491174820224,\n",
       "  0.12225134694278909,\n",
       "  0.12200976553963742,\n",
       "  0.11921548323014397,\n",
       "  0.11964778227084411,\n",
       "  0.11761169293330655,\n",
       "  0.11553299115336703,\n",
       "  0.11789448478091986,\n",
       "  0.11654169647670282,\n",
       "  0.117762185425226,\n",
       "  0.1142281636399721,\n",
       "  0.11524820596907129,\n",
       "  0.11335496473692915,\n",
       "  0.11194508691743509,\n",
       "  0.11334547099296122,\n",
       "  0.109902780145684,\n",
       "  0.11277411776042956,\n",
       "  0.11447792425440946,\n",
       "  0.10508798739257669,\n",
       "  0.096182320590848372,\n",
       "  0.09801625723797884,\n",
       "  0.097412889918837786,\n",
       "  0.095253579483454448,\n",
       "  0.097487512035636312,\n",
       "  0.096716019167496989,\n",
       "  0.097619523413486495,\n",
       "  0.094962623626481349,\n",
       "  0.095141659705684736,\n",
       "  0.093697804002701418,\n",
       "  0.093957878297521433,\n",
       "  0.095184833298508936,\n",
       "  0.093155133284929859,\n",
       "  0.09353264529505409,\n",
       "  0.092515683158357287,\n",
       "  0.092153588567439437,\n",
       "  0.094096674845889475,\n",
       "  0.09464153934020357,\n",
       "  0.096775423841748739,\n",
       "  0.090294329354519795,\n",
       "  0.089980563253928447,\n",
       "  0.092119967628344554,\n",
       "  0.087956544488697122,\n",
       "  0.091139834866118735,\n",
       "  0.088058866562849447,\n",
       "  0.090491248865348067,\n",
       "  0.088944636827904253,\n",
       "  0.089385474979743612,\n",
       "  0.088496092418886965,\n",
       "  0.091434665934986739,\n",
       "  0.091058222710704198,\n",
       "  0.08987528808993947,\n",
       "  0.09031734231083792,\n",
       "  0.090957558153625634,\n",
       "  0.089548226052708152,\n",
       "  0.090797118701573781,\n",
       "  0.087992673917271058,\n",
       "  0.088221354706165114,\n",
       "  0.08947975385454858],\n",
       " 'val_acc': [0.41880000000000001,\n",
       "  0.50519999999999998,\n",
       "  0.69110000000000005,\n",
       "  0.70509999999999995,\n",
       "  0.70289999999999997,\n",
       "  0.72009999999999996,\n",
       "  0.77180000000000004,\n",
       "  0.77429999999999999,\n",
       "  0.76990000000000003,\n",
       "  0.75760000000000005,\n",
       "  0.76959999999999995,\n",
       "  0.82240000000000002,\n",
       "  0.82230000000000003,\n",
       "  0.83079999999999998,\n",
       "  0.8155,\n",
       "  0.82650000000000001,\n",
       "  0.81140000000000001,\n",
       "  0.85499999999999998,\n",
       "  0.83579999999999999,\n",
       "  0.84419999999999995,\n",
       "  0.83960000000000001,\n",
       "  0.8377,\n",
       "  0.8337,\n",
       "  0.84899999999999998,\n",
       "  0.82950000000000002,\n",
       "  0.83189999999999997,\n",
       "  0.82750000000000001,\n",
       "  0.84950000000000003,\n",
       "  0.84709999999999996,\n",
       "  0.86219999999999997,\n",
       "  0.87780000000000002,\n",
       "  0.86050000000000004,\n",
       "  0.86099999999999999,\n",
       "  0.86629999999999996,\n",
       "  0.85570000000000002,\n",
       "  0.86480000000000001,\n",
       "  0.84630000000000005,\n",
       "  0.83050000000000002,\n",
       "  0.85850000000000004,\n",
       "  0.8508,\n",
       "  0.85450000000000004,\n",
       "  0.87880000000000003,\n",
       "  0.86760000000000004,\n",
       "  0.85450000000000004,\n",
       "  0.8821,\n",
       "  0.90410000000000001,\n",
       "  0.90169999999999995,\n",
       "  0.89949999999999997,\n",
       "  0.90169999999999995,\n",
       "  0.89759999999999995,\n",
       "  0.90210000000000001,\n",
       "  0.89970000000000006,\n",
       "  0.90359999999999996,\n",
       "  0.90090000000000003,\n",
       "  0.90429999999999999,\n",
       "  0.90439999999999998,\n",
       "  0.90500000000000003,\n",
       "  0.90549999999999997,\n",
       "  0.90339999999999998,\n",
       "  0.90310000000000001,\n",
       "  0.90669999999999995,\n",
       "  0.89980000000000004,\n",
       "  0.90180000000000005,\n",
       "  0.90429999999999999,\n",
       "  0.90200000000000002,\n",
       "  0.90110000000000001,\n",
       "  0.90310000000000001,\n",
       "  0.90300000000000002,\n",
       "  0.90559999999999996,\n",
       "  0.90500000000000003,\n",
       "  0.90090000000000003,\n",
       "  0.89829999999999999,\n",
       "  0.90429999999999999,\n",
       "  0.90380000000000005,\n",
       "  0.90310000000000001,\n",
       "  0.90229999999999999,\n",
       "  0.89590000000000003,\n",
       "  0.90080000000000005,\n",
       "  0.89959999999999996,\n",
       "  0.90110000000000001,\n",
       "  0.90200000000000002,\n",
       "  0.90029999999999999,\n",
       "  0.90210000000000001,\n",
       "  0.90139999999999998,\n",
       "  0.90449999999999997,\n",
       "  0.9042,\n",
       "  0.90690000000000004,\n",
       "  0.90549999999999997,\n",
       "  0.90600000000000003,\n",
       "  0.90680000000000005,\n",
       "  0.90549999999999997,\n",
       "  0.90790000000000004,\n",
       "  0.90639999999999998,\n",
       "  0.90649999999999997,\n",
       "  0.90720000000000001,\n",
       "  0.90639999999999998,\n",
       "  0.90680000000000005,\n",
       "  0.90649999999999997,\n",
       "  0.90880000000000005,\n",
       "  0.90620000000000001,\n",
       "  0.90720000000000001,\n",
       "  0.90810000000000002,\n",
       "  0.90739999999999998,\n",
       "  0.90859999999999996,\n",
       "  0.90590000000000004,\n",
       "  0.90810000000000002,\n",
       "  0.90839999999999999,\n",
       "  0.9073,\n",
       "  0.90869999999999995,\n",
       "  0.90890000000000004,\n",
       "  0.90949999999999998,\n",
       "  0.9083,\n",
       "  0.90849999999999997,\n",
       "  0.90859999999999996,\n",
       "  0.90910000000000002,\n",
       "  0.90800000000000003,\n",
       "  0.9093,\n",
       "  0.90880000000000005,\n",
       "  0.90810000000000002,\n",
       "  0.90739999999999998,\n",
       "  0.90780000000000005,\n",
       "  0.90780000000000005,\n",
       "  0.90839999999999999,\n",
       "  0.90810000000000002,\n",
       "  0.90680000000000005],\n",
       " 'val_loss': [1.7875607238769531,\n",
       "  1.5348016590118407,\n",
       "  0.86361009130477906,\n",
       "  0.9352071256637573,\n",
       "  0.90875644245147702,\n",
       "  0.83966041450500484,\n",
       "  0.67099821376800539,\n",
       "  0.67786681823730466,\n",
       "  0.69155780582427984,\n",
       "  0.8180355596542358,\n",
       "  0.72021733207702632,\n",
       "  0.52520639686584469,\n",
       "  0.539760949754715,\n",
       "  0.52080535058975219,\n",
       "  0.57641452021598816,\n",
       "  0.52251162815093999,\n",
       "  0.60437858810424805,\n",
       "  0.43852492616176603,\n",
       "  0.51217488943338396,\n",
       "  0.46853918352127077,\n",
       "  0.48352188265323637,\n",
       "  0.51172312061786651,\n",
       "  0.51454243392944332,\n",
       "  0.4673844656467438,\n",
       "  0.5263325037956238,\n",
       "  0.54939556434154513,\n",
       "  0.52544570541381841,\n",
       "  0.47566120660305022,\n",
       "  0.47269547638893128,\n",
       "  0.42027156696319579,\n",
       "  0.37305774097442629,\n",
       "  0.42899540238380435,\n",
       "  0.42530649738311765,\n",
       "  0.42911994595527647,\n",
       "  0.45230552721023559,\n",
       "  0.43049073958396911,\n",
       "  0.48138497371673583,\n",
       "  0.59262640304565428,\n",
       "  0.44040409526824953,\n",
       "  0.50827225875854487,\n",
       "  0.47630139479637146,\n",
       "  0.39655281574726103,\n",
       "  0.42588812294006345,\n",
       "  0.48706969103813169,\n",
       "  0.37603504338860511,\n",
       "  0.30376424612402914,\n",
       "  0.31024667701721192,\n",
       "  0.309562015581131,\n",
       "  0.31379749791622163,\n",
       "  0.31655456297397616,\n",
       "  0.30500125408172607,\n",
       "  0.31401428554058075,\n",
       "  0.31005199437141417,\n",
       "  0.31484467260837556,\n",
       "  0.3098857442140579,\n",
       "  0.29847388854026796,\n",
       "  0.31103178673386572,\n",
       "  0.30699392960071564,\n",
       "  0.31270448412299157,\n",
       "  0.30073051984310151,\n",
       "  0.30428982211351396,\n",
       "  0.32468120126724243,\n",
       "  0.33039527035951616,\n",
       "  0.31378156551718711,\n",
       "  0.31950998237133027,\n",
       "  0.31689284546375274,\n",
       "  0.3178385786652565,\n",
       "  0.31114185079336165,\n",
       "  0.31663501622676848,\n",
       "  0.31547080844640729,\n",
       "  0.32701862782239915,\n",
       "  0.3476112013936043,\n",
       "  0.31866539267301558,\n",
       "  0.32465626392364499,\n",
       "  0.317691816586256,\n",
       "  0.32624540778398514,\n",
       "  0.35761140108108519,\n",
       "  0.32394806808233262,\n",
       "  0.33933723812103272,\n",
       "  0.33356347647011281,\n",
       "  0.34392605421543121,\n",
       "  0.33789743151664736,\n",
       "  0.34614874868988993,\n",
       "  0.33831380281448364,\n",
       "  0.31862779193520546,\n",
       "  0.31847149174213407,\n",
       "  0.31568424248695376,\n",
       "  0.3150411995470524,\n",
       "  0.31484188393354418,\n",
       "  0.31167865800857542,\n",
       "  0.31494670382738116,\n",
       "  0.31489534178972245,\n",
       "  0.31321741982698442,\n",
       "  0.31330751797556877,\n",
       "  0.31341099464893341,\n",
       "  0.31901066800355909,\n",
       "  0.31498031468391419,\n",
       "  0.31434283699393273,\n",
       "  0.31273079044222829,\n",
       "  0.31566300812363624,\n",
       "  0.31483525218963621,\n",
       "  0.31277420688867569,\n",
       "  0.31252778185606001,\n",
       "  0.31418879240751268,\n",
       "  0.31777140440940854,\n",
       "  0.31081881973743436,\n",
       "  0.30953380093574523,\n",
       "  0.31109952614307401,\n",
       "  0.31074253852367401,\n",
       "  0.31099783973693845,\n",
       "  0.31095046598911286,\n",
       "  0.31111435841321944,\n",
       "  0.31282141078710557,\n",
       "  0.3107584072589874,\n",
       "  0.31205932754278182,\n",
       "  0.31333375996351243,\n",
       "  0.31160361856818197,\n",
       "  0.31204778220057489,\n",
       "  0.31249608072042467,\n",
       "  0.31253126636743545,\n",
       "  0.31291036455631255,\n",
       "  0.31245756261348723,\n",
       "  0.31223620495796206,\n",
       "  0.31311553472876547,\n",
       "  0.31459430990219117]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(act):# SWISH - 125\n",
    "    init_shape = (3, 32, 32) if K.image_dim_ordering() == 'th' else (32, 32, 3)\n",
    "    # For WRN-16-8 put N = 2, k = 8\n",
    "    # For WRN-28-10 put N = 4, k = 10\n",
    "    # For WRN-40-4 put N = 6, k = 4\n",
    "    model = wrn.build_model(16, 4, init_shape, num_classes)\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Model Created\")\n",
    "    batch_size  = 128\n",
    "    epochs = 150\n",
    "\n",
    "    opt = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    print(\"Finished compiling\")\n",
    "\n",
    "    ####################\n",
    "    # Network training #\n",
    "    ####################\n",
    "\n",
    "    print(\"Gonna fit the model\")\n",
    "#     his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "#     print(his.history)\n",
    "    return his.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 1.5876 - acc: 0.4164 - val_loss: 1.7169 - val_acc: 0.4275\n",
      "Epoch 2/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 1.1048 - acc: 0.6065 - val_loss: 1.3743 - val_acc: 0.5580\n",
      "Epoch 3/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.8679 - acc: 0.6948 - val_loss: 0.9323 - val_acc: 0.6794\n",
      "Epoch 4/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.7192 - acc: 0.7482 - val_loss: 0.8433 - val_acc: 0.7124\n",
      "Epoch 5/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.6179 - acc: 0.7842 - val_loss: 0.6428 - val_acc: 0.7787\n",
      "Epoch 6/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.5489 - acc: 0.8103 - val_loss: 0.5743 - val_acc: 0.8018\n",
      "Epoch 7/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.4939 - acc: 0.8293 - val_loss: 0.5800 - val_acc: 0.8008\n",
      "Epoch 8/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.4509 - acc: 0.8422 - val_loss: 0.4698 - val_acc: 0.8412\n",
      "Epoch 9/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.4110 - acc: 0.8568 - val_loss: 0.6941 - val_acc: 0.7676\n",
      "Epoch 10/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3863 - acc: 0.8646 - val_loss: 0.5822 - val_acc: 0.8031\n",
      "Epoch 11/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.3548 - acc: 0.8766 - val_loss: 0.5646 - val_acc: 0.8127\n",
      "Epoch 12/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3318 - acc: 0.8844 - val_loss: 0.4546 - val_acc: 0.8493\n",
      "Epoch 13/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3127 - acc: 0.8907 - val_loss: 0.4260 - val_acc: 0.8574\n",
      "Epoch 14/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.2961 - acc: 0.8957 - val_loss: 0.4369 - val_acc: 0.8592\n",
      "Epoch 15/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.2766 - acc: 0.9029 - val_loss: 0.5007 - val_acc: 0.8400\n",
      "Epoch 16/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.2572 - acc: 0.9095 - val_loss: 0.4962 - val_acc: 0.8462\n",
      "Epoch 17/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.2530 - acc: 0.9117 - val_loss: 0.4432 - val_acc: 0.8716\n",
      "Epoch 18/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.2292 - acc: 0.9200 - val_loss: 0.3442 - val_acc: 0.8852\n",
      "Epoch 19/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.2207 - acc: 0.9231 - val_loss: 0.4921 - val_acc: 0.8489\n",
      "Epoch 20/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.2069 - acc: 0.9271 - val_loss: 0.5167 - val_acc: 0.8519\n",
      "Epoch 21/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1961 - acc: 0.9315 - val_loss: 0.4259 - val_acc: 0.8676\n",
      "Epoch 22/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1869 - acc: 0.9340 - val_loss: 0.8235 - val_acc: 0.8027\n",
      "Epoch 23/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1819 - acc: 0.9351 - val_loss: 0.4198 - val_acc: 0.8722\n",
      "Epoch 24/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1681 - acc: 0.9410 - val_loss: 0.3681 - val_acc: 0.8914\n",
      "Epoch 25/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1671 - acc: 0.9412 - val_loss: 0.6312 - val_acc: 0.8404\n",
      "Epoch 26/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1554 - acc: 0.9451 - val_loss: 0.3691 - val_acc: 0.8920\n",
      "Epoch 27/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1505 - acc: 0.9461 - val_loss: 0.3927 - val_acc: 0.8836\n",
      "Epoch 28/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1435 - acc: 0.9486 - val_loss: 0.3957 - val_acc: 0.8919\n",
      "Epoch 29/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1397 - acc: 0.9513 - val_loss: 0.3623 - val_acc: 0.8954\n",
      "Epoch 30/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.1300 - acc: 0.9540 - val_loss: 0.6251 - val_acc: 0.8411\n",
      "Epoch 31/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1249 - acc: 0.9566 - val_loss: 0.4725 - val_acc: 0.8731\n",
      "Epoch 32/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1208 - acc: 0.9574 - val_loss: 0.3419 - val_acc: 0.9021\n",
      "Epoch 33/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1153 - acc: 0.9588 - val_loss: 0.3585 - val_acc: 0.8987\n",
      "Epoch 34/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1101 - acc: 0.9606 - val_loss: 0.4963 - val_acc: 0.8717\n",
      "Epoch 35/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1017 - acc: 0.9624 - val_loss: 0.4172 - val_acc: 0.8889\n",
      "Epoch 36/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.1008 - acc: 0.9647 - val_loss: 0.3505 - val_acc: 0.9044\n",
      "Epoch 37/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0992 - acc: 0.9641 - val_loss: 0.4096 - val_acc: 0.8963\n",
      "Epoch 38/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0960 - acc: 0.9656 - val_loss: 0.3979 - val_acc: 0.8990\n",
      "Epoch 39/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0915 - acc: 0.9670 - val_loss: 0.4566 - val_acc: 0.8908\n",
      "Epoch 40/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0855 - acc: 0.9700 - val_loss: 0.4150 - val_acc: 0.8933\n",
      "Epoch 41/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0800 - acc: 0.9718 - val_loss: 0.3647 - val_acc: 0.9022\n",
      "Epoch 42/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0847 - acc: 0.9703 - val_loss: 0.4342 - val_acc: 0.8878\n",
      "Epoch 43/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0805 - acc: 0.9711 - val_loss: 0.3794 - val_acc: 0.9040\n",
      "Epoch 44/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0731 - acc: 0.9740 - val_loss: 0.4468 - val_acc: 0.8955\n",
      "Epoch 45/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0696 - acc: 0.9748 - val_loss: 0.3454 - val_acc: 0.9091\n",
      "Epoch 46/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0429 - acc: 0.9854 - val_loss: 0.2817 - val_acc: 0.9253\n",
      "Epoch 47/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0303 - acc: 0.9904 - val_loss: 0.2736 - val_acc: 0.9262\n",
      "Epoch 48/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0261 - acc: 0.9916 - val_loss: 0.2765 - val_acc: 0.9271\n",
      "Epoch 49/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.2826 - val_acc: 0.9286\n",
      "Epoch 50/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0213 - acc: 0.9933 - val_loss: 0.2898 - val_acc: 0.9254\n",
      "Epoch 51/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.2856 - val_acc: 0.9293\n",
      "Epoch 52/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.2851 - val_acc: 0.9296\n",
      "Epoch 53/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0181 - acc: 0.9946 - val_loss: 0.2875 - val_acc: 0.9292\n",
      "Epoch 54/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.2849 - val_acc: 0.9317\n",
      "Epoch 55/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0178 - acc: 0.9942 - val_loss: 0.2879 - val_acc: 0.9311\n",
      "Epoch 56/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0149 - acc: 0.9958 - val_loss: 0.3019 - val_acc: 0.9293\n",
      "Epoch 57/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0163 - acc: 0.9949 - val_loss: 0.2910 - val_acc: 0.9279\n",
      "Epoch 58/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0155 - acc: 0.9949 - val_loss: 0.2951 - val_acc: 0.9310\n",
      "Epoch 59/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0153 - acc: 0.9954 - val_loss: 0.3031 - val_acc: 0.9301\n",
      "Epoch 60/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.2985 - val_acc: 0.9300\n",
      "Epoch 61/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0136 - acc: 0.9959 - val_loss: 0.2961 - val_acc: 0.9321\n",
      "Epoch 62/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.3121 - val_acc: 0.9289\n",
      "Epoch 63/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 0.2988 - val_acc: 0.9310\n",
      "Epoch 64/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.3201 - val_acc: 0.9298\n",
      "Epoch 65/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.3149 - val_acc: 0.9300\n",
      "Epoch 66/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.3051 - val_acc: 0.9320\n",
      "Epoch 67/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.3324 - val_acc: 0.9277\n",
      "Epoch 68/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0124 - acc: 0.9964 - val_loss: 0.3177 - val_acc: 0.9302\n",
      "Epoch 69/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0123 - acc: 0.9961 - val_loss: 0.3169 - val_acc: 0.9298\n",
      "Epoch 70/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.3177 - val_acc: 0.9303\n",
      "Epoch 71/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.3171 - val_acc: 0.9305\n",
      "Epoch 72/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.3151 - val_acc: 0.9307\n",
      "Epoch 73/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0100 - acc: 0.9973 - val_loss: 0.3200 - val_acc: 0.9299\n",
      "Epoch 74/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.3137 - val_acc: 0.9302\n",
      "Epoch 75/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.3269 - val_acc: 0.9282\n",
      "Epoch 76/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.3384 - val_acc: 0.9276\n",
      "Epoch 77/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.3319 - val_acc: 0.9316\n",
      "Epoch 78/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0104 - acc: 0.9967 - val_loss: 0.3329 - val_acc: 0.9296\n",
      "Epoch 79/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0088 - acc: 0.9974 - val_loss: 0.3283 - val_acc: 0.9288\n",
      "Epoch 80/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0094 - acc: 0.9973 - val_loss: 0.3462 - val_acc: 0.9274\n",
      "Epoch 81/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.3310 - val_acc: 0.9305\n",
      "Epoch 82/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0096 - acc: 0.9969 - val_loss: 0.3247 - val_acc: 0.9325\n",
      "Epoch 83/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.3308 - val_acc: 0.9305\n",
      "Epoch 84/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.3369 - val_acc: 0.9286\n",
      "Epoch 85/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.3371 - val_acc: 0.9309\n",
      "Epoch 86/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.3319 - val_acc: 0.9314\n",
      "Epoch 87/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.3290 - val_acc: 0.9316\n",
      "Epoch 88/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.3280 - val_acc: 0.9300\n",
      "Epoch 89/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0078 - acc: 0.9977 - val_loss: 0.3255 - val_acc: 0.9303\n",
      "Epoch 90/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.3244 - val_acc: 0.9320\n",
      "Epoch 91/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.3235 - val_acc: 0.9314\n",
      "Epoch 92/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.3259 - val_acc: 0.9318\n",
      "Epoch 93/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.3255 - val_acc: 0.93260.00 - ETA: 3s - loss: 0.\n",
      "Epoch 94/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.3260 - val_acc: 0.9322\n",
      "Epoch 95/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.3263 - val_acc: 0.9313\n",
      "Epoch 96/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.3251 - val_acc: 0.9322\n",
      "Epoch 97/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.3257 - val_acc: 0.9316\n",
      "Epoch 98/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.3255 - val_acc: 0.9322\n",
      "Epoch 99/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.3244 - val_acc: 0.9315\n",
      "Epoch 100/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.3237 - val_acc: 0.9324\n",
      "Epoch 101/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3245 - val_acc: 0.9325\n",
      "Epoch 102/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.3266 - val_acc: 0.9315\n",
      "Epoch 103/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 0.3252 - val_acc: 0.9318\n",
      "Epoch 104/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.3262 - val_acc: 0.9320\n",
      "Epoch 105/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.3274 - val_acc: 0.9326\n",
      "Epoch 106/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.3279 - val_acc: 0.9322\n",
      "Epoch 107/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.3273 - val_acc: 0.9327\n",
      "Epoch 108/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.3271 - val_acc: 0.9322\n",
      "Epoch 109/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.3279 - val_acc: 0.9321\n",
      "Epoch 110/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.3270 - val_acc: 0.9321\n",
      "Epoch 111/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.3270 - val_acc: 0.9318\n",
      "Epoch 112/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.3275 - val_acc: 0.9326\n",
      "Epoch 113/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.3276 - val_acc: 0.9322\n",
      "Epoch 114/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.3271 - val_acc: 0.9321\n",
      "Epoch 115/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3261 - val_acc: 0.9324\n",
      "Epoch 116/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.3261 - val_acc: 0.9322\n",
      "Epoch 117/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.3273 - val_acc: 0.9323\n",
      "Epoch 118/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.3274 - val_acc: 0.9328\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.3272 - val_acc: 0.9318\n",
      "Epoch 120/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3263 - val_acc: 0.9320\n",
      "Epoch 121/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.3275 - val_acc: 0.9319\n",
      "Epoch 122/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.3276 - val_acc: 0.9322\n",
      "Epoch 123/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.3266 - val_acc: 0.9323\n",
      "Epoch 124/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3269 - val_acc: 0.9320\n",
      "Epoch 125/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.3278 - val_acc: 0.9328\n",
      "Epoch 126/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.3278 - val_acc: 0.9325\n",
      "Epoch 127/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.3278 - val_acc: 0.9322\n",
      "Epoch 128/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.3277 - val_acc: 0.9325\n",
      "Epoch 129/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.3268 - val_acc: 0.9326\n",
      "Epoch 130/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.3278 - val_acc: 0.9323\n",
      "Epoch 131/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.3275 - val_acc: 0.9329\n",
      "Epoch 132/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.3265 - val_acc: 0.9321\n",
      "Epoch 133/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.3265 - val_acc: 0.9324\n",
      "Epoch 134/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.3269 - val_acc: 0.9318\n",
      "Epoch 135/150\n",
      "390/390 [==============================] - 84s 214ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.3272 - val_acc: 0.9319\n",
      "Epoch 136/150\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.3264 - val_acc: 0.9320\n",
      "Epoch 137/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.3270 - val_acc: 0.9324\n",
      "Epoch 138/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.3263 - val_acc: 0.9318\n",
      "Epoch 139/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0061 - acc: 0.9984 - val_loss: 0.3262 - val_acc: 0.9321\n",
      "Epoch 140/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.3254 - val_acc: 0.9322\n",
      "Epoch 141/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.3274 - val_acc: 0.9321\n",
      "Epoch 142/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.3279 - val_acc: 0.9321\n",
      "Epoch 143/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.3269 - val_acc: 0.9323\n",
      "Epoch 144/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.3264 - val_acc: 0.9322\n",
      "Epoch 145/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.3262 - val_acc: 0.9318\n",
      "Epoch 146/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.3265 - val_acc: 0.9321\n",
      "Epoch 147/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.3263 - val_acc: 0.9324\n",
      "Epoch 148/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.3260 - val_acc: 0.9320\n",
      "Epoch 149/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.3260 - val_acc: 0.9322\n",
      "Epoch 150/150\n",
      "390/390 [==============================] - 84s 215ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.3269 - val_acc: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': [0.41630574268219295,\n",
       "  0.60651267244145013,\n",
       "  0.69475858200808771,\n",
       "  0.74823548285518271,\n",
       "  0.78426772535784262,\n",
       "  0.81027430217542207,\n",
       "  0.82930301572024379,\n",
       "  0.84213586784074579,\n",
       "  0.8568334937822295,\n",
       "  0.86469361561783464,\n",
       "  0.87662415782495839,\n",
       "  0.88438402311825626,\n",
       "  0.89066008979171984,\n",
       "  0.89575312800769968,\n",
       "  0.90295155594507237,\n",
       "  0.9095484439975603,\n",
       "  0.91167388512685121,\n",
       "  0.92001523899274795,\n",
       "  0.92314324673070414,\n",
       "  0.92705325633622071,\n",
       "  0.93144449791466155,\n",
       "  0.93397096567212068,\n",
       "  0.93511389154327729,\n",
       "  0.94102903428963447,\n",
       "  0.94126965028565779,\n",
       "  0.94509945456554079,\n",
       "  0.94610202115508357,\n",
       "  0.94858838626884823,\n",
       "  0.95128205128205123,\n",
       "  0.95399807323686725,\n",
       "  0.95662897012537396,\n",
       "  0.957451074732241,\n",
       "  0.95885466798819674,\n",
       "  0.96067933906987191,\n",
       "  0.96242380497889291,\n",
       "  0.96466955407751187,\n",
       "  0.96410811675983166,\n",
       "  0.9655718639335229,\n",
       "  0.96697545720860101,\n",
       "  0.97006336222637302,\n",
       "  0.97178777668925098,\n",
       "  0.97028392683362064,\n",
       "  0.97108598007083435,\n",
       "  0.97397337179364474,\n",
       "  0.97479547643875675,\n",
       "  0.98542268210433404,\n",
       "  0.9903954122171289,\n",
       "  0.99163859480269489,\n",
       "  0.99195941606698446,\n",
       "  0.99330285532871498,\n",
       "  0.99300881410256414,\n",
       "  0.9944203597425878,\n",
       "  0.99458614051973049,\n",
       "  0.99473157051282046,\n",
       "  0.99419958253050733,\n",
       "  0.99584937441758248,\n",
       "  0.99488691049085665,\n",
       "  0.99490696184138738,\n",
       "  0.99538819379518917,\n",
       "  0.9950473211421238,\n",
       "  0.99589342948717952,\n",
       "  0.9959657996146436,\n",
       "  0.99542829640063868,\n",
       "  0.99588947708039932,\n",
       "  0.99532803980096396,\n",
       "  0.99596968238691053,\n",
       "  0.99599358974358976,\n",
       "  0.99640735390481849,\n",
       "  0.99606993906307495,\n",
       "  0.99637419871794874,\n",
       "  0.99626685934489401,\n",
       "  0.99703240297067841,\n",
       "  0.9972529675970484,\n",
       "  0.99673163298042988,\n",
       "  0.99625040102662821,\n",
       "  0.99701522435897438,\n",
       "  0.99650770714823522,\n",
       "  0.99667467948717947,\n",
       "  0.9973908156903033,\n",
       "  0.99735322429233531,\n",
       "  0.99701235162014756,\n",
       "  0.99691506410256414,\n",
       "  0.99717003855478636,\n",
       "  0.99667467948717947,\n",
       "  0.9967717356623691,\n",
       "  0.99704961466268616,\n",
       "  0.99797481552775102,\n",
       "  0.99731312159127361,\n",
       "  0.99773419956997267,\n",
       "  0.99793471286493418,\n",
       "  0.99801491819056787,\n",
       "  0.99749358357394935,\n",
       "  0.99783445620789224,\n",
       "  0.99847609881296118,\n",
       "  0.9978766025641026,\n",
       "  0.99737327558549893,\n",
       "  0.99769187540141302,\n",
       "  0.99813522617901829,\n",
       "  0.99817708333333333,\n",
       "  0.99823378293500475,\n",
       "  0.99807507218479308,\n",
       "  0.99781440487648376,\n",
       "  0.99795476419634266,\n",
       "  0.99789461020211745,\n",
       "  0.99789461020211745,\n",
       "  0.99853766025641022,\n",
       "  0.99825385358368812,\n",
       "  0.99853625280718639,\n",
       "  0.99795673076923075,\n",
       "  0.99825385358368812,\n",
       "  0.9983974358974359,\n",
       "  0.99785244062916167,\n",
       "  0.99823717948717949,\n",
       "  0.99827392421323058,\n",
       "  0.99805502085338471,\n",
       "  0.9981953801732435,\n",
       "  0.99825553418659119,\n",
       "  0.99811698717948716,\n",
       "  0.99839434810533079,\n",
       "  0.99811698717948716,\n",
       "  0.99823378291586384,\n",
       "  0.99825553416746871,\n",
       "  0.99841594481873597,\n",
       "  0.99807692307692308,\n",
       "  0.99783236994219648,\n",
       "  0.99823548285518271,\n",
       "  0.99825553416746871,\n",
       "  0.99837740384615381,\n",
       "  0.99843448940269752,\n",
       "  0.99821543150465186,\n",
       "  0.99773637820512817,\n",
       "  0.99785244059087985,\n",
       "  0.99829563683028555,\n",
       "  0.99809512351620144,\n",
       "  0.99819711538461542,\n",
       "  0.99807321774553781,\n",
       "  0.99829727564102566,\n",
       "  0.99829399486191395,\n",
       "  0.9983958934873276,\n",
       "  0.99835579082451076,\n",
       "  0.99823717948717949,\n",
       "  0.99813522617901829,\n",
       "  0.9985749839626219,\n",
       "  0.99801491819056787,\n",
       "  0.99785657051282051,\n",
       "  0.99855491331393853,\n",
       "  0.99805502085338471,\n",
       "  0.99821543150465186,\n",
       "  0.99829563683028555,\n",
       "  0.99813701923076925],\n",
       " 'loss': [1.5878574134525596,\n",
       "  1.1046203077699956,\n",
       "  0.86788108506032891,\n",
       "  0.71920611531756651,\n",
       "  0.61782838516522953,\n",
       "  0.54899408543266692,\n",
       "  0.49400700686602855,\n",
       "  0.45091805711058724,\n",
       "  0.41091036435155714,\n",
       "  0.38607821668766229,\n",
       "  0.35476039276889476,\n",
       "  0.33195241231603961,\n",
       "  0.31274721411737755,\n",
       "  0.29605898206571296,\n",
       "  0.27656819253806164,\n",
       "  0.25710936293680009,\n",
       "  0.2530365376794495,\n",
       "  0.22915608509251578,\n",
       "  0.22073968279273762,\n",
       "  0.20689403652226959,\n",
       "  0.1961225794032479,\n",
       "  0.18696630893607991,\n",
       "  0.18193791802395295,\n",
       "  0.16803900442388833,\n",
       "  0.16699238880192349,\n",
       "  0.155404543320663,\n",
       "  0.15047864957730817,\n",
       "  0.14342267874880482,\n",
       "  0.13974685844702597,\n",
       "  0.12985646414618962,\n",
       "  0.12496762043884024,\n",
       "  0.12083574633759721,\n",
       "  0.11522512308557636,\n",
       "  0.10998032122305429,\n",
       "  0.10177299863024518,\n",
       "  0.10089158759663645,\n",
       "  0.099180079312958622,\n",
       "  0.096078292510621929,\n",
       "  0.091541963908193885,\n",
       "  0.085450268980685737,\n",
       "  0.079949735612738465,\n",
       "  0.084666980165757549,\n",
       "  0.080463631057685711,\n",
       "  0.073060859301876557,\n",
       "  0.069585689031278472,\n",
       "  0.042917693421641409,\n",
       "  0.030308313931902899,\n",
       "  0.026146314063258552,\n",
       "  0.025207675877512745,\n",
       "  0.021333893580570754,\n",
       "  0.022119126898141054,\n",
       "  0.018430111513460534,\n",
       "  0.018061119834194783,\n",
       "  0.017800645123964225,\n",
       "  0.017783929219717064,\n",
       "  0.014894075002395785,\n",
       "  0.016266187023398947,\n",
       "  0.015471837205772297,\n",
       "  0.015310549250731283,\n",
       "  0.015434762288780119,\n",
       "  0.013565765960643498,\n",
       "  0.01340086039363729,\n",
       "  0.014756793471786468,\n",
       "  0.014111257188696203,\n",
       "  0.014561826264008253,\n",
       "  0.01295924800402405,\n",
       "  0.012621456070785195,\n",
       "  0.012411584898945501,\n",
       "  0.012264326244967944,\n",
       "  0.011832097564370205,\n",
       "  0.011677783822094065,\n",
       "  0.010380331810652245,\n",
       "  0.0099695503598459748,\n",
       "  0.010379467347746175,\n",
       "  0.011339970282316304,\n",
       "  0.010634614832890339,\n",
       "  0.010974838237867941,\n",
       "  0.010446313942800491,\n",
       "  0.0088123724760811283,\n",
       "  0.0093244028184772069,\n",
       "  0.0093666580871684564,\n",
       "  0.0095758269229778443,\n",
       "  0.0089150462247247052,\n",
       "  0.0099396838920746133,\n",
       "  0.010159293855942324,\n",
       "  0.0088748627403743841,\n",
       "  0.0069193997719756281,\n",
       "  0.0084234591487052334,\n",
       "  0.0078236771679346204,\n",
       "  0.0072706390856766704,\n",
       "  0.0067417947518680198,\n",
       "  0.0079814260973414906,\n",
       "  0.007001362341924615,\n",
       "  0.0056857417117165479,\n",
       "  0.007157425526663876,\n",
       "  0.007878532864148604,\n",
       "  0.0073570860530367613,\n",
       "  0.0065151061176988399,\n",
       "  0.0063562209629125371,\n",
       "  0.0062744191644332559,\n",
       "  0.0063796870356361704,\n",
       "  0.0067469560981402157,\n",
       "  0.0072013754825644679,\n",
       "  0.0066770605610269439,\n",
       "  0.0071124161682991002,\n",
       "  0.0053441047024814625,\n",
       "  0.0057576447069758877,\n",
       "  0.005782083795251451,\n",
       "  0.0062050859312129877,\n",
       "  0.0066330827321461235,\n",
       "  0.0062675979163032023,\n",
       "  0.0068476320384638521,\n",
       "  0.0060975836105614852,\n",
       "  0.0062488287185003544,\n",
       "  0.0063806714881169688,\n",
       "  0.0059634892943250667,\n",
       "  0.0060875875525622314,\n",
       "  0.0060826542183535937,\n",
       "  0.0057479486319900186,\n",
       "  0.0063573113782928346,\n",
       "  0.006421038497827933,\n",
       "  0.0058858863747791338,\n",
       "  0.00540224454015491,\n",
       "  0.0063624425089353906,\n",
       "  0.0068352288986282877,\n",
       "  0.005720229279805652,\n",
       "  0.0059659839638027505,\n",
       "  0.0055577497636803835,\n",
       "  0.0058733500124466135,\n",
       "  0.006305849751319233,\n",
       "  0.006773734138009903,\n",
       "  0.0066717235566168907,\n",
       "  0.0059040703325668924,\n",
       "  0.0063672963396341617,\n",
       "  0.0063402206839945838,\n",
       "  0.0062347605889370005,\n",
       "  0.0063938099632744128,\n",
       "  0.0059094262420329611,\n",
       "  0.006058175457956106,\n",
       "  0.0058663398322323923,\n",
       "  0.0059108963615681909,\n",
       "  0.0060699599871458527,\n",
       "  0.0053549603572609975,\n",
       "  0.006614808423839328,\n",
       "  0.0063788847936591944,\n",
       "  0.0050086921326039681,\n",
       "  0.0061286125495479482,\n",
       "  0.0058153001227041713,\n",
       "  0.0054872959303494239,\n",
       "  0.0062743481125485581],\n",
       " 'val_acc': [0.42749999999999999,\n",
       "  0.55800000000000005,\n",
       "  0.6794,\n",
       "  0.71240000000000003,\n",
       "  0.77869999999999995,\n",
       "  0.80179999999999996,\n",
       "  0.80079999999999996,\n",
       "  0.84119999999999995,\n",
       "  0.76759999999999995,\n",
       "  0.80310000000000004,\n",
       "  0.81269999999999998,\n",
       "  0.84930000000000005,\n",
       "  0.85740000000000005,\n",
       "  0.85919999999999996,\n",
       "  0.83999999999999997,\n",
       "  0.84619999999999995,\n",
       "  0.87160000000000004,\n",
       "  0.88519999999999999,\n",
       "  0.84889999999999999,\n",
       "  0.85189999999999999,\n",
       "  0.86760000000000004,\n",
       "  0.80269999999999997,\n",
       "  0.87219999999999998,\n",
       "  0.89139999999999997,\n",
       "  0.84040000000000004,\n",
       "  0.89200000000000002,\n",
       "  0.88360000000000005,\n",
       "  0.89190000000000003,\n",
       "  0.89539999999999997,\n",
       "  0.84109999999999996,\n",
       "  0.87309999999999999,\n",
       "  0.90210000000000001,\n",
       "  0.89870000000000005,\n",
       "  0.87170000000000003,\n",
       "  0.88890000000000002,\n",
       "  0.90439999999999998,\n",
       "  0.89629999999999999,\n",
       "  0.89900000000000002,\n",
       "  0.89080000000000004,\n",
       "  0.89329999999999998,\n",
       "  0.9022,\n",
       "  0.88780000000000003,\n",
       "  0.90400000000000003,\n",
       "  0.89549999999999996,\n",
       "  0.90910000000000002,\n",
       "  0.92530000000000001,\n",
       "  0.92620000000000002,\n",
       "  0.92710000000000004,\n",
       "  0.92859999999999998,\n",
       "  0.9254,\n",
       "  0.92930000000000001,\n",
       "  0.92959999999999998,\n",
       "  0.92920000000000003,\n",
       "  0.93169999999999997,\n",
       "  0.93110000000000004,\n",
       "  0.92930000000000001,\n",
       "  0.92789999999999995,\n",
       "  0.93100000000000005,\n",
       "  0.93010000000000004,\n",
       "  0.93000000000000005,\n",
       "  0.93210000000000004,\n",
       "  0.92889999999999995,\n",
       "  0.93100000000000005,\n",
       "  0.92979999999999996,\n",
       "  0.93000000000000005,\n",
       "  0.93200000000000005,\n",
       "  0.92769999999999997,\n",
       "  0.93020000000000003,\n",
       "  0.92979999999999996,\n",
       "  0.93030000000000002,\n",
       "  0.93049999999999999,\n",
       "  0.93069999999999997,\n",
       "  0.92989999999999995,\n",
       "  0.93020000000000003,\n",
       "  0.92820000000000003,\n",
       "  0.92759999999999998,\n",
       "  0.93159999999999998,\n",
       "  0.92959999999999998,\n",
       "  0.92879999999999996,\n",
       "  0.9274,\n",
       "  0.93049999999999999,\n",
       "  0.9325,\n",
       "  0.93049999999999999,\n",
       "  0.92859999999999998,\n",
       "  0.93089999999999995,\n",
       "  0.93140000000000001,\n",
       "  0.93159999999999998,\n",
       "  0.93000000000000005,\n",
       "  0.93030000000000002,\n",
       "  0.93200000000000005,\n",
       "  0.93140000000000001,\n",
       "  0.93179999999999996,\n",
       "  0.93259999999999998,\n",
       "  0.93220000000000003,\n",
       "  0.93130000000000002,\n",
       "  0.93220000000000003,\n",
       "  0.93159999999999998,\n",
       "  0.93220000000000003,\n",
       "  0.93149999999999999,\n",
       "  0.93240000000000001,\n",
       "  0.9325,\n",
       "  0.93149999999999999,\n",
       "  0.93179999999999996,\n",
       "  0.93200000000000005,\n",
       "  0.93259999999999998,\n",
       "  0.93220000000000003,\n",
       "  0.93269999999999997,\n",
       "  0.93220000000000003,\n",
       "  0.93210000000000004,\n",
       "  0.93210000000000004,\n",
       "  0.93179999999999996,\n",
       "  0.93259999999999998,\n",
       "  0.93220000000000003,\n",
       "  0.93210000000000004,\n",
       "  0.93240000152587887,\n",
       "  0.93220000000000003,\n",
       "  0.93230000000000002,\n",
       "  0.93279999999999996,\n",
       "  0.93179999999999996,\n",
       "  0.93200000000000005,\n",
       "  0.93189999999999995,\n",
       "  0.93220000000000003,\n",
       "  0.93230000000000002,\n",
       "  0.93200000000000005,\n",
       "  0.93279999999999996,\n",
       "  0.9325,\n",
       "  0.93220000000000003,\n",
       "  0.9325,\n",
       "  0.93259999999999998,\n",
       "  0.93230000000000002,\n",
       "  0.93289999999999995,\n",
       "  0.93210000000000004,\n",
       "  0.93240000000000001,\n",
       "  0.93179999999999996,\n",
       "  0.93189999999999995,\n",
       "  0.93200000000000005,\n",
       "  0.93240000000000001,\n",
       "  0.93179999999999996,\n",
       "  0.93210000000000004,\n",
       "  0.93220000000000003,\n",
       "  0.93210000000000004,\n",
       "  0.93210000000000004,\n",
       "  0.93230000000000002,\n",
       "  0.93220000000000003,\n",
       "  0.93179999999999996,\n",
       "  0.93210000000000004,\n",
       "  0.93240000000000001,\n",
       "  0.93200000000000005,\n",
       "  0.93220000000000003,\n",
       "  0.93240000000000001],\n",
       " 'val_loss': [1.7168946941375733,\n",
       "  1.3742601840972901,\n",
       "  0.93227429618835445,\n",
       "  0.84331794042587283,\n",
       "  0.64278037695884704,\n",
       "  0.57432736549377439,\n",
       "  0.57999803194999699,\n",
       "  0.46977667989730837,\n",
       "  0.69412958567142491,\n",
       "  0.58222676663398742,\n",
       "  0.56464111700057984,\n",
       "  0.45456848335266115,\n",
       "  0.42603105611801145,\n",
       "  0.43690005235671997,\n",
       "  0.50069644851684569,\n",
       "  0.49618645248413085,\n",
       "  0.44315238494873049,\n",
       "  0.34422427916526793,\n",
       "  0.49210006351470947,\n",
       "  0.51668885238170625,\n",
       "  0.4258785704612732,\n",
       "  0.82352846016883852,\n",
       "  0.41980796756744387,\n",
       "  0.36812101697921751,\n",
       "  0.63119109666347506,\n",
       "  0.36908114356994631,\n",
       "  0.39265749378204345,\n",
       "  0.39568018059730531,\n",
       "  0.3622884970188141,\n",
       "  0.62513647767901426,\n",
       "  0.47254780826568604,\n",
       "  0.34190111244916915,\n",
       "  0.35845414004325865,\n",
       "  0.49633987152576448,\n",
       "  0.41721706361770627,\n",
       "  0.35047458066940307,\n",
       "  0.40956838645935056,\n",
       "  0.39787706918716431,\n",
       "  0.45659670181274414,\n",
       "  0.41498505764007571,\n",
       "  0.36465478255748751,\n",
       "  0.43420578342378141,\n",
       "  0.37943905949592588,\n",
       "  0.44679811312407253,\n",
       "  0.34539203970432281,\n",
       "  0.28166014761924746,\n",
       "  0.27363970355987549,\n",
       "  0.27652324759960173,\n",
       "  0.28258590369224551,\n",
       "  0.28978198008537293,\n",
       "  0.28564639885425569,\n",
       "  0.28508682172298433,\n",
       "  0.28753927085399628,\n",
       "  0.28492268899679185,\n",
       "  0.28791579409837725,\n",
       "  0.30193009991645814,\n",
       "  0.29097592504024505,\n",
       "  0.29509745352268218,\n",
       "  0.30308650145530702,\n",
       "  0.29848175077438355,\n",
       "  0.29614632213115694,\n",
       "  0.31208607051372528,\n",
       "  0.29878436019420623,\n",
       "  0.32006974343061445,\n",
       "  0.31493295516967773,\n",
       "  0.30513684761524201,\n",
       "  0.33238100016117095,\n",
       "  0.31770595312118532,\n",
       "  0.31690624816417695,\n",
       "  0.31765802872180937,\n",
       "  0.31706219773292543,\n",
       "  0.31514596808552742,\n",
       "  0.31997460327148436,\n",
       "  0.31366422953605649,\n",
       "  0.32689222505092619,\n",
       "  0.33839743235111236,\n",
       "  0.33191470801830292,\n",
       "  0.3328973644256592,\n",
       "  0.32832193417549133,\n",
       "  0.34619320404529569,\n",
       "  0.33095267982482912,\n",
       "  0.32474470863342286,\n",
       "  0.3307954493522644,\n",
       "  0.33685790221691131,\n",
       "  0.33707563166618348,\n",
       "  0.33186796021461484,\n",
       "  0.32898042275905609,\n",
       "  0.32799979567527771,\n",
       "  0.3255476164340973,\n",
       "  0.32435694665908815,\n",
       "  0.32351831338405607,\n",
       "  0.32587593116760255,\n",
       "  0.32549579615592955,\n",
       "  0.32601084330081942,\n",
       "  0.32631855852603914,\n",
       "  0.32505167009830477,\n",
       "  0.32567907285690306,\n",
       "  0.32552086100578309,\n",
       "  0.32439614162445068,\n",
       "  0.32374145846366881,\n",
       "  0.3244614089488983,\n",
       "  0.32662760989665984,\n",
       "  0.32521197085380554,\n",
       "  0.32615865330696103,\n",
       "  0.32740272965431211,\n",
       "  0.32786250004768369,\n",
       "  0.32729980077743531,\n",
       "  0.32707363772392273,\n",
       "  0.32788505594730377,\n",
       "  0.32704112663269042,\n",
       "  0.32704800620079039,\n",
       "  0.32747921786308287,\n",
       "  0.32762928414344789,\n",
       "  0.32707825407981872,\n",
       "  0.32608381684124471,\n",
       "  0.32608397283554075,\n",
       "  0.32734346454143526,\n",
       "  0.32735418148040774,\n",
       "  0.3271743685245514,\n",
       "  0.32632108039855956,\n",
       "  0.32745459203720095,\n",
       "  0.32759048168659211,\n",
       "  0.32664380135536192,\n",
       "  0.32692776103019716,\n",
       "  0.32779620883464811,\n",
       "  0.32778534409999849,\n",
       "  0.32782883470058444,\n",
       "  0.32771572165489199,\n",
       "  0.32681918907165525,\n",
       "  0.32778359305858612,\n",
       "  0.32749332423210142,\n",
       "  0.32650164179801938,\n",
       "  0.32651130664348604,\n",
       "  0.32691677784919737,\n",
       "  0.32717469792366027,\n",
       "  0.32642932262420654,\n",
       "  0.32695652611255643,\n",
       "  0.32630554909706116,\n",
       "  0.32615230619907382,\n",
       "  0.32544301631450651,\n",
       "  0.32744472146034242,\n",
       "  0.32791603326797486,\n",
       "  0.32685281372070313,\n",
       "  0.32639099063873289,\n",
       "  0.3262358028650284,\n",
       "  0.32646659579277038,\n",
       "  0.3262822607755661,\n",
       "  0.32601168701648714,\n",
       "  0.32601012248992922,\n",
       "  0.32691902470588685]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
